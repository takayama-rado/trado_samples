{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJGpz8Zvn13NW3ab6NeB3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takayama-rado/trado_samples/blob/main/colab_files/exp_track_interp_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load library"
      ],
      "metadata": {
        "id": "v1TnaRCUrOg5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0HqCstdLoBGH"
      },
      "outputs": [],
      "source": [
        "# Standard modules.\n",
        "import gc\n",
        "import sys\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "# CV/ML.\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python:{sys.version}\")\n",
        "print(f\"Numpy:{np.__version__}\")\n",
        "print(f\"PyTorch:{torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuPoXHoNr7dX",
        "outputId": "8add7594-9987-4d45-af32-9582715708f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python:3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
            "Numpy:1.23.5\n",
            "PyTorch:2.1.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load data"
      ],
      "metadata": {
        "id": "IyrLelRcrUYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/takayama-rado/trado_samples/raw/main/test_data/finger_far0_non_static.npy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NoM1VcOrWA5",
        "outputId": "d01aee7f-6254-4a80-c01c-0de3a2734ac0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-30 04:24:25--  https://github.com/takayama-rado/trado_samples/raw/main/test_data/finger_far0_non_static.npy\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/takayama-rado/trado_samples/main/test_data/finger_far0_non_static.npy [following]\n",
            "--2023-10-30 04:24:25--  https://raw.githubusercontent.com/takayama-rado/trado_samples/main/test_data/finger_far0_non_static.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2300608 (2.2M) [application/octet-stream]\n",
            "Saving to: ‘finger_far0_non_static.npy’\n",
            "\n",
            "finger_far0_non_sta 100%[===================>]   2.19M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-10-30 04:24:26 (30.7 MB/s) - ‘finger_far0_non_static.npy’ saved [2300608/2300608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/takayama-rado/trado_samples/raw/main/test_data/finger_far0_non_static_interp.npy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmuGhSaxrXn5",
        "outputId": "bf399201-1b62-42f7-ff1e-9eafad846057"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-30 04:24:26--  https://github.com/takayama-rado/trado_samples/raw/main/test_data/finger_far0_non_static_interp.npy\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/takayama-rado/trado_samples/main/test_data/finger_far0_non_static_interp.npy [following]\n",
            "--2023-10-30 04:24:26--  https://raw.githubusercontent.com/takayama-rado/trado_samples/main/test_data/finger_far0_non_static_interp.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2300608 (2.2M) [application/octet-stream]\n",
            "Saving to: ‘finger_far0_non_static_interp.npy’\n",
            "\n",
            "finger_far0_non_sta 100%[===================>]   2.19M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-10-30 04:24:26 (31.0 MB/s) - ‘finger_far0_non_static_interp.npy’ saved [2300608/2300608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxSDY5hdrZox",
        "outputId": "68f7aba3-a076-4559-a1cc-3c849b7fd075"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finger_far0_non_static_interp.npy  finger_far0_non_static.npy  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluation settings"
      ],
      "metadata": {
        "id": "CrQ7_swesCmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_perf_str(val):\n",
        "    token_si = [\"\", \"m\", \"µ\", \"n\", \"p\"]\n",
        "    exp_si = [1, 1e3, 1e6, 1e9, 1e12]\n",
        "    perf_str = f\"{val:3g}s\"\n",
        "    si = \"\"\n",
        "    sval = val\n",
        "    for token, exp in zip(token_si, exp_si):\n",
        "        if val * exp > 1.0:\n",
        "            si = token\n",
        "            sval = val * exp\n",
        "            break\n",
        "    perf_str = f\"{sval:3g}{si}s\"\n",
        "    return perf_str"
      ],
      "metadata": {
        "id": "HSnJrEuRsKre"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_perf_time(intervals, top_k=None):\n",
        "    if top_k is not None:\n",
        "        intervals = np.sort(intervals)[:top_k]\n",
        "    min = intervals.min()\n",
        "    max = intervals.max()\n",
        "    mean = intervals.mean()\n",
        "    std = intervals.std()\n",
        "\n",
        "    smin = get_perf_str(min)\n",
        "    smax = get_perf_str(max)\n",
        "    mean = get_perf_str(mean)\n",
        "    std = get_perf_str(std)\n",
        "    if top_k:\n",
        "        print(f\"Top {top_k} summary: Max {smax}, Min {smin}, Mean +/- Std {mean} +/- {std}\")\n",
        "    else:\n",
        "        print(f\"Overall summary: Max {smax}, Min {smin}, Mean +/- Std {mean} +/- {std}\")"
      ],
      "metadata": {
        "id": "fxEp2d1dsM1e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerfMeasure():\n",
        "    def __init__(self,\n",
        "                 trials=100,\n",
        "                 top_k=10):\n",
        "        self.trials = trials\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def __call__(self, func):\n",
        "        gc.collect()\n",
        "        gc.disable()\n",
        "        intervals = []\n",
        "        for _ in range(self.trials):\n",
        "            start = time.perf_counter()\n",
        "            func()\n",
        "            end = time.perf_counter()\n",
        "            intervals.append(end - start)\n",
        "        intervals = np.array(intervals)\n",
        "        print_perf_time(intervals)\n",
        "        if self.top_k:\n",
        "            print_perf_time(intervals, self.top_k)\n",
        "        gc.enable()\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "OVuHFZ2CsOe2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRIALS = 100\n",
        "TOPK = 10\n",
        "pmeasure = PerfMeasure(TRIALS, TOPK)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Target device is {DEVICE}.\")\n",
        "JIT_OPT = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ-D1uWfsQsg",
        "outputId": "679485bb-6db3-4de2-f35f-443b4cc9f8ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target device is cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. PyTorch implementation"
      ],
      "metadata": {
        "id": "pP5SQyAFra3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Implementation based on define-by-run mode"
      ],
      "metadata": {
        "id": "aEfGL9a4rdiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_interp_torch(track):\n",
        "    orig_shape = track.shape\n",
        "    tlength = orig_shape[0]\n",
        "    mask = track[:, 0, -1] != 0\n",
        "    valid = mask.sum()\n",
        "    if valid == tlength:\n",
        "        return track\n",
        "\n",
        "    xs = torch.where(mask != 0)[0]\n",
        "    ys = track.reshape([tlength, -1])[xs, :]\n",
        "    x = torch.arange(tlength, device=xs.device)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Interpolation.\n",
        "    # ========================================================================\n",
        "    xs = xs.to(ys.dtype)\n",
        "    x = x.to(ys.dtype)\n",
        "    # Pad control points for extrapolation.\n",
        "    # Unexpectedly, torch.finfo(torch.float64).min returns -inf.\n",
        "    # So we use torch.finfo(torch.float32).min alternatively.\n",
        "    xs = torch.cat([torch.tensor([torch.finfo(torch.float32).min], device=xs.device),\n",
        "                    xs,\n",
        "                    torch.tensor([torch.finfo(torch.float32).max], device=xs.device)], dim=0)\n",
        "    ys = torch.cat([ys[:1], ys, ys[-1:]], dim=0)\n",
        "\n",
        "    # Compute slopes, pad at the edges to flatten.\n",
        "    sloops = (ys[1:] - ys[:-1]) / torch.unsqueeze((xs[1:] - xs[:-1]), dim=-1)\n",
        "    sloops = F.pad(sloops[:-1], (0, 0, 1, 1))\n",
        "\n",
        "    # Solve for intercepts.\n",
        "    intercepts = ys - sloops * torch.unsqueeze(xs, dim=-1)\n",
        "\n",
        "    # Search for the line parameters at each input data point.\n",
        "    # Create a grid of the inputs and piece breakpoints for thresholding.\n",
        "    # Rely on argmax stopping on the first true when there are duplicates,\n",
        "    # which gives us an index into the parameter vectors.\n",
        "    mask_bk_indicator = torch.unsqueeze(xs, dim=-2) > torch.unsqueeze(x, dim=-1)\n",
        "    idx = torch.argmax(mask_bk_indicator.to(torch.int32), dim=-1)\n",
        "    sloop = sloops[idx]\n",
        "    intercept = intercepts[idx]\n",
        "\n",
        "    # Apply the linear mapping at each input data point.\n",
        "    y = sloop * torch.unsqueeze(x, dim=-1) + intercept\n",
        "    y = y.to(ys.dtype)\n",
        "    y = y.reshape(orig_shape)\n",
        "    return y\n",
        "\n",
        "\n",
        "def partsbased_interp_torch(trackdata, device=\"cpu\"):\n",
        "    trackdata = torch.from_numpy(trackdata).to(device)\n",
        "    pose = trackdata[:, :33]\n",
        "    lhand = trackdata[:, 33:33+21]\n",
        "    rhand = trackdata[:, 33+21:33+21+21]\n",
        "    face = trackdata[:, 33+21+21:]\n",
        "\n",
        "    pose = matrix_interp_torch(pose)\n",
        "    lhand = matrix_interp_torch(lhand)\n",
        "    rhand = matrix_interp_torch(rhand)\n",
        "    face = matrix_interp_torch(face)\n",
        "    return torch.cat([pose, lhand, rhand, face], dim=1)"
      ],
      "metadata": {
        "id": "BVnMUpoxr7Jh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trackdata = np.load(\"finger_far0_non_static.npy\")\n",
        "reftrack = np.load(\"finger_far0_non_static_interp.npy\")\n",
        "# Remove person axis.\n",
        "trackdata = trackdata[0]\n",
        "reftrack = reftrack[0]"
      ],
      "metadata": {
        "id": "s75MzPw3sIL4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch.\n",
        "# The 1st call may be slow because of the computation graph construction.\n",
        "print(f\"Time of first call.\")\n",
        "start = time.time()\n",
        "newtrack = partsbased_interp_torch(trackdata, device=DEVICE)\n",
        "interval = time.time() - start\n",
        "print_perf_time(np.array([interval]))\n",
        "\n",
        "diff = (reftrack - newtrack.detach().cpu().numpy()).sum()\n",
        "print(f\"Sum of error:{diff}\")\n",
        "\n",
        "print(\"Time after second call.\")\n",
        "target_fn = partial(partsbased_interp_torch, trackdata=trackdata, device=DEVICE)\n",
        "pmeasure(target_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3NzovShsLrh",
        "outputId": "c9b13744-4327-43fe-c073-f04dc6bdf89a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time of first call.\n",
            "Overall summary: Max 147.673ms, Min 147.673ms, Mean +/- Std 147.673ms +/-   0s\n",
            "Sum of error:-6.935119145623503e-12\n",
            "Time after second call.\n",
            "Overall summary: Max 20.5691ms, Min 1.33844ms, Mean +/- Std 2.20689ms +/- 2.63837ms\n",
            "Top 10 summary: Max 1.40347ms, Min 1.33844ms, Mean +/- Std 1.37999ms +/- 22.2161µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch.\n",
        "# The 1st call may be slow because of the computation graph construction.\n",
        "print(f\"Time of first call.\")\n",
        "start = time.time()\n",
        "newtrack = partsbased_interp_torch(trackdata[:-1], device=DEVICE)\n",
        "interval = time.time() - start\n",
        "print_perf_time(np.array([interval]))\n",
        "\n",
        "diff = (reftrack[:-1] - newtrack.detach().cpu().numpy()).sum()\n",
        "print(f\"Sum of error:{diff}\")\n",
        "\n",
        "print(\"Time after second call.\")\n",
        "target_fn = partial(partsbased_interp_torch, trackdata=trackdata[:-1], device=DEVICE)\n",
        "pmeasure(target_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9lEcO_osVWx",
        "outputId": "aa0c1de3-89bd-4947-8538-ecac437196af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time of first call.\n",
            "Overall summary: Max 10.7508ms, Min 10.7508ms, Mean +/- Std 10.7508ms +/-   0s\n",
            "Sum of error:-6.935119145623503e-12\n",
            "Time after second call.\n",
            "Overall summary: Max 6.57484ms, Min 1.06022ms, Mean +/- Std 1.59762ms +/- 723.317µs\n",
            "Top 10 summary: Max 1.30698ms, Min 1.06022ms, Mean +/- Std 1.23992ms +/- 67.8562µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Implementation based on define-and-run (JIT compile)"
      ],
      "metadata": {
        "id": "QeZaVpMYscc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.jit.script\n",
        "def matrix_interp_torch_jit(track):\n",
        "    orig_shape = track.shape\n",
        "    tlength = orig_shape[0]\n",
        "    mask = track[:, 0, -1] != 0\n",
        "    valid = mask.sum()\n",
        "    if valid == tlength:\n",
        "        return track\n",
        "\n",
        "    xs = torch.where(mask != 0)[0]\n",
        "    ys = track.reshape([tlength, -1])[xs, :]\n",
        "    x = torch.arange(tlength, device=xs.device)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Interpolation.\n",
        "    # ========================================================================\n",
        "    xs = xs.to(ys.dtype)\n",
        "    x = x.to(ys.dtype)\n",
        "    # Pad control points for extrapolation.\n",
        "    # Unexpectedly, torch.finfo(torch.float64).min returns -inf.\n",
        "    # So we use torch.finfo(torch.float32).min alternatively.\n",
        "    # xs = torch.cat([torch.tensor([torch.finfo(torch.float32).min]), xs, torch.tensor([torch.finfo(torch.float32).max])], dim=0)\n",
        "    # torch.finfo is not been supported in JIT.\n",
        "    xs = torch.cat([torch.tensor([-1000], device=xs.device),\n",
        "                    xs,\n",
        "                    torch.tensor([1000], device=xs.device)], dim=0)\n",
        "    ys = torch.cat([ys[:1], ys, ys[-1:]], dim=0)\n",
        "\n",
        "    # Compute slopes, pad at the edges to flatten.\n",
        "    sloops = (ys[1:] - ys[:-1]) / torch.unsqueeze((xs[1:] - xs[:-1]), dim=-1)\n",
        "    sloops = F.pad(sloops[:-1], (0, 0, 1, 1))\n",
        "\n",
        "    # Solve for intercepts.\n",
        "    intercepts = ys - sloops * torch.unsqueeze(xs, dim=-1)\n",
        "\n",
        "    # Search for the line parameters at each input data point.\n",
        "    # Create a grid of the inputs and piece breakpoints for thresholding.\n",
        "    # Rely on argmax stopping on the first true when there are duplicates,\n",
        "    # which gives us an index into the parameter vectors.\n",
        "    mask_bk_indicator = torch.unsqueeze(xs, dim=-2) > torch.unsqueeze(x, dim=-1)\n",
        "    idx = torch.argmax(mask_bk_indicator.to(torch.int32), dim=-1)\n",
        "    sloop = sloops[idx]\n",
        "    intercept = intercepts[idx]\n",
        "\n",
        "    # Apply the linear mapping at each input data point.\n",
        "    y = sloop * torch.unsqueeze(x, dim=-1) + intercept\n",
        "    y = y.to(ys.dtype)\n",
        "    y = y.reshape(orig_shape)\n",
        "    return y\n",
        "\n",
        "\n",
        "def partsbased_interp_torch_jit(trackdata, device=\"cpu\"):\n",
        "    trackdata = torch.from_numpy(trackdata).to(device)\n",
        "    pose = trackdata[:, :33]\n",
        "    lhand = trackdata[:, 33:33+21]\n",
        "    rhand = trackdata[:, 33+21:33+21+21]\n",
        "    face = trackdata[:, 33+21+21:]\n",
        "\n",
        "    pose = matrix_interp_torch_jit(pose)\n",
        "    lhand = matrix_interp_torch_jit(lhand)\n",
        "    rhand = matrix_interp_torch_jit(rhand)\n",
        "    face = matrix_interp_torch_jit(face)\n",
        "    return torch.cat([pose, lhand, rhand, face], dim=1)"
      ],
      "metadata": {
        "id": "k0MzSbZGshXI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch.\n",
        "with torch.jit.optimized_execution(JIT_OPT):\n",
        "    # The 1st call may be slow because of the computation graph construction.\n",
        "    print(f\"Time of first call.\")\n",
        "    start = time.time()\n",
        "    newtrack = partsbased_interp_torch_jit(trackdata, device=DEVICE)\n",
        "    interval = time.time() - start\n",
        "    print_perf_time(np.array([interval]))\n",
        "\n",
        "    diff = (reftrack - newtrack.detach().cpu().numpy()).sum()\n",
        "    print(f\"Sum of error:{diff}\")\n",
        "\n",
        "    print(\"Time after second call.\")\n",
        "    target_fn = partial(partsbased_interp_torch_jit, trackdata=trackdata, device=DEVICE)\n",
        "    pmeasure(target_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WF_KJ_3tHEx",
        "outputId": "594737d8-08d8-4fe4-de38-8c5305ddb6f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time of first call.\n",
            "Overall summary: Max 6.38986ms, Min 6.38986ms, Mean +/- Std 6.38986ms +/-   0s\n",
            "Sum of error:-6.935119145623503e-12\n",
            "Time after second call.\n",
            "Overall summary: Max 23.9719ms, Min 1.15678ms, Mean +/- Std 3.9527ms +/- 4.79447ms\n",
            "Top 10 summary: Max 1.25467ms, Min 1.15678ms, Mean +/- Std 1.21958ms +/- 30.8053µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch.\n",
        "with torch.jit.optimized_execution(JIT_OPT):\n",
        "    # The 1st call may be slow because of the computation graph construction.\n",
        "    print(f\"Time of first call.\")\n",
        "    start = time.time()\n",
        "    newtrack = partsbased_interp_torch_jit(trackdata[:-1], device=DEVICE)\n",
        "    interval = time.time() - start\n",
        "    print_perf_time(np.array([interval]))\n",
        "\n",
        "    diff = (reftrack[:-1] - newtrack.detach().cpu().numpy()).sum()\n",
        "    print(f\"Sum of error:{diff}\")\n",
        "\n",
        "    print(\"Time after second call.\")\n",
        "    target_fn = partial(partsbased_interp_torch_jit, trackdata=trackdata[:-1], device=DEVICE)\n",
        "    pmeasure(target_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKkfc0VCtOWY",
        "outputId": "c38109ad-adfb-42ad-c96b-7f02b6d79dd5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time of first call.\n",
            "Overall summary: Max 2.16985ms, Min 2.16985ms, Mean +/- Std 2.16985ms +/-   0s\n",
            "Sum of error:-6.935119145623503e-12\n",
            "Time after second call.\n",
            "Overall summary: Max 22.0589ms, Min 1.09035ms, Mean +/- Std 2.68458ms +/- 3.44788ms\n",
            "Top 10 summary: Max 1.16749ms, Min 1.09035ms, Mean +/- Std 1.14482ms +/- 22.0932µs\n"
          ]
        }
      ]
    }
  ]
}