{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO12mhHJ04GlI2v3S6Srffx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takayama-rado/trado_samples/blob/main/colab_files/gislr_access_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load library"
      ],
      "metadata": {
        "id": "BAjNcCBo_0O3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tb-zowpa_cMa"
      },
      "outputs": [],
      "source": [
        "# Standard modules.\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "    Any,\n",
        "    Dict\n",
        ")\n",
        "\n",
        "# Third party's modules.\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader\n",
        ")\n",
        "\n",
        "from torchvision.transforms import Compose"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Download dataset"
      ],
      "metadata": {
        "id": "quceeJ34BdlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiYD26h-OgDI",
        "outputId": "5b9d4d3f-49c1-4693-9f28-13cbff9d13a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to local.\n",
        "!cp drive/MyDrive/Datasets/gislr_dataset_top10.zip gislr_top10.zip"
      ],
      "metadata": {
        "id": "v2dAz1AuBg-Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gislr_top10.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bxFe9iCB8Vg",
        "outputId": "ca7e6cf4-6bef-4ed9-a4e3-6bbc2b25919f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gislr_top10.zip\n",
            "   creating: dataset_top10/\n",
            "  inflating: dataset_top10/16069.hdf5  \n",
            "  inflating: dataset_top10/18796.hdf5  \n",
            "  inflating: dataset_top10/2044.hdf5  \n",
            "  inflating: dataset_top10/22343.hdf5  \n",
            "  inflating: dataset_top10/25571.hdf5  \n",
            "  inflating: dataset_top10/26734.hdf5  \n",
            "  inflating: dataset_top10/27610.hdf5  \n",
            "  inflating: dataset_top10/28656.hdf5  \n",
            "  inflating: dataset_top10/29302.hdf5  \n",
            "  inflating: dataset_top10/30680.hdf5  \n",
            "  inflating: dataset_top10/32319.hdf5  \n",
            "  inflating: dataset_top10/34503.hdf5  \n",
            "  inflating: dataset_top10/36257.hdf5  \n",
            "  inflating: dataset_top10/37055.hdf5  \n",
            "  inflating: dataset_top10/37779.hdf5  \n",
            "  inflating: dataset_top10/4718.hdf5  \n",
            "  inflating: dataset_top10/49445.hdf5  \n",
            "  inflating: dataset_top10/53618.hdf5  \n",
            "  inflating: dataset_top10/55372.hdf5  \n",
            "  inflating: dataset_top10/61333.hdf5  \n",
            "  inflating: dataset_top10/62590.hdf5  \n",
            "  inflating: dataset_top10/LICENSE.txt  \n",
            "  inflating: dataset_top10/sign_to_prediction_index_map.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls dataset_top10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7mxsmo5PAUH",
        "outputId": "08429a0e-8cc3-4383-ac79-479ee506ec54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16069.hdf5  25571.hdf5\t29302.hdf5  36257.hdf5\t49445.hdf5  62590.hdf5\n",
            "18796.hdf5  26734.hdf5\t30680.hdf5  37055.hdf5\t53618.hdf5  LICENSE.txt\n",
            "2044.hdf5   27610.hdf5\t32319.hdf5  37779.hdf5\t55372.hdf5  sign_to_prediction_index_map.json\n",
            "22343.hdf5  28656.hdf5\t34503.hdf5  4718.hdf5\t61333.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat dataset_top10/sign_to_prediction_index_map.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yd6_Y-7sr4_",
        "outputId": "ed6c6319-3fe3-4f16-8598-4fc917828659"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"listen\": 0,\n",
            "    \"look\": 1,\n",
            "    \"shhh\": 2,\n",
            "    \"donkey\": 3,\n",
            "    \"mouse\": 4,\n",
            "    \"duck\": 5,\n",
            "    \"uncle\": 6,\n",
            "    \"hear\": 7,\n",
            "    \"pretend\": 8,\n",
            "    \"cow\": 9\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat dataset_top10/LICENSE.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvfr0b0v7WO-",
        "outputId": "966b01d9-57ec-4ead-972a-fa00e1466f94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset provided by Natsuki Takayama (Takayama Research and Development Office) is licensed under CC-BY 4.0.\r\n",
            "Author: Copyright 2024 Natsuki Takayama\r\n",
            "Title: GISLR Top 10 dataset\r\n",
            "Original licenser: Deaf Professional Arts Network and the Georgia Institute of Technology\r\n",
            "Modification\r\n",
            "- Extract 10 most frequent words.\r\n",
            "- Packaged into HDF5 format.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(\"dataset_top10/16069.hdf5\", \"r\") as fread:\n",
        "    keys = list(fread.keys())\n",
        "    print(keys)\n",
        "    group = fread[keys[0]]\n",
        "    print(group.keys())\n",
        "    feature = group[\"feature\"][:]\n",
        "    token = group[\"token\"][:]\n",
        "    print(feature.shape)\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVN9m2KoPLwI",
        "outputId": "1492462d-df44-4c50-ccbd-3f4efde7aa27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1109479272', '11121526', '1120349077', '1125456612', '1159046211', '1169128497', '1186032553', '1223803476', '1259708636', '1281972067', '129232566', '1334702305', '1340979012', '1352964057', '1370052047', '1383455381', '1431148933', '1435203624', '1437919781', '1458770030', '1462881097', '1469867050', '1474050058', '147607644', '1513539919', '1520635928', '1524297234', '153676122', '1537751003', '1542212461', '1551759770', '158232956', '1585855916', '1632709119', '1638742455', '1696757219', '177479476', '1779635114', '1791684792', '1801566440', '1831167282', '1867385690', '1880570146', '1901587887', '1910121429', '1919659282', '192873038', '1935012113', '1949620203', '195098847', '1983552660', '1989475963', '1998715062', '2007709802', '2036010239', '2036360025', '2046296211', '2082116372', '2098813002', '2109640010', '2121879330', '2141852087', '2148832702', '2152013823', '2166805079', '2176613834', '2184232774', '2213965523', '2249630763', '2256677805', '2263287955', '2263683020', '2264826605', '2270117351', '2281372282', '2285328250', '2322449232', '2326092712', '2336131857', '2346410171', '2405684784', '2467219838', '2473091535', '2488751095', '2515673021', '252176572', '2555449229', '2601816518', '2624310025', '2672738462', '2679264929', '2708514724', '2708544445', '2721276879', '2723773273', '2728839865', '2740241949', '2758304792', '2772051790', '2779816680', '2804441762', '2811346655', '2811646800', '2820103955', '2839681895', '2900998385', '2976200180', '2994456336', '3057994128', '3095010437', '3104317075', '3109672704', '3137179145', '3153285641', '3173321541', '3181666887', '3201889607', '3208820205', '3248401349', '3280527321', '3283246198', '3300057240', '3355963469', '3378306304', '3383637897', '3395075403', '3399503065', '3417877266', '3433092803', '347484677', '348956807', '3490841593', '3509324997', '3527069516', '3537611572', '3543711249', '3550826234', '3574700791', '3577812263', '3608551465', '3609537739', '3622960580', '3666781780', '3701422761', '3721927307', '3751410902', '3756613238', '3769544385', '3783554953', '3794712078', '380369091', '3813230821', '3817572483', '3843929707', '3869664504', '3892179393', '3923913515', '3981562204', '4078929101', '4101587978', '4150900444', '4191136837', '4200829752', '4209136328', '420958416', '422113773', '42222766', '4233346443', '4233985575', '4236057556', '4252161228', '425542996', '4276854486', '4294775577', '431609031', '439844759', '446236576', '470582324', '472128010', '492897929', '537402693', '5501547', '561170018', '593416697', '631898947', '649410423', '654529993', '678960043', '699907539', '707716966', '713615518', '778875120', '792981518', '796465415', '798649922', '856068671', '923153024', '957739544', '970593849', '976754415']\n",
            "<KeysViewHDF5 ['feature', 'token']>\n",
            "(3, 23, 543)\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implement Dataset class"
      ],
      "metadata": {
        "id": "iibrZAsdP5KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor():\n",
        "    \"\"\" Convert data to torch.Tensor.\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def __call__(self,\n",
        "                 data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        new_data = {}\n",
        "        for key, val in data.items():\n",
        "            if val is not None:\n",
        "                if isinstance(val, list):\n",
        "                    for i, subval in enumerate(val):\n",
        "                        if subval.dtype in [float, np.float64]:\n",
        "                            # pylint: disable=no-member\n",
        "                            val[i] = torch.from_numpy(subval.astype(np.float32))\n",
        "                        else:\n",
        "                            val[i] = torch.from_numpy(subval)  # pylint: disable=no-member\n",
        "                elif isinstance(val, np.ndarray):\n",
        "                    if val.dtype in [float, np.float64]:\n",
        "                        # pylint: disable=no-member\n",
        "                        val = torch.from_numpy(val.astype(np.float32))\n",
        "                    else:\n",
        "                        val = torch.from_numpy(val)  # pylint: disable=no-member\n",
        "            new_data[key] = val\n",
        "        return new_data\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.__class__.__name__}:{self.__dict__}\""
      ],
      "metadata": {
        "id": "P3QbP-nCQcVv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 hdf5files,\n",
        "                 load_into_ram=False,\n",
        "                 pre_transforms=None,\n",
        "                 transforms=None):\n",
        "        self.pre_transforms = pre_transforms\n",
        "        self.load_into_ram = load_into_ram\n",
        "        data_info = []\n",
        "        # Load file pointers.\n",
        "        for fin in hdf5files:\n",
        "            swap = 1 if \"_swap\" in fin.name else 0\n",
        "            # filename should be [pid].hdf5 or [pid]_swap.hdf5\n",
        "            pid = int(fin.stem.split(\"_\")[0])\n",
        "            with h5py.File(fin.resolve(), \"r\") as fread:\n",
        "                keys = list(fread.keys())\n",
        "                for key in keys:\n",
        "                    if load_into_ram:\n",
        "                        data = {\"feature\": fread[key][\"feature\"][:],\n",
        "                                \"token\": fread[key][\"token\"][:]}\n",
        "                        if self.pre_transforms:\n",
        "                            data = self.pre_transforms(data)\n",
        "                    else:\n",
        "                        data = None\n",
        "                    data_info.append({\n",
        "                        \"file\": fin,\n",
        "                        \"data_key\": key,\n",
        "                        \"swap\": swap,\n",
        "                        \"pid\": pid,\n",
        "                        \"data\": data})\n",
        "        self.data_info = data_info\n",
        "\n",
        "        # Check and assign transforms.\n",
        "        self.transforms = self._check_transforms(transforms)\n",
        "\n",
        "    def _check_transforms(self, transforms):\n",
        "        # Check transforms.\n",
        "        if transforms:\n",
        "            if isinstance(transforms, Compose):\n",
        "                _transforms = transforms.transforms\n",
        "            else:\n",
        "                _transforms = transforms\n",
        "            check_totensor = False\n",
        "            for trans in _transforms:\n",
        "                if isinstance(trans, ToTensor):\n",
        "                    check_totensor = True\n",
        "                    break\n",
        "            message = \"Dataset should return torch.Tensor but transforms does \" \\\n",
        "                + \"not include ToTensor class.\"\n",
        "            assert check_totensor, message\n",
        "\n",
        "        if transforms is None:\n",
        "            transforms = Compose([ToTensor()])\n",
        "        elif not isinstance(transforms, Compose):\n",
        "            transforms = Compose(transforms)\n",
        "        return transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        info = self.data_info[index]\n",
        "        if info[\"data\"]:\n",
        "            data = info[\"data\"]\n",
        "        else:\n",
        "            with h5py.File(info[\"file\"], \"r\") as fread:\n",
        "                data = {\"feature\": fread[info[\"data_key\"]][\"feature\"][:],\n",
        "                        \"token\": fread[info[\"data_key\"]][\"token\"][:]}\n",
        "        if self.load_into_ram is False and self.pre_transforms:\n",
        "            data = self.pre_transforms(data)\n",
        "        data = self.transforms(data)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_info)"
      ],
      "metadata": {
        "id": "QphBTP4ZP9Pv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access check.\n",
        "dataset_dir = Path(\"dataset_top10\")\n",
        "files = list(dataset_dir.iterdir())\n",
        "dictionary = [fin for fin in files if \".json\" in fin.name][0]\n",
        "hdf5_files = [fin for fin in files if \".hdf5\" in fin.name]\n",
        "\n",
        "print(dictionary)\n",
        "print(hdf5_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txXhRZeRQpqH",
        "outputId": "2f666d90-e657-4e75-f98a-e17047c1e445"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_top10/sign_to_prediction_index_map.json\n",
            "[PosixPath('dataset_top10/55372.hdf5'), PosixPath('dataset_top10/27610.hdf5'), PosixPath('dataset_top10/37779.hdf5'), PosixPath('dataset_top10/2044.hdf5'), PosixPath('dataset_top10/36257.hdf5'), PosixPath('dataset_top10/53618.hdf5'), PosixPath('dataset_top10/32319.hdf5'), PosixPath('dataset_top10/37055.hdf5'), PosixPath('dataset_top10/4718.hdf5'), PosixPath('dataset_top10/25571.hdf5'), PosixPath('dataset_top10/18796.hdf5'), PosixPath('dataset_top10/29302.hdf5'), PosixPath('dataset_top10/34503.hdf5'), PosixPath('dataset_top10/28656.hdf5'), PosixPath('dataset_top10/26734.hdf5'), PosixPath('dataset_top10/16069.hdf5'), PosixPath('dataset_top10/30680.hdf5'), PosixPath('dataset_top10/49445.hdf5'), PosixPath('dataset_top10/22343.hdf5'), PosixPath('dataset_top10/62590.hdf5'), PosixPath('dataset_top10/61333.hdf5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = HDF5Dataset(hdf5_files)\n",
        "print(len(dataset))\n",
        "\n",
        "data = next(iter(dataset))\n",
        "feature = data[\"feature\"]\n",
        "token = data[\"token\"]\n",
        "\n",
        "print(feature.shape)\n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gie98dpvRHQH",
        "outputId": "7c5d3f8b-6d05-4f10-daae-062c321d20e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4081\n",
            "torch.Size([3, 25, 543])\n",
            "tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Implement merging process for DataLoader class"
      ],
      "metadata": {
        "id": "NZ2xTZFDRjfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access check.\n",
        "dataloader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "data = next(iter(dataloader))\n",
        "feature = data[\"feature\"]\n",
        "token = data[\"token\"]\n",
        "\n",
        "print(feature.shape)\n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnQ_xDowRxAP",
        "outputId": "d62d7d5e-b63b-484a-d1f0-f726361ce27c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 25, 543])\n",
            "tensor([[3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access check.\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "try:\n",
        "    data = next(iter(dataloader))\n",
        "    feature = data[\"feature\"]\n",
        "    token = data[\"token\"]\n",
        "\n",
        "    print(feature.shape)\n",
        "    print(token)\n",
        "except Exception as inst:\n",
        "    print(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23XZcPsMR5gH",
        "outputId": "4c7b008f-83a1-400d-c396-e2218c1fbcd4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stack expects each tensor to be equal size, but got [3, 25, 543] at entry 0 and [3, 23, 543] at entry 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(sequences, merged_shape, padding_val=0):\n",
        "    merged = torch.full(tuple(merged_shape),\n",
        "                        padding_val,\n",
        "                        dtype=sequences[0].dtype)\n",
        "    if len(merged_shape) == 2:\n",
        "        for i, seq in enumerate(sequences):\n",
        "            merged[i,\n",
        "                   :seq.shape[0]] = seq\n",
        "    if len(merged_shape) == 3:\n",
        "        for i, seq in enumerate(sequences):\n",
        "            merged[i,\n",
        "                   :seq.shape[0],\n",
        "                   :seq.shape[1]] = seq\n",
        "    if len(merged_shape) == 4:\n",
        "        for i, seq in enumerate(sequences):\n",
        "            merged[i,\n",
        "                   :seq.shape[0],\n",
        "                   :seq.shape[1],\n",
        "                   :seq.shape[2]] = seq\n",
        "    if len(merged_shape) == 5:\n",
        "        for i, seq in enumerate(sequences):\n",
        "            merged[i,\n",
        "                   :seq.shape[0],\n",
        "                   :seq.shape[1],\n",
        "                   :seq.shape[2],\n",
        "                   :seq.shape[3]] = seq\n",
        "    return merged\n",
        "\n",
        "\n",
        "def merge_padded_batch(batch,\n",
        "                       feature_shape,\n",
        "                       token_shape,\n",
        "                       feature_padding_val=0,\n",
        "                       token_padding_val=0):\n",
        "    feature_batch = [sample[\"feature\"] for sample in batch]\n",
        "    token_batch = [sample[\"token\"] for sample in batch]\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge feature.\n",
        "    # ==========================================================\n",
        "    # `[B, C, T, J]`\n",
        "    merged_shape = [len(batch), *feature_shape]\n",
        "    # Use maximum frame length in a batch as padded length.\n",
        "    if merged_shape[2] == -1:\n",
        "        tlen = max([feature.shape[1] for feature in feature_batch])\n",
        "        merged_shape[2] = tlen\n",
        "    merged_feature = merge(feature_batch, merged_shape, padding_val=feature_padding_val)\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge tocken.\n",
        "    # ==========================================================\n",
        "    # `[B, L]`\n",
        "    merged_shape = [len(batch), *token_shape]\n",
        "    merged_token = merge(token_batch, merged_shape, padding_val=token_padding_val)\n",
        "\n",
        "    # Generate padding mask.\n",
        "    # Pad: 0, Signal: 1\n",
        "    # The frames which all channels and landmarks are equals to padding value\n",
        "    # should be padded.\n",
        "    feature_pad_mask = merged_feature == feature_padding_val\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=1)\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=-1)\n",
        "    feature_pad_mask = torch.logical_not(feature_pad_mask)\n",
        "    token_pad_mask = torch.logical_not(merged_token == token_padding_val)\n",
        "\n",
        "    retval = {\n",
        "        \"feature\": merged_feature,\n",
        "        \"token\": merged_token,\n",
        "        \"feature_pad_mask\": feature_pad_mask,\n",
        "        \"token_pad_mask\": token_pad_mask}\n",
        "    return retval"
      ],
      "metadata": {
        "id": "N1_zOwN2SEQO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "feature_shape = (3, -1, 543)\n",
        "token_shape = (1,)\n",
        "merge_fn = partial(merge_padded_batch,\n",
        "                   feature_shape=feature_shape,\n",
        "                   token_shape=token_shape,\n",
        "                   feature_padding_val=0.0,\n",
        "                   token_padding_val=0)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=merge_fn)\n",
        "\n",
        "try:\n",
        "    data = next(iter(dataloader))\n",
        "    feature = data[\"feature\"]\n",
        "    token = data[\"token\"]\n",
        "    feature_pad_mask = data[\"feature_pad_mask\"]\n",
        "    token_pad_mask = data[\"token_pad_mask\"]\n",
        "\n",
        "    print(feature.shape)\n",
        "    print(token)\n",
        "    print(feature_pad_mask)\n",
        "    print(token_pad_mask)\n",
        "except Exception as inst:\n",
        "    print(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ish0hcO5SiMu",
        "outputId": "f694ac4d-d9ab-426c-e015-f7c05eb0e902"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 25, 543])\n",
            "tensor([[3],\n",
            "        [0]])\n",
            "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True, False, False]])\n",
            "tensor([[ True],\n",
            "        [False]])\n"
          ]
        }
      ]
    }
  ]
}