{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4Ia9w4YgkqYIVfXpui5+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takayama-rado/trado_samples/blob/main/colab_files/gislr_simple_islr_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download dataset and modules"
      ],
      "metadata": {
        "id": "wScni5-7JoB_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od6I1s00Jk9n",
        "outputId": "fed9258f-674e-4016-9e00-6c1cbad8b403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gdown==4.6.0\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.0) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.6.0) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.6.0\n"
          ]
        }
      ],
      "source": [
        "# I do not why, but only older virsion is works.\n",
        "# https://github.com/wkentaro/gdown/issues/43#issuecomment-1426653602\n",
        "!pip install gdown==4.6.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1LJDGEwr4zqpBftjqO9SiSOLgUcNhMiSe\"\n",
        "output = \"gislr_top10.zip\"\n",
        "gdown.download(url, output, quiet=False, fuzzy=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "sTJKQ-9WJ4px",
        "outputId": "7cfa8330-5cb5-4a4b-c2a3-8187b64e3bb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LJDGEwr4zqpBftjqO9SiSOLgUcNhMiSe\n",
            "To: /content/gislr_top10.zip\n",
            "100%|██████████| 880M/880M [00:05<00:00, 162MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gislr_top10.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o gislr_top10.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWZvfO_0J5On",
        "outputId": "ab13b295-3e44-4ae2-af25-d1c14322901d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gislr_top10.zip\n",
            "   creating: dataset_top10/\n",
            "  inflating: dataset_top10/16069.hdf5  \n",
            "  inflating: dataset_top10/18796.hdf5  \n",
            "  inflating: dataset_top10/2044.hdf5  \n",
            "  inflating: dataset_top10/22343.hdf5  \n",
            "  inflating: dataset_top10/25571.hdf5  \n",
            "  inflating: dataset_top10/26734.hdf5  \n",
            "  inflating: dataset_top10/27610.hdf5  \n",
            "  inflating: dataset_top10/28656.hdf5  \n",
            "  inflating: dataset_top10/29302.hdf5  \n",
            "  inflating: dataset_top10/30680.hdf5  \n",
            "  inflating: dataset_top10/32319.hdf5  \n",
            "  inflating: dataset_top10/34503.hdf5  \n",
            "  inflating: dataset_top10/36257.hdf5  \n",
            "  inflating: dataset_top10/37055.hdf5  \n",
            "  inflating: dataset_top10/37779.hdf5  \n",
            "  inflating: dataset_top10/4718.hdf5  \n",
            "  inflating: dataset_top10/49445.hdf5  \n",
            "  inflating: dataset_top10/53618.hdf5  \n",
            "  inflating: dataset_top10/55372.hdf5  \n",
            "  inflating: dataset_top10/61333.hdf5  \n",
            "  inflating: dataset_top10/62590.hdf5  \n",
            "  inflating: dataset_top10/LICENSE.txt  \n",
            "  inflating: dataset_top10/sign_to_prediction_index_map.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls dataset_top10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0li39OxJ7df",
        "outputId": "9b4d2ca2-920d-4398-975c-1b17c54a9a05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16069.hdf5  25571.hdf5\t29302.hdf5  36257.hdf5\t49445.hdf5  62590.hdf5\n",
            "18796.hdf5  26734.hdf5\t30680.hdf5  37055.hdf5\t53618.hdf5  LICENSE.txt\n",
            "2044.hdf5   27610.hdf5\t32319.hdf5  37779.hdf5\t55372.hdf5  sign_to_prediction_index_map.json\n",
            "22343.hdf5  28656.hdf5\t34503.hdf5  4718.hdf5\t61333.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat dataset_top10/sign_to_prediction_index_map.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSHBTUxKJ9Zf",
        "outputId": "302694dd-eb4a-40d1-c567-9340e0569c2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"listen\": 0,\n",
            "    \"look\": 1,\n",
            "    \"shhh\": 2,\n",
            "    \"donkey\": 3,\n",
            "    \"mouse\": 4,\n",
            "    \"duck\": 5,\n",
            "    \"uncle\": 6,\n",
            "    \"hear\": 7,\n",
            "    \"pretend\": 8,\n",
            "    \"cow\": 9\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat dataset_top10/LICENSE.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swy-t-5YKAZv",
        "outputId": "beb3bac2-a3d5-4401-dd7c-e9126f13212f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset provided by Natsuki Takayama (Takayama Research and Development Office) is licensed under CC-BY 4.0.\r\n",
            "Author: Copyright 2024 Natsuki Takayama\r\n",
            "Title: GISLR Top 10 dataset\r\n",
            "Original licenser: Deaf Professional Arts Network and the Georgia Institute of Technology\r\n",
            "Modification\r\n",
            "- Extract 10 most frequent words.\r\n",
            "- Packaged into HDF5 format.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "with h5py.File(\"dataset_top10/16069.hdf5\", \"r\") as fread:\n",
        "    keys = list(fread.keys())\n",
        "    print(keys)\n",
        "    group = fread[keys[0]]\n",
        "    print(group.keys())\n",
        "    feature = group[\"feature\"][:]\n",
        "    token = group[\"token\"][:]\n",
        "    print(feature.shape)\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXQbjUq0KCZv",
        "outputId": "81ea26b5-c115-44ad-a391-9e3822ec35f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1109479272', '11121526', '1120349077', '1125456612', '1159046211', '1169128497', '1186032553', '1223803476', '1259708636', '1281972067', '129232566', '1334702305', '1340979012', '1352964057', '1370052047', '1383455381', '1431148933', '1435203624', '1437919781', '1458770030', '1462881097', '1469867050', '1474050058', '147607644', '1513539919', '1520635928', '1524297234', '153676122', '1537751003', '1542212461', '1551759770', '158232956', '1585855916', '1632709119', '1638742455', '1696757219', '177479476', '1779635114', '1791684792', '1801566440', '1831167282', '1867385690', '1880570146', '1901587887', '1910121429', '1919659282', '192873038', '1935012113', '1949620203', '195098847', '1983552660', '1989475963', '1998715062', '2007709802', '2036010239', '2036360025', '2046296211', '2082116372', '2098813002', '2109640010', '2121879330', '2141852087', '2148832702', '2152013823', '2166805079', '2176613834', '2184232774', '2213965523', '2249630763', '2256677805', '2263287955', '2263683020', '2264826605', '2270117351', '2281372282', '2285328250', '2322449232', '2326092712', '2336131857', '2346410171', '2405684784', '2467219838', '2473091535', '2488751095', '2515673021', '252176572', '2555449229', '2601816518', '2624310025', '2672738462', '2679264929', '2708514724', '2708544445', '2721276879', '2723773273', '2728839865', '2740241949', '2758304792', '2772051790', '2779816680', '2804441762', '2811346655', '2811646800', '2820103955', '2839681895', '2900998385', '2976200180', '2994456336', '3057994128', '3095010437', '3104317075', '3109672704', '3137179145', '3153285641', '3173321541', '3181666887', '3201889607', '3208820205', '3248401349', '3280527321', '3283246198', '3300057240', '3355963469', '3378306304', '3383637897', '3395075403', '3399503065', '3417877266', '3433092803', '347484677', '348956807', '3490841593', '3509324997', '3527069516', '3537611572', '3543711249', '3550826234', '3574700791', '3577812263', '3608551465', '3609537739', '3622960580', '3666781780', '3701422761', '3721927307', '3751410902', '3756613238', '3769544385', '3783554953', '3794712078', '380369091', '3813230821', '3817572483', '3843929707', '3869664504', '3892179393', '3923913515', '3981562204', '4078929101', '4101587978', '4150900444', '4191136837', '4200829752', '4209136328', '420958416', '422113773', '42222766', '4233346443', '4233985575', '4236057556', '4252161228', '425542996', '4276854486', '4294775577', '431609031', '439844759', '446236576', '470582324', '472128010', '492897929', '537402693', '5501547', '561170018', '593416697', '631898947', '649410423', '654529993', '678960043', '699907539', '707716966', '713615518', '778875120', '792981518', '796465415', '798649922', '856068671', '923153024', '957739544', '970593849', '976754415']\n",
            "<KeysViewHDF5 ['feature', 'token']>\n",
            "(3, 23, 543)\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/takayama-rado/trado_samples/archive/master.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbbfALIiKG3X",
        "outputId": "0ae05ff8-cede-42c5-804f-3fd15d8c9896"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-21 11:01:47--  https://github.com/takayama-rado/trado_samples/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/takayama-rado/trado_samples/zip/main [following]\n",
            "--2024-01-21 11:01:47--  https://codeload.github.com/takayama-rado/trado_samples/zip/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [         <=>        ]  72.20M  19.4MB/s    in 3.7s    \n",
            "\n",
            "2024-01-21 11:01:51 (19.4 MB/s) - ‘master.zip’ saved [75710869]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o master.zip -d master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O84WgKgNKrC4",
        "outputId": "51152946-b765-4396-da74-68a89a1128c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  master.zip\n",
            "641b06a0ca7f5430a945a53b4825e22b5f3b8eb6\n",
            "   creating: master/trado_samples-main/\n",
            "  inflating: master/trado_samples-main/.gitignore  \n",
            "  inflating: master/trado_samples-main/LICENSE  \n",
            "  inflating: master/trado_samples-main/README.md  \n",
            "   creating: master/trado_samples-main/colab_files/\n",
            " extracting: master/trado_samples-main/colab_files/.gitkeep  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_affine_np_einsum.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_jax_static.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_mpholistic_0.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_mpothers_0.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_affine_jax.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_affine_numpy.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_affine_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_affine_torch.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_interp_jax.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_interp_numpy_0.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_interp_numpy_1.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_interp_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/exp_track_interp_torch.ipynb  \n",
            "  inflating: master/trado_samples-main/colab_files/gislr_access_dataset.ipynb  \n",
            "   creating: master/trado_samples-main/src/\n",
            "   creating: master/trado_samples-main/src/modules_gislr/\n",
            " extracting: master/trado_samples-main/src/modules_gislr/__init__.py  \n",
            "  inflating: master/trado_samples-main/src/modules_gislr/dataset.py  \n",
            "  inflating: master/trado_samples-main/src/modules_gislr/defines.py  \n",
            "  inflating: master/trado_samples-main/src/modules_gislr/layers.py  \n",
            "  inflating: master/trado_samples-main/src/modules_gislr/transforms.py  \n",
            "   creating: master/trado_samples-main/test_data/\n",
            "  inflating: master/trado_samples-main/test_data/finger_far0.mp4  \n",
            "  inflating: master/trado_samples-main/test_data/finger_far0_non_static.npy  \n",
            "  inflating: master/trado_samples-main/test_data/finger_far0_non_static_affine.npy  \n",
            "  inflating: master/trado_samples-main/test_data/finger_far0_non_static_interp.npy  \n",
            "  inflating: master/trado_samples-main/test_data/finger_middle0.mp4  \n",
            "  inflating: master/trado_samples-main/test_data/finger_near0.mp4  \n",
            "  inflating: master/trado_samples-main/test_data/hand_only.mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv master/trado_samples-main/src/modules_gislr ."
      ],
      "metadata": {
        "id": "4qq4vgIaaVNY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf master master.zip"
      ],
      "metadata": {
        "id": "oX2zLvICaZ44"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYwKfuCab1OQ",
        "outputId": "03de9552-768e-4e98-f998-5f3240a0d437"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_top10  gislr_top10.zip\tmodules_gislr  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load library"
      ],
      "metadata": {
        "id": "g54Rq2AUKs6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "    Any,\n",
        "    Dict\n",
        ")\n",
        "\n",
        "# Third party's modules\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import (\n",
        "    DataLoader)\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "# Local modules\n",
        "sys.path.append(\"modules_gislr\")\n",
        "from modules_gislr.dataset import (\n",
        "    HDF5Dataset,\n",
        "    merge_padded_batch)\n",
        "from modules_gislr.transforms import (\n",
        "    ToTensor\n",
        ")"
      ],
      "metadata": {
        "id": "tRjIxCv0KxKf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implement preprocess"
      ],
      "metadata": {
        "id": "3bV2HLPBM39u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplaceNan():\n",
        "    \"\"\" Replace NaN value in the feature.\n",
        "    \"\"\"\n",
        "    def __init__(self, replace_val=0.0) -> None:\n",
        "        self.replace_val = replace_val\n",
        "\n",
        "    def __call__(self,\n",
        "                 data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        feature = data[\"feature\"]\n",
        "        feature[np.isnan(feature)] = self.replace_val\n",
        "        data[\"feature\"] = feature\n",
        "        return data"
      ],
      "metadata": {
        "id": "vaZWRk6MM2Ef"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access check.\n",
        "dataset_dir = Path(\"dataset_top10\")\n",
        "files = list(dataset_dir.iterdir())\n",
        "dictionary = [fin for fin in files if \".json\" in fin.name][0]\n",
        "hdf5_files = [fin for fin in files if \".hdf5\" in fin.name]\n",
        "\n",
        "print(dictionary)\n",
        "print(hdf5_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srS0oWPDNU1X",
        "outputId": "e4a85212-49b2-4d29-8ab6-91c943e9cca6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_top10/sign_to_prediction_index_map.json\n",
            "[PosixPath('dataset_top10/34503.hdf5'), PosixPath('dataset_top10/25571.hdf5'), PosixPath('dataset_top10/37779.hdf5'), PosixPath('dataset_top10/29302.hdf5'), PosixPath('dataset_top10/53618.hdf5'), PosixPath('dataset_top10/61333.hdf5'), PosixPath('dataset_top10/26734.hdf5'), PosixPath('dataset_top10/16069.hdf5'), PosixPath('dataset_top10/27610.hdf5'), PosixPath('dataset_top10/37055.hdf5'), PosixPath('dataset_top10/22343.hdf5'), PosixPath('dataset_top10/49445.hdf5'), PosixPath('dataset_top10/55372.hdf5'), PosixPath('dataset_top10/28656.hdf5'), PosixPath('dataset_top10/32319.hdf5'), PosixPath('dataset_top10/18796.hdf5'), PosixPath('dataset_top10/62590.hdf5'), PosixPath('dataset_top10/36257.hdf5'), PosixPath('dataset_top10/30680.hdf5'), PosixPath('dataset_top10/4718.hdf5'), PosixPath('dataset_top10/2044.hdf5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = HDF5Dataset(hdf5_files)\n",
        "\n",
        "batch_size = 2\n",
        "feature_shape = (3, -1, 543)\n",
        "token_shape = (1,)\n",
        "merge_fn = partial(merge_padded_batch,\n",
        "                   feature_shape=feature_shape,\n",
        "                   token_shape=token_shape,\n",
        "                   feature_padding_val=0.0,\n",
        "                   token_padding_val=0)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=merge_fn)\n",
        "\n",
        "try:\n",
        "    data = next(iter(dataloader))\n",
        "    feature = data[\"feature\"]\n",
        "    token = data[\"token\"]\n",
        "    feature_pad_mask = data[\"feature_pad_mask\"]\n",
        "    token_pad_mask = data[\"token_pad_mask\"]\n",
        "\n",
        "    print(feature.shape)\n",
        "    print(token)\n",
        "    print(feature_pad_mask)\n",
        "    print(token_pad_mask)\n",
        "except Exception as inst:\n",
        "    print(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11xxGxEKNktn",
        "outputId": "97ef4acc-59c5-4e42-bee2-28bd4d986863"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 88, 543])\n",
            "tensor([[1],\n",
            "        [5]])\n",
            "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True, False]])\n",
            "tensor([[True],\n",
            "        [True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(feature)\n",
        "print(torch.isnan(feature).any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LAQ6mwdNtIg",
        "outputId": "4a1e65d2-1269-4734-b055-a5f95d99237b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.3574,  0.3642,  0.3579,  ...,     nan,     nan,     nan],\n",
            "          [ 0.3601,  0.3618,  0.3556,  ...,     nan,     nan,     nan],\n",
            "          [ 0.3620,  0.3612,  0.3551,  ...,     nan,     nan,     nan],\n",
            "          ...,\n",
            "          [ 0.3246,  0.3196,  0.3203,  ...,     nan,     nan,     nan],\n",
            "          [ 0.3257,  0.3191,  0.3199,  ...,     nan,     nan,     nan],\n",
            "          [ 0.3256,  0.3195,  0.3202,  ...,     nan,     nan,     nan]],\n",
            "\n",
            "         [[ 0.5894,  0.5341,  0.5505,  ...,     nan,     nan,     nan],\n",
            "          [ 0.5891,  0.5350,  0.5511,  ...,     nan,     nan,     nan],\n",
            "          [ 0.5886,  0.5353,  0.5513,  ...,     nan,     nan,     nan],\n",
            "          ...,\n",
            "          [ 0.5727,  0.5132,  0.5310,  ...,     nan,     nan,     nan],\n",
            "          [ 0.5732,  0.5123,  0.5304,  ...,     nan,     nan,     nan],\n",
            "          [ 0.5733,  0.5121,  0.5303,  ...,     nan,     nan,     nan]],\n",
            "\n",
            "         [[-0.0583, -0.0979, -0.0543,  ...,     nan,     nan,     nan],\n",
            "          [-0.0574, -0.0979, -0.0542,  ...,     nan,     nan,     nan],\n",
            "          [-0.0574, -0.0974, -0.0541,  ...,     nan,     nan,     nan],\n",
            "          ...,\n",
            "          [-0.0630, -0.1018, -0.0579,  ...,     nan,     nan,     nan],\n",
            "          [-0.0641, -0.1024, -0.0586,  ...,     nan,     nan,     nan],\n",
            "          [-0.0641, -0.1025, -0.0586,  ...,     nan,     nan,     nan]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4579,  0.4695,  0.4604,  ...,     nan,     nan,     nan],\n",
            "          [ 0.4585,  0.4690,  0.4607,  ...,     nan,     nan,     nan],\n",
            "          [ 0.4620,  0.4709,  0.4623,  ...,     nan,     nan,     nan],\n",
            "          ...,\n",
            "          [ 0.5033,  0.5159,  0.5011,  ...,     nan,     nan,     nan],\n",
            "          [ 0.5009,  0.5180,  0.5033,  ...,     nan,     nan,     nan],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.6169,  0.5718,  0.5878,  ...,     nan,     nan,     nan],\n",
            "          [ 0.6171,  0.5722,  0.5892,  ...,     nan,     nan,     nan],\n",
            "          [ 0.6177,  0.5704,  0.5876,  ...,     nan,     nan,     nan],\n",
            "          ...,\n",
            "          [ 0.6039,  0.5590,  0.5732,  ...,     nan,     nan,     nan],\n",
            "          [ 0.6042,  0.5586,  0.5730,  ...,     nan,     nan,     nan],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0646, -0.0879, -0.0546,  ...,     nan,     nan,     nan],\n",
            "          [-0.0640, -0.0909, -0.0559,  ...,     nan,     nan,     nan],\n",
            "          [-0.0635, -0.0893, -0.0544,  ...,     nan,     nan,     nan],\n",
            "          ...,\n",
            "          [-0.0506, -0.0759, -0.0458,  ...,     nan,     nan,     nan],\n",
            "          [-0.0510, -0.0759, -0.0461,  ...,     nan,     nan,     nan],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_transforms = Compose([ReplaceNan()])\n",
        "transforms = Compose([ToTensor()])\n",
        "\n",
        "dataset = HDF5Dataset(hdf5_files, pre_transforms=pre_transforms, transforms=transforms)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=merge_fn)\n",
        "try:\n",
        "    data = next(iter(dataloader))\n",
        "    feature = data[\"feature\"]\n",
        "\n",
        "    print(feature)\n",
        "    print(torch.isnan(feature).any())\n",
        "except Exception as inst:\n",
        "    print(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSEvt5MlNvhP",
        "outputId": "c758f1a6-b0fa-42a3-a9d2-81894e3c8ca0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.3574,  0.3642,  0.3579,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.3601,  0.3618,  0.3556,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.3620,  0.3612,  0.3551,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.3246,  0.3196,  0.3203,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.3257,  0.3191,  0.3199,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.3256,  0.3195,  0.3202,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.5894,  0.5341,  0.5505,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.5891,  0.5350,  0.5511,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.5886,  0.5353,  0.5513,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.5727,  0.5132,  0.5310,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.5732,  0.5123,  0.5304,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.5733,  0.5121,  0.5303,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0583, -0.0979, -0.0543,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0574, -0.0979, -0.0542,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0574, -0.0974, -0.0541,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [-0.0630, -0.1018, -0.0579,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0641, -0.1024, -0.0586,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0641, -0.1025, -0.0586,  ...,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4579,  0.4695,  0.4604,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.4585,  0.4690,  0.4607,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.4620,  0.4709,  0.4623,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.5033,  0.5159,  0.5011,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.5009,  0.5180,  0.5033,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.6169,  0.5718,  0.5878,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.6171,  0.5722,  0.5892,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.6177,  0.5704,  0.5876,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.6039,  0.5590,  0.5732,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.6042,  0.5586,  0.5730,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0646, -0.0879, -0.0546,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0640, -0.0909, -0.0559,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0635, -0.0893, -0.0544,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [-0.0506, -0.0759, -0.0458,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0510, -0.0759, -0.0461,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])\n",
            "tensor(False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Implement ISLR model"
      ],
      "metadata": {
        "id": "DyKkUBEWOaMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPoolRecognitionHead(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.head = nn.Linear(in_channels, out_channels)\n",
        "        self._init_weight()\n",
        "\n",
        "    def _init_weight(self):\n",
        "        nn.init.normal_(self.head.weight,\n",
        "                        mean=0.,\n",
        "                        std=math.sqrt(1. / self.out_channels))\n",
        "\n",
        "    def forward(self, feature):\n",
        "        # Averaging over temporal axis.\n",
        "        # `[N, C, T] -> [N, C, 1] -> [N, C]`\n",
        "        feature = feature = F.avg_pool1d(feature, kernel_size=feature.shape[-1])\n",
        "        feature = feature.reshape(feature.shape[0], -1)\n",
        "\n",
        "        # Predict.\n",
        "        feature = self.head(feature)\n",
        "        return feature"
      ],
      "metadata": {
        "id": "WHK70-GIOmgI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleISLR(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(in_channels, 64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.head = GPoolRecognitionHead(64, out_channels)\n",
        "\n",
        "    def forward(self, feature):\n",
        "        # Feature extraction.\n",
        "        # `[N, C, T, J] -> [N, T, C, J] -> [N, T, C*J] -> [N, T, C']`\n",
        "        N, C, T, J = feature.shape\n",
        "        feature = feature.permute([0, 2, 1, 3])\n",
        "        feature = feature.reshape(N, T, -1)\n",
        "\n",
        "        feature = self.linear(feature)\n",
        "        feature = self.activation(feature)\n",
        "\n",
        "        # `[N, T, C'] -> [N, C', T]`\n",
        "        feature = feature.permute(0, 2, 1)\n",
        "        logit = self.head(feature)\n",
        "        return logit"
      ],
      "metadata": {
        "id": "Exb_4syWTLQh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model.\n",
        "# in_channels: J * C (543*3=1629)\n",
        "#   J: use_landmarks (543)\n",
        "#   C: use_channels (3)\n",
        "# out_channels: 10\n",
        "feature_shape = (3, -1, 543)\n",
        "in_channels = feature_shape[0] * feature_shape[2]\n",
        "out_channels = 10\n",
        "\n",
        "model = SimpleISLR(in_channels, out_channels)\n",
        "print(model)\n",
        "\n",
        "# Sanity check.\n",
        "logit = model(feature)\n",
        "print(logit.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUdZEKnBTQlR",
        "outputId": "cb5a327d-5fa1-45d6-95ca-e205ced450e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleISLR(\n",
            "  (linear): Linear(in_features=1629, out_features=64, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (head): GPoolRecognitionHead(\n",
            "    (head): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([2, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Implement training loop"
      ],
      "metadata": {
        "id": "142dfXYxU23x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    # Switch to training mode.\n",
        "    model.train()\n",
        "    # Main loop.\n",
        "    print(\"Start training.\")\n",
        "    start = time.perf_counter()\n",
        "    for batch_idx, batch_sample in enumerate(dataloader):\n",
        "        feature = batch_sample[\"feature\"]\n",
        "        token = batch_sample[\"token\"]\n",
        "        feature = feature.to(device)\n",
        "        token = token.to(device)\n",
        "\n",
        "        # Predict and compute loss.\n",
        "        pred = model(feature)\n",
        "        loss = loss_fn(pred, token.squeeze(-1))\n",
        "\n",
        "        # Back propagation.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print current loss per 100 steps.\n",
        "        if batch_idx % 100 == 0:\n",
        "            loss = loss.item()\n",
        "            steps = batch_idx * len(feature)\n",
        "            print(f\"loss:{loss:>7f} [{steps:>5d}/{size:>5d}]\")\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "\n",
        "def val_loop(dataloader, model, loss_fn, device):\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss = 0\n",
        "\n",
        "    # Switch to evaluation mode.\n",
        "    model.eval()\n",
        "    # Main loop.\n",
        "    print(\"Start validation.\")\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch_sample in dataloader:\n",
        "            feature = batch_sample[\"feature\"]\n",
        "            token = batch_sample[\"token\"]\n",
        "            feature = feature.to(device)\n",
        "            token = token.to(device)\n",
        "\n",
        "            pred = model(feature)\n",
        "            val_loss += loss_fn(pred, token.squeeze(-1)).item()\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average loss.\n",
        "    val_loss /= num_batches\n",
        "    print(\"Validation performance: \\n\",\n",
        "          f\"Avg loss:{val_loss:>8f}\\n\")\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "\n",
        "    # Switch to evaluation mode.\n",
        "    model.eval()\n",
        "    # Main loop.\n",
        "    print(\"Start evaluation.\")\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch_sample in dataloader:\n",
        "            feature = batch_sample[\"feature\"]\n",
        "            token = batch_sample[\"token\"]\n",
        "            feature = feature.to(device)\n",
        "            token = token.to(device)\n",
        "\n",
        "            pred = model(feature)\n",
        "            pred_ids = pred.argmax(dim=1).unsqueeze(-1)\n",
        "            count = (pred_ids == token).sum().detach().cpu().numpy()\n",
        "            correct += int(count)\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    acc = correct / size * 100\n",
        "    print(\"Test performance: \\n\",\n",
        "          f\"Accuracy:{acc:>0.1f}%\")\n",
        "    return acc"
      ],
      "metadata": {
        "id": "1-xHI8pJU6hX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train and evaluation"
      ],
      "metadata": {
        "id": "PoXUv61zVHEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataloaders.\n",
        "batch_size = 32\n",
        "load_into_ram = True\n",
        "test_pid = 16069\n",
        "num_workers = 1\n",
        "\n",
        "train_hdf5files = [fin for fin in hdf5_files if str(test_pid) not in fin.name]\n",
        "val_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "test_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "\n",
        "train_dataset = HDF5Dataset(train_hdf5files, pre_transforms=pre_transforms,\n",
        "    transforms=transforms, load_into_ram=load_into_ram)\n",
        "val_dataset = HDF5Dataset(val_hdf5files, pre_transforms=pre_transforms,\n",
        "    transforms=transforms, load_into_ram=load_into_ram)\n",
        "test_dataset = HDF5Dataset(test_hdf5files, pre_transforms=pre_transforms,\n",
        "    transforms=transforms, load_into_ram=load_into_ram)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=merge_fn, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "A1ys2cVTVLoo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "uyW7zbfRVahA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, validation, and evaluation.\n",
        "epochs = 10\n",
        "eval_every_n_epochs = 1\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} for computation.\")\n",
        "model.to(device)\n",
        "\n",
        "val_losses = []\n",
        "test_accs = []\n",
        "print(\"Start training.\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    val_loss = val_loop(val_dataloader, model, loss_fn, device)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if (epoch+1) % eval_every_n_epochs == 0:\n",
        "        acc = test_loop(test_dataloader, model, device)\n",
        "        test_accs.append(acc)\n",
        "val_losses = np.array(val_losses)\n",
        "test_accs = np.array(test_accs)\n",
        "print(f\"Minimum validation loss:{val_losses.min()} at {np.argmin(val_losses)+1} epoch.\")\n",
        "print(f\"Maximum accuracy:{test_accs.max()} at {np.argmin(test_accs)*eval_every_n_epochs+1} epoch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ol38Vb_VXwX",
        "outputId": "0f528497-55f8-44af-89de-91b9cdc4439f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu for computation.\n",
            "Start training.\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1\n",
            "Start training.\n",
            "loss:2.327837 [    0/ 3881]\n",
            "loss:2.299225 [ 3200/ 3881]\n",
            "Done. Time:15.036912306999994\n",
            "Start validation.\n",
            "Done. Time:0.9952567240000008\n",
            "Validation performance: \n",
            " Avg loss:2.304255\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:0.8790689000000214\n",
            "Test performance: \n",
            " Accuracy:12.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Start training.\n",
            "loss:2.297051 [    0/ 3881]\n",
            "loss:2.302500 [ 3200/ 3881]\n",
            "Done. Time:14.372072914\n",
            "Start validation.\n",
            "Done. Time:0.9947358640000061\n",
            "Validation performance: \n",
            " Avg loss:2.303029\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:0.8722371040000212\n",
            "Test performance: \n",
            " Accuracy:11.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Start training.\n",
            "loss:2.278781 [    0/ 3881]\n",
            "loss:2.297563 [ 3200/ 3881]\n",
            "Done. Time:14.64723844300002\n",
            "Start validation.\n",
            "Done. Time:0.9777030600000103\n",
            "Validation performance: \n",
            " Avg loss:2.301181\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:0.8898330359999989\n",
            "Test performance: \n",
            " Accuracy:11.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Start training.\n",
            "loss:2.269833 [    0/ 3881]\n",
            "loss:2.292092 [ 3200/ 3881]\n",
            "Done. Time:15.206554993999987\n",
            "Start validation.\n",
            "Done. Time:1.7043356190000054\n",
            "Validation performance: \n",
            " Avg loss:2.297754\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:1.3496165749999989\n",
            "Test performance: \n",
            " Accuracy:12.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Start training.\n",
            "loss:2.261648 [    0/ 3881]\n",
            "loss:2.286035 [ 3200/ 3881]\n",
            "Done. Time:14.408959385999992\n",
            "Start validation.\n",
            "Done. Time:0.9848449959999925\n",
            "Validation performance: \n",
            " Avg loss:2.286773\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:0.8978049389999967\n",
            "Test performance: \n",
            " Accuracy:14.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Start training.\n",
            "loss:2.252501 [    0/ 3881]\n",
            "loss:2.283591 [ 3200/ 3881]\n",
            "Done. Time:14.355513452999986\n",
            "Start validation.\n",
            "Done. Time:1.0085164080000197\n",
            "Validation performance: \n",
            " Avg loss:2.276987\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:0.8892808180000031\n",
            "Test performance: \n",
            " Accuracy:14.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Start training.\n",
            "loss:2.235267 [    0/ 3881]\n",
            "loss:2.278558 [ 3200/ 3881]\n",
            "Done. Time:14.360912597999999\n",
            "Start validation.\n",
            "Done. Time:1.0228356309999924\n",
            "Validation performance: \n",
            " Avg loss:2.263627\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:0.9211625920000017\n",
            "Test performance: \n",
            " Accuracy:16.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Start training.\n",
            "loss:2.225675 [    0/ 3881]\n",
            "loss:2.279137 [ 3200/ 3881]\n",
            "Done. Time:14.775577197000018\n",
            "Start validation.\n",
            "Done. Time:0.9909610290000046\n",
            "Validation performance: \n",
            " Avg loss:2.249405\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:0.921565883000028\n",
            "Test performance: \n",
            " Accuracy:18.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Start training.\n",
            "loss:2.213695 [    0/ 3881]\n",
            "loss:2.275609 [ 3200/ 3881]\n",
            "Done. Time:14.708239789000004\n",
            "Start validation.\n",
            "Done. Time:1.5293673219999846\n",
            "Validation performance: \n",
            " Avg loss:2.230494\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:1.3110202949999916\n",
            "Test performance: \n",
            " Accuracy:20.0%\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Start training.\n",
            "loss:2.201327 [    0/ 3881]\n",
            "loss:2.246720 [ 3200/ 3881]\n",
            "Done. Time:14.426871964000043\n",
            "Start validation.\n",
            "Done. Time:0.9862042870000209\n",
            "Validation performance: \n",
            " Avg loss:2.216677\n",
            "\n",
            "Start evaluation.\n",
            "Done. Time:0.8873461139999677\n",
            "Test performance: \n",
            " Accuracy:19.5%\n",
            "Minimum validation loss:2.216676643916539 at 10 epoch.\n",
            "Maximum accuracy:20.0 at 2 epoch.\n"
          ]
        }
      ]
    }
  ]
}