{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxYAqk9kE8hBBpFPwtBY/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takayama-rado/trado_samples/blob/main/colab_files/gafs_rnn_encoder_decoder_cslr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download dataset and modules"
      ],
      "metadata": {
        "id": "eQgl6G6nE7D0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADnfnnDUE0t1",
        "outputId": "432f9246-d3b6-41f4-96d6-9772268ce8da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to local.\n",
        "!cp ./drive/MyDrive/Datasets/gafs_dataset_very_small.zip gafs_dataset.zip"
      ],
      "metadata": {
        "id": "1qMzlk1ilsWk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o gafs_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnzLCqD8FR7T",
        "outputId": "33a21f91-96e9-4642-a48a-4a05b83b8598"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gafs_dataset.zip\n",
            "   creating: gafs_dataset_very_small/\n",
            "  inflating: gafs_dataset_very_small/0.hdf5  \n",
            "  inflating: gafs_dataset_very_small/1.hdf5  \n",
            "  inflating: gafs_dataset_very_small/10.hdf5  \n",
            "  inflating: gafs_dataset_very_small/102.hdf5  \n",
            "  inflating: gafs_dataset_very_small/105.hdf5  \n",
            "  inflating: gafs_dataset_very_small/107.hdf5  \n",
            "  inflating: gafs_dataset_very_small/109.hdf5  \n",
            "  inflating: gafs_dataset_very_small/112.hdf5  \n",
            "  inflating: gafs_dataset_very_small/113.hdf5  \n",
            "  inflating: gafs_dataset_very_small/115.hdf5  \n",
            "  inflating: gafs_dataset_very_small/117.hdf5  \n",
            "  inflating: gafs_dataset_very_small/121.hdf5  \n",
            "  inflating: gafs_dataset_very_small/122.hdf5  \n",
            "  inflating: gafs_dataset_very_small/125.hdf5  \n",
            "  inflating: gafs_dataset_very_small/128.hdf5  \n",
            "  inflating: gafs_dataset_very_small/13.hdf5  \n",
            "  inflating: gafs_dataset_very_small/135.hdf5  \n",
            "  inflating: gafs_dataset_very_small/136.hdf5  \n",
            "  inflating: gafs_dataset_very_small/138.hdf5  \n",
            "  inflating: gafs_dataset_very_small/141.hdf5  \n",
            "  inflating: gafs_dataset_very_small/143.hdf5  \n",
            "  inflating: gafs_dataset_very_small/145.hdf5  \n",
            "  inflating: gafs_dataset_very_small/147.hdf5  \n",
            "  inflating: gafs_dataset_very_small/15.hdf5  \n",
            "  inflating: gafs_dataset_very_small/151.hdf5  \n",
            "  inflating: gafs_dataset_very_small/153.hdf5  \n",
            "  inflating: gafs_dataset_very_small/154.hdf5  \n",
            "  inflating: gafs_dataset_very_small/157.hdf5  \n",
            "  inflating: gafs_dataset_very_small/158.hdf5  \n",
            "  inflating: gafs_dataset_very_small/160.hdf5  \n",
            "  inflating: gafs_dataset_very_small/161.hdf5  \n",
            "  inflating: gafs_dataset_very_small/168.hdf5  \n",
            "  inflating: gafs_dataset_very_small/169.hdf5  \n",
            "  inflating: gafs_dataset_very_small/171.hdf5  \n",
            "  inflating: gafs_dataset_very_small/176.hdf5  \n",
            "  inflating: gafs_dataset_very_small/178.hdf5  \n",
            "  inflating: gafs_dataset_very_small/18.hdf5  \n",
            "  inflating: gafs_dataset_very_small/181.hdf5  \n",
            "  inflating: gafs_dataset_very_small/186.hdf5  \n",
            "  inflating: gafs_dataset_very_small/187.hdf5  \n",
            "  inflating: gafs_dataset_very_small/188.hdf5  \n",
            "  inflating: gafs_dataset_very_small/192.hdf5  \n",
            "  inflating: gafs_dataset_very_small/196.hdf5  \n",
            "  inflating: gafs_dataset_very_small/2.hdf5  \n",
            "  inflating: gafs_dataset_very_small/20.hdf5  \n",
            "  inflating: gafs_dataset_very_small/202.hdf5  \n",
            "  inflating: gafs_dataset_very_small/203.hdf5  \n",
            "  inflating: gafs_dataset_very_small/21.hdf5  \n",
            "  inflating: gafs_dataset_very_small/216.hdf5  \n",
            "  inflating: gafs_dataset_very_small/217.hdf5  \n",
            "  inflating: gafs_dataset_very_small/219.hdf5  \n",
            "  inflating: gafs_dataset_very_small/223.hdf5  \n",
            "  inflating: gafs_dataset_very_small/225.hdf5  \n",
            "  inflating: gafs_dataset_very_small/227.hdf5  \n",
            "  inflating: gafs_dataset_very_small/230.hdf5  \n",
            "  inflating: gafs_dataset_very_small/231.hdf5  \n",
            "  inflating: gafs_dataset_very_small/233.hdf5  \n",
            "  inflating: gafs_dataset_very_small/236.hdf5  \n",
            "  inflating: gafs_dataset_very_small/239.hdf5  \n",
            "  inflating: gafs_dataset_very_small/24.hdf5  \n",
            "  inflating: gafs_dataset_very_small/241.hdf5  \n",
            "  inflating: gafs_dataset_very_small/242.hdf5  \n",
            "  inflating: gafs_dataset_very_small/246.hdf5  \n",
            "  inflating: gafs_dataset_very_small/25.hdf5  \n",
            "  inflating: gafs_dataset_very_small/251.hdf5  \n",
            "  inflating: gafs_dataset_very_small/254.hdf5  \n",
            "  inflating: gafs_dataset_very_small/27.hdf5  \n",
            "  inflating: gafs_dataset_very_small/36.hdf5  \n",
            "  inflating: gafs_dataset_very_small/38.hdf5  \n",
            "  inflating: gafs_dataset_very_small/4.hdf5  \n",
            "  inflating: gafs_dataset_very_small/40.hdf5  \n",
            "  inflating: gafs_dataset_very_small/43.hdf5  \n",
            "  inflating: gafs_dataset_very_small/53.hdf5  \n",
            "  inflating: gafs_dataset_very_small/56.hdf5  \n",
            "  inflating: gafs_dataset_very_small/59.hdf5  \n",
            "  inflating: gafs_dataset_very_small/6.hdf5  \n",
            "  inflating: gafs_dataset_very_small/63.hdf5  \n",
            "  inflating: gafs_dataset_very_small/68.hdf5  \n",
            "  inflating: gafs_dataset_very_small/70.hdf5  \n",
            "  inflating: gafs_dataset_very_small/71.hdf5  \n",
            "  inflating: gafs_dataset_very_small/72.hdf5  \n",
            "  inflating: gafs_dataset_very_small/73.hdf5  \n",
            "  inflating: gafs_dataset_very_small/74.hdf5  \n",
            "  inflating: gafs_dataset_very_small/76.hdf5  \n",
            "  inflating: gafs_dataset_very_small/80.hdf5  \n",
            "  inflating: gafs_dataset_very_small/81.hdf5  \n",
            "  inflating: gafs_dataset_very_small/88.hdf5  \n",
            "  inflating: gafs_dataset_very_small/89.hdf5  \n",
            "  inflating: gafs_dataset_very_small/9.hdf5  \n",
            "  inflating: gafs_dataset_very_small/92.hdf5  \n",
            "  inflating: gafs_dataset_very_small/93.hdf5  \n",
            "  inflating: gafs_dataset_very_small/95.hdf5  \n",
            "  inflating: gafs_dataset_very_small/character_to_prediction_index.json  \n",
            "  inflating: gafs_dataset_very_small/LICENSE.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gafs_dataset_very_small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ZWwV56FWVD",
        "outputId": "c2661506-8131-4771-824b-67e494f4a60c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.hdf5\t  135.hdf5  160.hdf5  1.hdf5\t236.hdf5  43.hdf5  80.hdf5\n",
            "102.hdf5  136.hdf5  161.hdf5  202.hdf5\t239.hdf5  4.hdf5   81.hdf5\n",
            "105.hdf5  138.hdf5  168.hdf5  203.hdf5\t241.hdf5  53.hdf5  88.hdf5\n",
            "107.hdf5  13.hdf5   169.hdf5  20.hdf5\t242.hdf5  56.hdf5  89.hdf5\n",
            "109.hdf5  141.hdf5  171.hdf5  216.hdf5\t246.hdf5  59.hdf5  92.hdf5\n",
            "10.hdf5   143.hdf5  176.hdf5  217.hdf5\t24.hdf5   63.hdf5  93.hdf5\n",
            "112.hdf5  145.hdf5  178.hdf5  219.hdf5\t251.hdf5  68.hdf5  95.hdf5\n",
            "113.hdf5  147.hdf5  181.hdf5  21.hdf5\t254.hdf5  6.hdf5   9.hdf5\n",
            "115.hdf5  151.hdf5  186.hdf5  223.hdf5\t25.hdf5   70.hdf5  character_to_prediction_index.json\n",
            "117.hdf5  153.hdf5  187.hdf5  225.hdf5\t27.hdf5   71.hdf5  LICENSE.txt\n",
            "121.hdf5  154.hdf5  188.hdf5  227.hdf5\t2.hdf5\t  72.hdf5\n",
            "122.hdf5  157.hdf5  18.hdf5   230.hdf5\t36.hdf5   73.hdf5\n",
            "125.hdf5  158.hdf5  192.hdf5  231.hdf5\t38.hdf5   74.hdf5\n",
            "128.hdf5  15.hdf5   196.hdf5  233.hdf5\t40.hdf5   76.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gafs_dataset_very_small/character_to_prediction_index.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLP9RpORFaLL",
        "outputId": "542b6986-67b8-4a5a-ef0c-33e4e375f84e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\" \":0,\"!\":1,\"#\":2,\"$\":3,\"%\":4,\"&\":5,\"'\":6,\"(\":7,\")\":8,\"*\":9,\"+\":10,\",\":11,\"-\":12,\".\":13,\"\\/\":14,\"0\":15,\"1\":16,\"2\":17,\"3\":18,\"4\":19,\"5\":20,\"6\":21,\"7\":22,\"8\":23,\"9\":24,\":\":25,\";\":26,\"=\":27,\"?\":28,\"@\":29,\"[\":30,\"_\":31,\"a\":32,\"b\":33,\"c\":34,\"d\":35,\"e\":36,\"f\":37,\"g\":38,\"h\":39,\"i\":40,\"j\":41,\"k\":42,\"l\":43,\"m\":44,\"n\":45,\"o\":46,\"p\":47,\"q\":48,\"r\":49,\"s\":50,\"t\":51,\"u\":52,\"v\":53,\"w\":54,\"x\":55,\"y\":56,\"z\":57,\"~\":58}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gafs_dataset_very_small/LICENSE.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iQlJCDtU4d8",
        "outputId": "3922c4cb-5868-4717-8785-54dd3ecbf089"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset provided by Natsuki Takayama (Takayama Research and Development Office) is licensed under CC-BY 4.0.\r\n",
            "Author: Copyright 2024 Natsuki Takayama\r\n",
            "Title: GASF very small dataset\r\n",
            "Original licenser: Google LLC\r\n",
            "Modification\r\n",
            "- Extract only 3 parquet file.\r\n",
            "- Packaged into HDF5 format.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "with h5py.File(\"gafs_dataset_very_small/0.hdf5\", \"r\") as fread:\n",
        "    keys = list(fread.keys())\n",
        "    print(keys[:10])\n",
        "    group = fread[keys[0]]\n",
        "    print(group.keys())\n",
        "    feature = group[\"feature\"][:]\n",
        "    token = group[\"token\"][:]\n",
        "    print(feature.shape)\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37SBiPQeVSB7",
        "outputId": "a3334772-a08d-4235-fafb-0c081b8dd8c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1720198121', '1722303176', '1723157122', '1731934631', '1737624109', '1739256200', '1743069372', '1743412187', '1744795751', '1746320345']\n",
            "<KeysViewHDF5 ['feature', 'token']>\n",
            "(2, 271, 543)\n",
            "[14 38 32 45 44 36 40 32 43 43 36 56 14 43 40 45 32 12 34 32 49 50 51 36\n",
            " 45 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/takayama-rado/trado_samples/archive/refs/tags/v0.3.4.zip -O master.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjcUxrq2VmlE",
        "outputId": "ea21b23c-6e5c-4a14-a149-9521e049eed3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-23 02:18:45--  https://github.com/takayama-rado/trado_samples/archive/refs/tags/v0.3.4.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/takayama-rado/trado_samples/zip/refs/tags/v0.3.4 [following]\n",
            "--2024-09-23 02:18:45--  https://codeload.github.com/takayama-rado/trado_samples/zip/refs/tags/v0.3.4\n",
            "Resolving codeload.github.com (codeload.github.com)... 20.205.243.165\n",
            "Connecting to codeload.github.com (codeload.github.com)|20.205.243.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [        <=>         ]  76.54M  14.8MB/s    in 6.0s    \n",
            "\n",
            "2024-09-23 02:18:51 (12.7 MB/s) - ‘master.zip’ saved [80254068]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o master.zip -d master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUTwKBOMVw-j",
        "outputId": "9c8c2285-b106-40e2-c740-64f34a81bc53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  master.zip\n",
            "3406d5a0072e08879272e622ff8efdc1c7b78ee8\n",
            "   creating: master/trado_samples-0.3.4/\n",
            "  inflating: master/trado_samples-0.3.4/.gitignore  \n",
            "  inflating: master/trado_samples-0.3.4/LICENSE  \n",
            "  inflating: master/trado_samples-0.3.4/README.md  \n",
            "   creating: master/trado_samples-0.3.4/colab_files/\n",
            " extracting: master/trado_samples-0.3.4/colab_files/.gitkeep  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_affine_np_einsum.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_jax_static.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_mpholistic_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_mpothers_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_jax.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_numpy.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_torch.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_jax.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_numpy_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_numpy_1.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_torch.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gafs_create_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_access_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_conformer_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_create_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_macaronnet_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_normalize_landmarks.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_1.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_2.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_3.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_select_landmarks.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_simple_attention.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_simple_islr_model.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_drop.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_hflip.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_resize.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_saffine.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_snoise.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_tclip.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_tinterp.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_twarping.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_w_ls.ipynb  \n",
            "   creating: master/trado_samples-0.3.4/src/\n",
            "   creating: master/trado_samples-0.3.4/src/modules_gislr/\n",
            " extracting: master/trado_samples-0.3.4/src/modules_gislr/__init__.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/activation.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/dataset.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/defines.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/draw_functions.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/layers.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/train_functions.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/transforms.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/utils.py  \n",
            "   creating: master/trado_samples-0.3.4/test_data/\n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static_affine.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static_interp.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_middle0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_near0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/hand_only.mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv master/trado_samples-0.3.4/src/modules_gislr ."
      ],
      "metadata": {
        "id": "yXsIhVAWVyej"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf master master.zip gafs_dataset_very_small.zip"
      ],
      "metadata": {
        "id": "ohykNs7zV3TL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koeHiooRV7qr",
        "outputId": "53c680fa-565e-4fc9-e2b2-c0bf3a8edbce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  gafs_dataset_very_small\tgafs_dataset.zip  modules_gislr  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load library"
      ],
      "metadata": {
        "id": "ddZ2NhLDV8yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "# Third party's modules\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import (\n",
        "    DataLoader)\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "# Local modules\n",
        "sys.path.append(\"modules_gislr\")\n",
        "from modules_gislr.dataset import (\n",
        "    HDF5Dataset,\n",
        "    merge)\n",
        "from modules_gislr.defines import (\n",
        "    get_fullbody_landmarks\n",
        ")\n",
        "from modules_gislr.layers import (\n",
        "    RNNEncoder,\n",
        "    apply_norm,\n",
        "    create_norm\n",
        ")\n",
        "from modules_gislr.train_functions import (\n",
        "    LabelSmoothingCrossEntropyLoss\n",
        ")\n",
        "from modules_gislr.transforms import (\n",
        "    PartsBasedNormalization,\n",
        "    ReplaceNan,\n",
        "    SelectLandmarksAndFeature,\n",
        "    ToTensor\n",
        ")\n",
        "from modules_gislr.utils import (\n",
        "    select_reluwise_activation\n",
        ")"
      ],
      "metadata": {
        "id": "8K-wtRChV-n7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implement RNN Encoder-Decoder CSLR model"
      ],
      "metadata": {
        "id": "Klup1x0iWlHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention module"
      ],
      "metadata": {
        "id": "aPHXLRYMXE9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttentionEnergy(nn.Module):\n",
        "    def __init__(self,\n",
        "                 key_dim,\n",
        "                 query_dim,\n",
        "                 att_dim,\n",
        "                 add_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_key = nn.Linear(key_dim, att_dim, bias=add_bias)\n",
        "        self.w_query = nn.Linear(query_dim, att_dim, bias=add_bias)\n",
        "        self.w_out = nn.Linear(att_dim, 1, bias=add_bias)\n",
        "\n",
        "    def forward(self, key, query):\n",
        "        # key: `[N, key_len, key_dim]`\n",
        "        # query: `[N, 1, query_dim]`\n",
        "        key = self.w_key(key)\n",
        "        query = self.w_query(query)\n",
        "        # Adding with broadcasting.\n",
        "        # key: `[N, key_len, key_dim]`\n",
        "        # query: `[N, 1, query_dim]`\n",
        "        # query should be broadcasted to `[N, key_len, query_dim]`\n",
        "        temp = key + query\n",
        "        # `[N, key_len, att_dim] -> [N, key_len, 1] -> [N, 1, key_len]`\n",
        "        energy = self.w_out(torch.tanh(temp))\n",
        "        energy = torch.permute(energy, [0, 2, 1])\n",
        "        return energy"
      ],
      "metadata": {
        "id": "9AClEx01WqmU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 key_dim,\n",
        "                 query_dim,\n",
        "                 att_dim,\n",
        "                 add_bias):\n",
        "        super().__init__()\n",
        "\n",
        "        self.att_energy = BahdanauAttentionEnergy(\n",
        "            key_dim=key_dim,\n",
        "            query_dim=query_dim,\n",
        "            att_dim=att_dim,\n",
        "            add_bias=add_bias)\n",
        "\n",
        "        self.neg_inf = None\n",
        "\n",
        "    def forward(self,\n",
        "                key,\n",
        "                value,\n",
        "                query,\n",
        "                mask=None):\n",
        "        if self.neg_inf is None:\n",
        "            self.neg_inf = float(np.finfo(\n",
        "                torch.tensor(0, dtype=key.dtype).numpy().dtype).min)\n",
        "\n",
        "        batch, klen, kdim = key.shape\n",
        "        _, qlen, qdim = query.shape\n",
        "        energy = self.att_energy(key=key, query=query)\n",
        "        assert energy.shape == (batch, qlen, klen)\n",
        "\n",
        "        # Apply mask.\n",
        "        if mask is not None:\n",
        "            if len(mask.shape) == 2:\n",
        "                # `[N, klen] -> [N, qlen(=1), klen]`\n",
        "                mask = mask.unsqueeze(1)\n",
        "            # Negative infinity should be 0 in softmax.\n",
        "            energy = energy.masked_fill_(mask==0, self.neg_inf)\n",
        "\n",
        "        # Compute attention mask.\n",
        "        attw = torch.softmax(energy, dim=-1)\n",
        "        # attw: `[N, qlen, klen]`\n",
        "        # value: `[N, klen, kdim]`\n",
        "        # bmm: `[N, qlen, klen] x [N, klen, kdim] -> [N, qlen, kdim]`\n",
        "        cvec = torch.bmm(attw, value)\n",
        "        return cvec, attw"
      ],
      "metadata": {
        "id": "VEy2_ljtXNEz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Decoder"
      ],
      "metadata": {
        "id": "IRNyq0OkX8Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauRNNDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 hidden_channels,\n",
        "                 out_channels,\n",
        "                 emb_channels,\n",
        "                 att_dim,\n",
        "                 att_add_bias,\n",
        "                 rnn_type,\n",
        "                 num_layers,\n",
        "                 activation,\n",
        "                 dropout,\n",
        "                 padding_val,\n",
        "                 proj_size=0):\n",
        "        super().__init__()\n",
        "        assert rnn_type in [\"srnn\", \"lstm\", \"gru\"]\n",
        "\n",
        "        self.emb_layer = nn.Embedding(\n",
        "            num_embeddings=out_channels,\n",
        "            embedding_dim=emb_channels,\n",
        "            padding_idx=padding_val)\n",
        "\n",
        "        self.att_layer = SingleHeadAttention(\n",
        "            key_dim=in_channels,\n",
        "            query_dim=hidden_channels,\n",
        "            att_dim=att_dim,\n",
        "            add_bias=att_add_bias)\n",
        "\n",
        "        if rnn_type == \"srnn\":\n",
        "            self.rnn = nn.RNN(input_size=in_channels + emb_channels,\n",
        "                              hidden_size=hidden_channels,\n",
        "                              num_layers=num_layers,\n",
        "                              nonlinearity=activation,\n",
        "                              batch_first=True,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=False)\n",
        "        elif rnn_type == \"lstm\":\n",
        "            self.rnn = nn.LSTM(input_size=in_channels + emb_channels,\n",
        "                               hidden_size=hidden_channels,\n",
        "                               num_layers=num_layers,\n",
        "                               batch_first=True,\n",
        "                               dropout=dropout,\n",
        "                               bidirectional=False,\n",
        "                               proj_size=proj_size)\n",
        "        elif rnn_type == \"gru\":\n",
        "            self.rnn = nn.GRU(input_size=in_channels + emb_channels,\n",
        "                              hidden_size=hidden_channels,\n",
        "                              num_layers=num_layers,\n",
        "                              batch_first=True,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=False)\n",
        "        self.head = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.dec_hstate = None\n",
        "        self.attw = None\n",
        "\n",
        "        self.reset_parameters(emb_channels, padding_val)\n",
        "\n",
        "    def reset_parameters(self, embedding_dim, padding_val):\n",
        "        # Bellow initialization has strong effect to performance.\n",
        "        # Please refer.\n",
        "        # https://github.com/facebookresearch/fairseq/blob/main/fairseq/models/transformer/transformer_base.py#L189\n",
        "        nn.init.normal_(self.emb_layer.weight, mean=0, std=embedding_dim**-0.5)\n",
        "        nn.init.constant_(self.emb_layer.weight[padding_val], 0)\n",
        "\n",
        "        # Please refer.\n",
        "        # https://github.com/facebookresearch/fairseq/blob/main/fairseq/models/transformer/transformer_decoder.py\n",
        "        nn.init.xavier_uniform_(self.head.weight)\n",
        "        nn.init.constant_(self.head.bias, 0.0)\n",
        "\n",
        "    def init_dec_hstate(self, enc_hstate, init_as_zero=False):\n",
        "        if init_as_zero:\n",
        "            dec_hstate = torch.zeros_like(enc_hstate)\n",
        "        else:\n",
        "            dec_hstate = enc_hstate\n",
        "        # To avoid error at RNN layer.\n",
        "        self.dec_hstate = dec_hstate.contiguous()\n",
        "\n",
        "    def forward(self,\n",
        "                dec_inputs,\n",
        "                enc_seqs,\n",
        "                enc_mask):\n",
        "        assert self.dec_hstate is not None, f\"dec_hstate has not been initialized.\"\n",
        "        dec_hstate = self.dec_hstate\n",
        "\n",
        "        # Attention layer requires hidden state of 2nd rnn layer.\n",
        "        # as `[N, 1, C]`\n",
        "        query = dec_hstate[-1].unsqueeze(1)\n",
        "        cvec, self.attw = self.att_layer(\n",
        "            key=enc_seqs,\n",
        "            value=enc_seqs,\n",
        "            query=query,\n",
        "            mask=enc_mask)\n",
        "\n",
        "        emb_out = self.emb_layer(dec_inputs)\n",
        "        # `[N, C] -> [N, 1, C]`\n",
        "        emb_out = emb_out.reshape([-1, 1, emb_out.shape[-1]])\n",
        "        feature = torch.cat([cvec, emb_out], dim=-1)\n",
        "        if isinstance(self.rnn, nn.LSTM):\n",
        "            hidden_seqs, (last_hstate, last_cstate) = self.rnn(feature,\n",
        "                                                               dec_hstate)\n",
        "        else:\n",
        "            hidden_seqs, last_hstate = self.rnn(feature,\n",
        "                                                dec_hstate)\n",
        "            last_cstate = None\n",
        "\n",
        "        output_dec = self.head(hidden_seqs)\n",
        "        self.dec_hstate = last_hstate\n",
        "        return output_dec"
      ],
      "metadata": {
        "id": "UZ4Me3ZRX-GL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN CSLR model"
      ],
      "metadata": {
        "id": "IvVTsyTsYPOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCSLR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_in_channels,\n",
        "                 enc_hidden_channels,\n",
        "                 enc_rnn_type,\n",
        "                 enc_num_layers,\n",
        "                 enc_activation,\n",
        "                 enc_bidir,\n",
        "                 enc_dropout,\n",
        "                 enc_apply_mask,\n",
        "                 enc_proj_size,\n",
        "                 dec_in_channels,\n",
        "                 dec_hidden_channels,\n",
        "                 dec_out_channels,\n",
        "                 dec_emb_channels,\n",
        "                 dec_att_dim,\n",
        "                 dec_att_add_bias,\n",
        "                 dec_rnn_type,\n",
        "                 dec_num_layers,\n",
        "                 dec_activation,\n",
        "                 dec_dropout,\n",
        "                 dec_padding_val,\n",
        "                 dec_proj_size):\n",
        "        super().__init__()\n",
        "        self.enc_bidir = enc_bidir\n",
        "\n",
        "        self.linear = nn.Linear(enc_in_channels, enc_hidden_channels)\n",
        "        self.enc_activation = nn.ReLU()\n",
        "\n",
        "        self.encoder = RNNEncoder(\n",
        "            in_channels=enc_hidden_channels,\n",
        "            out_channels=enc_hidden_channels,\n",
        "            rnn_type=enc_rnn_type,\n",
        "            num_layers=enc_num_layers,\n",
        "            activation=enc_activation,\n",
        "            bidir=enc_bidir,\n",
        "            dropout=enc_dropout,\n",
        "            apply_mask=enc_apply_mask,\n",
        "            proj_size=enc_proj_size)\n",
        "\n",
        "        if enc_bidir:\n",
        "            dec_in_channels *= 2\n",
        "            dec_hidden_channels *= 2\n",
        "            dec_att_dim *= 2\n",
        "\n",
        "        self.decoder = BahdanauRNNDecoder(\n",
        "            in_channels=dec_in_channels,\n",
        "            hidden_channels=dec_hidden_channels,\n",
        "            out_channels=dec_out_channels,\n",
        "            emb_channels=dec_emb_channels,\n",
        "            att_dim=dec_att_dim,\n",
        "            att_add_bias=dec_att_add_bias,\n",
        "            rnn_type=dec_rnn_type,\n",
        "            num_layers=dec_num_layers,\n",
        "            activation=dec_activation,\n",
        "            dropout=dec_dropout,\n",
        "            padding_val=dec_padding_val,\n",
        "            proj_size=dec_proj_size)\n",
        "\n",
        "    def _apply_encoder(self, feature, feature_pad_mask=None):\n",
        "        # Feature extraction.\n",
        "        # `[N, C, T, J] -> [N, T, C, J] -> [N, T, C*J] -> [N, T, C']`\n",
        "        N, C, T, J = feature.shape\n",
        "        feature = feature.permute([0, 2, 1, 3])\n",
        "        feature = feature.reshape(N, T, -1)\n",
        "\n",
        "        feature = self.linear(feature)\n",
        "        feature = self.enc_activation(feature)\n",
        "\n",
        "        # Apply encoder.\n",
        "        enc_seqs, enc_hstate = self.encoder(feature, feature_pad_mask)[:2]\n",
        "\n",
        "        # Basically, decoder should not be bidirectional.\n",
        "        # So, we should concatenate backwarded feature.\n",
        "        if self.enc_bidir:\n",
        "            # `[2*layers, N, C] -> [layers, N, 2*C]`\n",
        "            enc_hstate = torch.permute(enc_hstate, [1, 0, 2])\n",
        "            enc_hstate = enc_hstate.reshape([enc_hstate.shape[0],\n",
        "                                             enc_hstate.shape[1] // 2,\n",
        "                                             -1])\n",
        "            enc_hstate = torch.permute(enc_hstate, [1, 0, 2])\n",
        "        return enc_seqs, enc_hstate\n",
        "\n",
        "    def forward(self,\n",
        "                feature, tokens,\n",
        "                feature_pad_mask=None, tokens_pad_mask=None):\n",
        "        \"\"\"Forward computation for train.\n",
        "        \"\"\"\n",
        "        enc_seqs, enc_hstate = self._apply_encoder(feature, feature_pad_mask)\n",
        "\n",
        "        # Apply decoder.\n",
        "        self.decoder.init_dec_hstate(enc_hstate)\n",
        "        preds = None\n",
        "        for t_index in range(0, tokens.shape[-1]):\n",
        "            # Teacher forcing.\n",
        "            dec_inputs = tokens[:, t_index].reshape([-1, 1])\n",
        "            pred = self.decoder(\n",
        "                dec_inputs=dec_inputs,\n",
        "                enc_seqs=enc_seqs,\n",
        "                enc_mask=feature_pad_mask)\n",
        "            if preds is None:\n",
        "                preds = pred\n",
        "            else:\n",
        "                # `[N, T, C]`\n",
        "                preds = torch.cat([preds, pred], dim=1)\n",
        "        return preds\n",
        "\n",
        "    def inference(self,\n",
        "                  feature,\n",
        "                  start_id,\n",
        "                  end_id,\n",
        "                  feature_pad_mask=None,\n",
        "                  max_seqlen=62):\n",
        "        \"\"\"Forward computation for test.\n",
        "        \"\"\"\n",
        "        enc_seqs, enc_hstate = self._apply_encoder(feature, feature_pad_mask)\n",
        "\n",
        "        # Apply decoder.\n",
        "        self.decoder.init_dec_hstate(enc_hstate)\n",
        "        dec_inputs = torch.tensor([start_id]).to(feature.device)\n",
        "        # `[N, T]`\n",
        "        dec_inputs = dec_inputs.reshape([1, 1])\n",
        "        preds = None\n",
        "        pred_ids = [start_id]\n",
        "        for _ in range(max_seqlen):\n",
        "            pred = self.decoder(\n",
        "                dec_inputs=dec_inputs,\n",
        "                enc_seqs=enc_seqs,\n",
        "                enc_mask=feature_pad_mask)\n",
        "            if preds is None:\n",
        "                preds = pred\n",
        "            else:\n",
        "                # `[N, T, C]`\n",
        "                preds = torch.cat([preds, pred], dim=1)\n",
        "\n",
        "            pid = torch.argmax(pred, dim=-1)\n",
        "            dec_inputs = pid\n",
        "\n",
        "            pid = pid.reshape([1]).detach().cpu().numpy()[0]\n",
        "            pred_ids.append(int(pid))\n",
        "            if int(pid) == end_id:\n",
        "                break\n",
        "\n",
        "        # `[N, T]`\n",
        "        pred_ids = np.array([pred_ids])\n",
        "        return pred_ids, preds"
      ],
      "metadata": {
        "id": "QHuAwB3VYRfE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Update to train CSLR model"
      ],
      "metadata": {
        "id": "6vRfzdWzaRFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add transformation to insert keywords"
      ],
      "metadata": {
        "id": "ho8wlf3UcNCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InsertTokensForS2S():\n",
        "    def __init__(self,\n",
        "                 sos_token,\n",
        "                 eos_token,\n",
        "                 error_at_exist=False):\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.error_at_exist = error_at_exist\n",
        "\n",
        "    def check_format(self, tokens):\n",
        "        insert_sos = False\n",
        "        if tokens[0] != self.sos_token:\n",
        "            insert_sos = True\n",
        "        elif self.error_at_exist:\n",
        "            message = f\"The sos_token:{self.sos_token} is exist in {tokens}.\" \\\n",
        "                + \"Please check the format.\"\n",
        "            raise ValueError(message)\n",
        "        insert_eos = False\n",
        "        if tokens[-1] != self.eos_token:\n",
        "            insert_eos = True\n",
        "        elif self.error_at_exist:\n",
        "            message = f\"The eos_token:{self.eos_token} is exist in {tokens}.\" \\\n",
        "                + \"Please check the format.\"\n",
        "            raise ValueError(message)\n",
        "        return insert_sos, insert_eos\n",
        "\n",
        "    def __call__(self, data):\n",
        "\n",
        "        tokens = data[\"token\"]\n",
        "        dtype = tokens.dtype\n",
        "\n",
        "        insert_sos, insert_eos = self.check_format(tokens)\n",
        "        # Insert.\n",
        "        new_tokens = []\n",
        "        if insert_sos:\n",
        "            new_tokens.append(self.sos_token)\n",
        "        new_tokens += tokens.tolist()\n",
        "        if insert_eos:\n",
        "            new_tokens.append(self.eos_token)\n",
        "        new_tokens = np.array(new_tokens, dtype=dtype)\n",
        "        data[\"token\"] = new_tokens\n",
        "        return data"
      ],
      "metadata": {
        "id": "5A9SURRYaW9d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update merge function to create padded token"
      ],
      "metadata": {
        "id": "pOioAkXmgg2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_padded_batch(batch,\n",
        "                       feature_shape,\n",
        "                       token_shape,\n",
        "                       feature_padding_val=0,\n",
        "                       token_padding_val=0):\n",
        "    feature_batch = [sample[\"feature\"] for sample in batch]\n",
        "    token_batch = [sample[\"token\"] for sample in batch]\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge feature.\n",
        "    # ==========================================================\n",
        "    # `[B, C, T, J]`\n",
        "    merged_shape = [len(batch), *feature_shape]\n",
        "    # Use maximum frame length in a batch as padded length.\n",
        "    if merged_shape[2] == -1:\n",
        "        tlen = max([feature.shape[1] for feature in feature_batch])\n",
        "        merged_shape[2] = tlen\n",
        "    merged_feature = merge(feature_batch, merged_shape, padding_val=feature_padding_val)\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge tocken.\n",
        "    # ==========================================================\n",
        "    # `[B, L]`\n",
        "    merged_shape = [len(batch), *token_shape]\n",
        "    # Use maximum token length in a batch as padded length.\n",
        "    if merged_shape[1] == -1:\n",
        "        tlen = max([token.shape[0] for token in token_batch])\n",
        "        merged_shape[1] = tlen\n",
        "    merged_token = merge(token_batch, merged_shape, padding_val=token_padding_val)\n",
        "\n",
        "    # Generate padding mask.\n",
        "    # Pad: 0, Signal: 1\n",
        "    # The frames which all channels and landmarks are equals to padding value\n",
        "    # should be padded.\n",
        "    feature_pad_mask = merged_feature == feature_padding_val\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=1)\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=-1)\n",
        "    feature_pad_mask = torch.logical_not(feature_pad_mask)\n",
        "    token_pad_mask = torch.logical_not(merged_token == token_padding_val)\n",
        "\n",
        "    retval = {\n",
        "        \"feature\": merged_feature,\n",
        "        \"token\": merged_token,\n",
        "        \"feature_pad_mask\": feature_pad_mask,\n",
        "        \"token_pad_mask\": token_pad_mask}\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Lw2jeDOTgtbc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update train, validation, and test loops"
      ],
      "metadata": {
        "id": "CmD69vQahEMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(model, feature, tokens, feature_pad_mask, tokens_pad_mask):\n",
        "    if isinstance(model, RNNCSLR):\n",
        "        preds = model(feature,\n",
        "                      tokens,\n",
        "                      feature_pad_mask=feature_pad_mask,\n",
        "                      tokens_pad_mask=tokens_pad_mask)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model type:{type(model)}.\")\n",
        "    return preds"
      ],
      "metadata": {
        "id": "r-CoDrCOhKy8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, feature, start_id, end_id, max_seqlen=62):\n",
        "    if isinstance(model, RNNCSLR):\n",
        "        pred_ids, _ = model.inference(feature,\n",
        "                                      start_id,\n",
        "                                      end_id,\n",
        "                                      max_seqlen=max_seqlen)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model type:{type(model)}.\")\n",
        "    return pred_ids"
      ],
      "metadata": {
        "id": "CryTqBaihbgE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_tokens_format(tokens, tokens_pad_mask, start_id, end_id):\n",
        "    # Check token's format.\n",
        "    end_indices0 = np.arange(len(tokens))\n",
        "    end_indices1 = tokens_pad_mask.sum(dim=-1).detach().cpu().numpy() - 1\n",
        "    message = \"The start and/or end ids are not included in tokens. \" \\\n",
        "        f\"Please check data format. start_id:{start_id}, \" \\\n",
        "        f\"end_id:{end_id}, enc_indices:{end_indices1}, tokens:{tokens}\"\n",
        "    ref_tokens = tokens.detach().cpu().numpy()\n",
        "    assert (ref_tokens[:, 0] == start_id).all(), message\n",
        "    assert (ref_tokens[end_indices0, end_indices1] == end_id).all(), message"
      ],
      "metadata": {
        "id": "Rchi8Hx2hZCs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_csir_s2s(dataloader,\n",
        "                        model,\n",
        "                        loss_fn,\n",
        "                        optimizer,\n",
        "                        device,\n",
        "                        start_id,\n",
        "                        end_id,\n",
        "                        return_pred_times=False):\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss = 0\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to training mode.\n",
        "    model.train()\n",
        "    # Main loop.\n",
        "    print(\"Start training.\")\n",
        "    start = time.perf_counter()\n",
        "    for batch_idx, batch_sample in enumerate(dataloader):\n",
        "        feature = batch_sample[\"feature\"]\n",
        "        feature_pad_mask = batch_sample[\"feature_pad_mask\"]\n",
        "        tokens = batch_sample[\"token\"]\n",
        "        tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "        check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "        feature = feature.to(device)\n",
        "        feature_pad_mask = feature_pad_mask.to(device)\n",
        "        tokens = tokens.to(device)\n",
        "        tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "        frames = feature.shape[-2]\n",
        "\n",
        "        # Predict.\n",
        "        pred_start = time.perf_counter()\n",
        "        preds = forward(model, feature, tokens,\n",
        "                        feature_pad_mask, tokens_pad_mask)\n",
        "        pred_end = time.perf_counter()\n",
        "        pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "        # Compute loss.\n",
        "        # Preds do not include <start>, so skip that of tokens.\n",
        "        loss = 0\n",
        "        if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "            for t_index in range(1, tokens.shape[-1]):\n",
        "                pred = preds[:, t_index-1, :]\n",
        "                token = tokens[:, t_index]\n",
        "                loss += loss_fn(pred, token)\n",
        "            loss /= tokens.shape[-1]\n",
        "        # LabelSmoothingCrossEntropyLoss\n",
        "        else:\n",
        "            # `[N, T, C] -> [N, C, T]`\n",
        "            preds = preds.permute([0, 2, 1])\n",
        "            # Remove prediction after the last token.\n",
        "            if preds.shape[-1] == tokens.shape[-1]:\n",
        "                preds = preds[:, :, :-1]\n",
        "            loss = loss_fn(preds, tokens[:, 1:])\n",
        "\n",
        "        # Back propagation.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Print current loss per 100 steps.\n",
        "        if batch_idx % 100 == 0:\n",
        "            loss = loss.item()\n",
        "            steps = batch_idx * len(feature)\n",
        "            print(f\"loss:{loss:>7f} [{steps:>5d}/{size:>5d}]\")\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "    # Average loss.\n",
        "    train_loss /= num_batches\n",
        "    print(\"Training performance: \\n\",\n",
        "          f\"Avg loss:{train_loss:>8f}\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (train_loss, pred_times) if return_pred_times else train_loss\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Fd3z-wjohe1d"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_loop_csir_s2s(dataloader,\n",
        "                      model,\n",
        "                      loss_fn,\n",
        "                      device,\n",
        "                      start_id,\n",
        "                      end_id,\n",
        "                      return_pred_times=False):\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss = 0\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to evaluation mode.\n",
        "    model.eval()\n",
        "    # Main loop.\n",
        "    print(\"Start validation.\")\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_sample in enumerate(dataloader):\n",
        "            feature = batch_sample[\"feature\"]\n",
        "            feature_pad_mask = batch_sample[\"feature_pad_mask\"]\n",
        "            tokens = batch_sample[\"token\"]\n",
        "            tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "            check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "            feature = feature.to(device)\n",
        "            feature_pad_mask = feature_pad_mask.to(device)\n",
        "            tokens = tokens.to(device)\n",
        "            tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "            frames = feature.shape[-2]\n",
        "\n",
        "            # Predict.\n",
        "            pred_start = time.perf_counter()\n",
        "            preds = forward(model, feature, tokens,\n",
        "                            feature_pad_mask, tokens_pad_mask)\n",
        "            pred_end = time.perf_counter()\n",
        "            pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "            # Compute loss.\n",
        "            # Preds do not include <start>, so skip that of tokens.\n",
        "            loss = 0\n",
        "            if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "                for t_index in range(1, tokens.shape[-1]):\n",
        "                    pred = preds[:, t_index-1, :]\n",
        "                    token = tokens[:, t_index]\n",
        "                    loss += loss_fn(pred, token)\n",
        "                loss /= tokens.shape[-1]\n",
        "            # LabelSmoothingCrossEntropyLoss\n",
        "            else:\n",
        "                # `[N, T, C] -> [N, C, T]`\n",
        "                preds = preds.permute([0, 2, 1])\n",
        "                # Remove prediction after the last token.\n",
        "                if preds.shape[-1] == tokens.shape[-1]:\n",
        "                    preds = preds[:, :, :-1]\n",
        "                loss = loss_fn(preds, tokens[:, 1:])\n",
        "\n",
        "            val_loss += loss.item()\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average loss.\n",
        "    val_loss /= num_batches\n",
        "    print(\"Validation performance: \\n\",\n",
        "          f\"Avg loss:{val_loss:>8f}\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (val_loss, pred_times) if return_pred_times else val_loss\n",
        "    return retval"
      ],
      "metadata": {
        "id": "ewn5yeIYhj0d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop_csir_s2s(dataloader,\n",
        "                       model,\n",
        "                       device,\n",
        "                       start_id,\n",
        "                       end_id,\n",
        "                       return_pred_times=False,\n",
        "                       max_seqlen=62):\n",
        "    size = len(dataloader.dataset)\n",
        "    total_wer = 0\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to evaluation mode.\n",
        "    model.eval()\n",
        "    # Main loop.\n",
        "    print(\"Start test.\")\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_sample in enumerate(dataloader):\n",
        "            feature = batch_sample[\"feature\"]\n",
        "            tokens = batch_sample[\"token\"]\n",
        "            tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "            check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "            feature = feature.to(device)\n",
        "            tokens = tokens.to(device)\n",
        "            tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "            frames = feature.shape[-2]\n",
        "\n",
        "            # Predict.\n",
        "            pred_start = time.perf_counter()\n",
        "            pred_ids = inference(model, feature, start_id, end_id, max_seqlen=max_seqlen)\n",
        "            pred_end = time.perf_counter()\n",
        "            pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "            # Compute WER.\n",
        "            # <sos> and <eos> should be removed because they may boost performance.\n",
        "            # print(tokens)\n",
        "            # print(pred_ids)\n",
        "            tokens = tokens[0, 1:-1]\n",
        "            # pred_ids = pred_ids[0, 1:-1]\n",
        "            pred_ids = [pid for pid in pred_ids[0] if pid not in [start_id, end_id]]\n",
        "            ref_length = len(tokens)\n",
        "            wer = edit_distance(tokens, pred_ids)\n",
        "            wer /= ref_length\n",
        "            total_wer += wer\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average WER.\n",
        "    awer = total_wer / size * 100\n",
        "    print(\"Test performance: \\n\",\n",
        "          f\"Avg WER:{awer:>0.1f}%\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (awer, pred_times) if return_pred_times else awer\n",
        "    return retval"
      ],
      "metadata": {
        "id": "55N9jLUGhmpd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sanity check"
      ],
      "metadata": {
        "id": "nMXYp4-gZVRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access check.\n",
        "dataset_dir = Path(\"gafs_dataset_very_small\")\n",
        "files = list(dataset_dir.iterdir())\n",
        "dictionary = [fin for fin in files if \".json\" in fin.name][0]\n",
        "hdf5_files = [fin for fin in files if \".hdf5\" in fin.name]\n",
        "\n",
        "print(dictionary)\n",
        "print(hdf5_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc19b-2pZXQE",
        "outputId": "8322930a-ecbc-483d-9f73-dc965576e67c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gafs_dataset_very_small/character_to_prediction_index.json\n",
            "[PosixPath('gafs_dataset_very_small/6.hdf5'), PosixPath('gafs_dataset_very_small/18.hdf5'), PosixPath('gafs_dataset_very_small/0.hdf5'), PosixPath('gafs_dataset_very_small/74.hdf5'), PosixPath('gafs_dataset_very_small/27.hdf5'), PosixPath('gafs_dataset_very_small/88.hdf5'), PosixPath('gafs_dataset_very_small/71.hdf5'), PosixPath('gafs_dataset_very_small/192.hdf5'), PosixPath('gafs_dataset_very_small/43.hdf5'), PosixPath('gafs_dataset_very_small/143.hdf5'), PosixPath('gafs_dataset_very_small/188.hdf5'), PosixPath('gafs_dataset_very_small/38.hdf5'), PosixPath('gafs_dataset_very_small/109.hdf5'), PosixPath('gafs_dataset_very_small/138.hdf5'), PosixPath('gafs_dataset_very_small/227.hdf5'), PosixPath('gafs_dataset_very_small/230.hdf5'), PosixPath('gafs_dataset_very_small/72.hdf5'), PosixPath('gafs_dataset_very_small/76.hdf5'), PosixPath('gafs_dataset_very_small/168.hdf5'), PosixPath('gafs_dataset_very_small/40.hdf5'), PosixPath('gafs_dataset_very_small/81.hdf5'), PosixPath('gafs_dataset_very_small/236.hdf5'), PosixPath('gafs_dataset_very_small/20.hdf5'), PosixPath('gafs_dataset_very_small/169.hdf5'), PosixPath('gafs_dataset_very_small/187.hdf5'), PosixPath('gafs_dataset_very_small/128.hdf5'), PosixPath('gafs_dataset_very_small/219.hdf5'), PosixPath('gafs_dataset_very_small/89.hdf5'), PosixPath('gafs_dataset_very_small/107.hdf5'), PosixPath('gafs_dataset_very_small/56.hdf5'), PosixPath('gafs_dataset_very_small/202.hdf5'), PosixPath('gafs_dataset_very_small/178.hdf5'), PosixPath('gafs_dataset_very_small/141.hdf5'), PosixPath('gafs_dataset_very_small/53.hdf5'), PosixPath('gafs_dataset_very_small/151.hdf5'), PosixPath('gafs_dataset_very_small/153.hdf5'), PosixPath('gafs_dataset_very_small/112.hdf5'), PosixPath('gafs_dataset_very_small/125.hdf5'), PosixPath('gafs_dataset_very_small/176.hdf5'), PosixPath('gafs_dataset_very_small/102.hdf5'), PosixPath('gafs_dataset_very_small/242.hdf5'), PosixPath('gafs_dataset_very_small/70.hdf5'), PosixPath('gafs_dataset_very_small/233.hdf5'), PosixPath('gafs_dataset_very_small/181.hdf5'), PosixPath('gafs_dataset_very_small/135.hdf5'), PosixPath('gafs_dataset_very_small/68.hdf5'), PosixPath('gafs_dataset_very_small/105.hdf5'), PosixPath('gafs_dataset_very_small/154.hdf5'), PosixPath('gafs_dataset_very_small/63.hdf5'), PosixPath('gafs_dataset_very_small/239.hdf5'), PosixPath('gafs_dataset_very_small/216.hdf5'), PosixPath('gafs_dataset_very_small/231.hdf5'), PosixPath('gafs_dataset_very_small/93.hdf5'), PosixPath('gafs_dataset_very_small/59.hdf5'), PosixPath('gafs_dataset_very_small/251.hdf5'), PosixPath('gafs_dataset_very_small/145.hdf5'), PosixPath('gafs_dataset_very_small/196.hdf5'), PosixPath('gafs_dataset_very_small/225.hdf5'), PosixPath('gafs_dataset_very_small/217.hdf5'), PosixPath('gafs_dataset_very_small/223.hdf5'), PosixPath('gafs_dataset_very_small/4.hdf5'), PosixPath('gafs_dataset_very_small/117.hdf5'), PosixPath('gafs_dataset_very_small/25.hdf5'), PosixPath('gafs_dataset_very_small/2.hdf5'), PosixPath('gafs_dataset_very_small/136.hdf5'), PosixPath('gafs_dataset_very_small/113.hdf5'), PosixPath('gafs_dataset_very_small/160.hdf5'), PosixPath('gafs_dataset_very_small/21.hdf5'), PosixPath('gafs_dataset_very_small/121.hdf5'), PosixPath('gafs_dataset_very_small/1.hdf5'), PosixPath('gafs_dataset_very_small/95.hdf5'), PosixPath('gafs_dataset_very_small/147.hdf5'), PosixPath('gafs_dataset_very_small/36.hdf5'), PosixPath('gafs_dataset_very_small/157.hdf5'), PosixPath('gafs_dataset_very_small/73.hdf5'), PosixPath('gafs_dataset_very_small/15.hdf5'), PosixPath('gafs_dataset_very_small/158.hdf5'), PosixPath('gafs_dataset_very_small/241.hdf5'), PosixPath('gafs_dataset_very_small/161.hdf5'), PosixPath('gafs_dataset_very_small/254.hdf5'), PosixPath('gafs_dataset_very_small/10.hdf5'), PosixPath('gafs_dataset_very_small/203.hdf5'), PosixPath('gafs_dataset_very_small/115.hdf5'), PosixPath('gafs_dataset_very_small/24.hdf5'), PosixPath('gafs_dataset_very_small/9.hdf5'), PosixPath('gafs_dataset_very_small/92.hdf5'), PosixPath('gafs_dataset_very_small/246.hdf5'), PosixPath('gafs_dataset_very_small/13.hdf5'), PosixPath('gafs_dataset_very_small/80.hdf5'), PosixPath('gafs_dataset_very_small/122.hdf5'), PosixPath('gafs_dataset_very_small/186.hdf5'), PosixPath('gafs_dataset_very_small/171.hdf5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dictionary.\n",
        "with open(dictionary, \"r\") as fread:\n",
        "    key2token = json.load(fread)\n",
        "\n",
        "VOCAB = len(key2token)\n",
        "# Add keywords.\n",
        "key2token[\"<sos>\"] = VOCAB\n",
        "key2token[\"<eos>\"] = VOCAB + 1\n",
        "key2token[\"<pad>\"] = VOCAB + 2\n",
        "# Reset.\n",
        "VOCAB = len(key2token)"
      ],
      "metadata": {
        "id": "BPoxLXpDZchs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, use_landmarks = get_fullbody_landmarks()\n",
        "use_features = [\"x\", \"y\"]\n",
        "\n",
        "trans_select_feature = SelectLandmarksAndFeature(landmarks=use_landmarks, features=use_features)\n",
        "trans_repnan = ReplaceNan()\n",
        "trans_norm = PartsBasedNormalization(align_mode=\"framewise\", scale_mode=\"unique\")\n",
        "trans_insert_token = InsertTokensForS2S(sos_token=key2token[\"<sos>\"], eos_token=key2token[\"<eos>\"])\n",
        "\n",
        "pre_transforms = Compose([\n",
        "    trans_select_feature,\n",
        "    trans_repnan,\n",
        "    trans_insert_token,\n",
        "    trans_norm\n",
        "])\n",
        "\n",
        "train_transforms = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "val_transforms = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = Compose([\n",
        "    ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "I6KEUSeqZf2U"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "feature_shape = (len(use_features), -1, len(use_landmarks))\n",
        "token_shape = (-1,)\n",
        "merge_fn = partial(merge_padded_batch,\n",
        "                   feature_shape=feature_shape,\n",
        "                   token_shape=token_shape,\n",
        "                   feature_padding_val=0.0,\n",
        "                   token_padding_val=key2token[\"<pad>\"])\n",
        "\n",
        "dataset = HDF5Dataset(hdf5_files, pre_transforms=pre_transforms, transforms=train_transforms)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=merge_fn)\n",
        "try:\n",
        "    data = next(iter(dataloader))\n",
        "    feature_origin = data[\"feature\"]\n",
        "    tokens_origin = data[\"token\"]\n",
        "\n",
        "    print(feature_origin.shape)\n",
        "    print(tokens_origin)\n",
        "except Exception as inst:\n",
        "    print(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rExP4cCiGAm",
        "outputId": "f4533b47-4544-412d-a7b5-44b5d66ce373"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 409, 130])\n",
            "tensor([[59, 10, 24, 24, 20, 12, 19, 17, 22, 16, 12, 23, 19, 22, 16, 20, 60, 61,\n",
            "         61, 61, 61, 61, 61, 61],\n",
            "        [59, 10, 19, 16, 12, 21, 24, 12, 24, 17, 20, 12, 16, 15, 12, 20, 23, 12,\n",
            "         24, 19, 16, 22, 24, 60]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model.\n",
        "# in_channels: J * C (130*2=260)\n",
        "#   J: use_landmarks (130)\n",
        "#   C: use_channels (2)\n",
        "# out_channels: 10\n",
        "in_channels = len(use_landmarks) * len(use_features)\n",
        "out_channels = VOCAB\n",
        "enc_hidden_channels = 64\n",
        "dec_hidden_channels = 64\n",
        "dec_emb_channels = 4\n",
        "dec_att_dim = 64\n",
        "model = RNNCSLR(\n",
        "    enc_in_channels=in_channels,\n",
        "    enc_hidden_channels=enc_hidden_channels,\n",
        "    enc_rnn_type=\"gru\",\n",
        "    enc_num_layers=2,\n",
        "    enc_activation=\"relu\",\n",
        "    enc_bidir=True,\n",
        "    enc_dropout=0.1,\n",
        "    enc_apply_mask=True,\n",
        "    enc_proj_size=0,\n",
        "    dec_in_channels=enc_hidden_channels,\n",
        "    dec_hidden_channels=dec_hidden_channels,\n",
        "    dec_out_channels=out_channels,\n",
        "    dec_emb_channels=dec_emb_channels,\n",
        "    dec_att_dim=dec_att_dim,\n",
        "    dec_att_add_bias=True,\n",
        "    dec_rnn_type=\"gru\",\n",
        "    dec_num_layers=2,\n",
        "    dec_activation=\"relu\",\n",
        "    dec_dropout=0.1,\n",
        "    dec_padding_val=key2token[\"<pad>\"],\n",
        "    dec_proj_size=0)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Sanity check.\n",
        "sample = next(iter(dataloader))\n",
        "logit = model(sample[\"feature\"],\n",
        "              tokens=sample[\"token\"],\n",
        "              feature_pad_mask=sample[\"feature_pad_mask\"])\n",
        "print(logit.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yQKHqQtiUTE",
        "outputId": "c28ba73c-2239-493b-dde2-345d730e49ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNCSLR(\n",
            "  (linear): Linear(in_features=260, out_features=64, bias=True)\n",
            "  (enc_activation): ReLU()\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (decoder): BahdanauRNNDecoder(\n",
            "    (emb_layer): Embedding(62, 4, padding_idx=61)\n",
            "    (att_layer): SingleHeadAttention(\n",
            "      (att_energy): BahdanauAttentionEnergy(\n",
            "        (w_key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_out): Linear(in_features=128, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (rnn): GRU(132, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (head): Linear(in_features=128, out_features=62, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([2, 24, 62])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train and evaluation"
      ],
      "metadata": {
        "id": "r4o9bbOMjG_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Set common parameters"
      ],
      "metadata": {
        "id": "22_3Ne9ojKLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set common parameters.\n",
        "batch_size = 32\n",
        "load_into_ram = True\n",
        "test_pid = 0\n",
        "num_workers = os.cpu_count()\n",
        "print(f\"Using {num_workers} cores for data loading.\")\n",
        "lr = 3e-4\n",
        "label_smoothing = 0.1\n",
        "sos_token = key2token[\"<sos>\"]\n",
        "eos_token = key2token[\"<eos>\"]\n",
        "pad_token = key2token[\"<pad>\"]\n",
        "max_seqlen = 60\n",
        "\n",
        "epochs = 50\n",
        "eval_every_n_epochs = 1\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} for computation.\")\n",
        "\n",
        "train_hdf5files = [fin for fin in hdf5_files if str(test_pid) not in fin.name]\n",
        "val_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "test_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "\n",
        "_, use_landmarks = get_fullbody_landmarks()\n",
        "use_features = [\"x\", \"y\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3UpkewijNd0",
        "outputId": "fd08365b-87f1-4a8f-ec0d-fda4ed37c574"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2 cores for data loading.\n",
            "Using cuda for computation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataloaders.\n",
        "train_dataset = HDF5Dataset(train_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=train_transforms, load_into_ram=load_into_ram)\n",
        "val_dataset = HDF5Dataset(val_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=val_transforms, load_into_ram=load_into_ram)\n",
        "test_dataset = HDF5Dataset(test_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=test_transforms, load_into_ram=load_into_ram)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=merge_fn, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "RiTJhuSSjW8k"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Run training process"
      ],
      "metadata": {
        "id": "idjHfhW4jerd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = RNNCSLR(\n",
        "    enc_in_channels=in_channels,\n",
        "    enc_hidden_channels=enc_hidden_channels,\n",
        "    enc_rnn_type=\"gru\",\n",
        "    enc_num_layers=2,\n",
        "    enc_activation=\"relu\",\n",
        "    enc_bidir=True,\n",
        "    enc_dropout=0.1,\n",
        "    enc_apply_mask=True,\n",
        "    enc_proj_size=0,\n",
        "    dec_in_channels=enc_hidden_channels,\n",
        "    dec_hidden_channels=dec_hidden_channels,\n",
        "    dec_out_channels=out_channels,\n",
        "    dec_emb_channels=dec_emb_channels,\n",
        "    dec_att_dim=dec_att_dim,\n",
        "    dec_att_add_bias=True,\n",
        "    dec_rnn_type=\"gru\",\n",
        "    dec_num_layers=2,\n",
        "    dec_activation=\"relu\",\n",
        "    dec_dropout=0.1,\n",
        "    dec_padding_val=pad_token,\n",
        "    dec_proj_size=0)\n",
        "\n",
        "print(model_rnn)\n",
        "\n",
        "loss_fn = LabelSmoothingCrossEntropyLoss(\n",
        "    ignore_indices=pad_token, reduction=\"mean_temporal_prior\",\n",
        "    label_smoothing=label_smoothing)\n",
        "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAXnUtz5jheb",
        "outputId": "aebb325f-cdf9-4f41-b249-f6f4952d7c91"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNCSLR(\n",
            "  (linear): Linear(in_features=260, out_features=64, bias=True)\n",
            "  (enc_activation): ReLU()\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (decoder): BahdanauRNNDecoder(\n",
            "    (emb_layer): Embedding(62, 4, padding_idx=61)\n",
            "    (att_layer): SingleHeadAttention(\n",
            "      (att_energy): BahdanauAttentionEnergy(\n",
            "        (w_key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_out): Linear(in_features=128, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (rnn): GRU(132, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (head): Linear(in_features=128, out_features=62, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, validation, and evaluation.\n",
        "model_rnn.to(device)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_wers = []\n",
        "print(\"Start training.\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "    train_loss, train_times = train_loop_csir_s2s(\n",
        "        train_dataloader, model_rnn, loss_fn, optimizer, device,\n",
        "        sos_token, eos_token,\n",
        "        return_pred_times=True)\n",
        "    val_loss, val_times = val_loop_csir_s2s(\n",
        "        val_dataloader, model_rnn, loss_fn, device,\n",
        "        sos_token, eos_token,\n",
        "        return_pred_times=True)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if (epoch+1) % eval_every_n_epochs == 0:\n",
        "        wer, test_times = test_loop_csir_s2s(\n",
        "            test_dataloader, model_rnn, device,\n",
        "            sos_token, eos_token,\n",
        "            return_pred_times=True,\n",
        "            max_seqlen=max_seqlen)\n",
        "        test_wers.append(wer)\n",
        "train_losses_rnn = np.array(train_losses)\n",
        "val_losses_rnn = np.array(val_losses)\n",
        "test_wers_rnn = np.array(test_wers)\n",
        "\n",
        "val_losses_rnn = np.array(val_losses_rnn)\n",
        "test_wers_rnn = np.array(test_wers_rnn)\n",
        "print(f\"Minimum validation loss:{val_losses_rnn.min()} at {np.argmin(val_losses_rnn)+1} epoch.\")\n",
        "print(f\"Minimum WER:{test_wers_rnn.min()} at {np.argmin(test_wers_rnn)*eval_every_n_epochs+1} epoch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSxf_6kBjzAk",
        "outputId": "6d6e05af-130f-47bb-ecd6-f968a90afaa7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training.\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1\n",
            "Start training.\n",
            "loss:4.128036 [    0/ 2513]\n",
            "Done. Time:18.599080279999953\n",
            "Training performance: \n",
            " Avg loss:3.716043\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.133126114000106\n",
            "Validation performance: \n",
            " Avg loss:3.606608\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.337116021000043\n",
            "Test performance: \n",
            " Avg WER:98.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Start training.\n",
            "loss:3.644798 [    0/ 2513]\n",
            "Done. Time:16.123500315\n",
            "Training performance: \n",
            " Avg loss:3.579681\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4026537380000264\n",
            "Validation performance: \n",
            " Avg loss:3.565014\n",
            "\n",
            "Start test.\n",
            "Done. Time:13.639842209999983\n",
            "Test performance: \n",
            " Avg WER:90.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Start training.\n",
            "loss:3.571981 [    0/ 2513]\n",
            "Done. Time:16.136020495000025\n",
            "Training performance: \n",
            " Avg loss:3.488334\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4263352960000475\n",
            "Validation performance: \n",
            " Avg loss:3.429332\n",
            "\n",
            "Start test.\n",
            "Done. Time:14.242035215999977\n",
            "Test performance: \n",
            " Avg WER:91.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Start training.\n",
            "loss:3.365581 [    0/ 2513]\n",
            "Done. Time:16.48567987900003\n",
            "Training performance: \n",
            " Avg loss:3.342248\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4087937580000016\n",
            "Validation performance: \n",
            " Avg loss:3.333175\n",
            "\n",
            "Start test.\n",
            "Done. Time:15.864711354000065\n",
            "Test performance: \n",
            " Avg WER:93.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Start training.\n",
            "loss:3.300078 [    0/ 2513]\n",
            "Done. Time:16.92353216899994\n",
            "Training performance: \n",
            " Avg loss:3.278554\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.8288074370000231\n",
            "Validation performance: \n",
            " Avg loss:3.282204\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.025813906000053\n",
            "Test performance: \n",
            " Avg WER:90.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Start training.\n",
            "loss:3.191658 [    0/ 2513]\n",
            "Done. Time:16.254130506000024\n",
            "Training performance: \n",
            " Avg loss:3.229312\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4383327849999432\n",
            "Validation performance: \n",
            " Avg loss:3.238499\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.916981331999978\n",
            "Test performance: \n",
            " Avg WER:92.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Start training.\n",
            "loss:3.151456 [    0/ 2513]\n",
            "Done. Time:16.142407675000072\n",
            "Training performance: \n",
            " Avg loss:3.180136\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.7417356810000229\n",
            "Validation performance: \n",
            " Avg loss:3.192232\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.115213643999937\n",
            "Test performance: \n",
            " Avg WER:92.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Start training.\n",
            "loss:3.164896 [    0/ 2513]\n",
            "Done. Time:16.107519826000043\n",
            "Training performance: \n",
            " Avg loss:3.137836\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.423902564000059\n",
            "Validation performance: \n",
            " Avg loss:3.152456\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.93878826799994\n",
            "Test performance: \n",
            " Avg WER:92.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Start training.\n",
            "loss:3.126635 [    0/ 2513]\n",
            "Done. Time:16.363783872\n",
            "Training performance: \n",
            " Avg loss:3.096428\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.7689571720000004\n",
            "Validation performance: \n",
            " Avg loss:3.121621\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.108656515999996\n",
            "Test performance: \n",
            " Avg WER:90.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Start training.\n",
            "loss:3.060817 [    0/ 2513]\n",
            "Done. Time:16.296569395000006\n",
            "Training performance: \n",
            " Avg loss:3.061824\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.457462349000025\n",
            "Validation performance: \n",
            " Avg loss:3.098741\n",
            "\n",
            "Start test.\n",
            "Done. Time:26.496567473000027\n",
            "Test performance: \n",
            " Avg WER:118.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Start training.\n",
            "loss:3.169017 [    0/ 2513]\n",
            "Done. Time:16.281296300000008\n",
            "Training performance: \n",
            " Avg loss:3.032513\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5311994920000416\n",
            "Validation performance: \n",
            " Avg loss:3.053706\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.103593453999906\n",
            "Test performance: \n",
            " Avg WER:87.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Start training.\n",
            "loss:3.071365 [    0/ 2513]\n",
            "Done. Time:16.53204749800011\n",
            "Training performance: \n",
            " Avg loss:3.005181\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.0040899060002175\n",
            "Validation performance: \n",
            " Avg loss:3.032400\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.582953089000057\n",
            "Test performance: \n",
            " Avg WER:86.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Start training.\n",
            "loss:3.079386 [    0/ 2513]\n",
            "Done. Time:16.821563751999975\n",
            "Training performance: \n",
            " Avg loss:2.982529\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4341991590001726\n",
            "Validation performance: \n",
            " Avg loss:3.008994\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.31806495500018\n",
            "Test performance: \n",
            " Avg WER:87.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Start training.\n",
            "loss:2.915373 [    0/ 2513]\n",
            "Done. Time:16.47633049000001\n",
            "Training performance: \n",
            " Avg loss:2.957891\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.059522298000047\n",
            "Validation performance: \n",
            " Avg loss:2.986972\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.756416403999992\n",
            "Test performance: \n",
            " Avg WER:85.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Start training.\n",
            "loss:2.934129 [    0/ 2513]\n",
            "Done. Time:16.262050604000024\n",
            "Training performance: \n",
            " Avg loss:2.939122\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4538088049998805\n",
            "Validation performance: \n",
            " Avg loss:2.972334\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.04351513499978\n",
            "Test performance: \n",
            " Avg WER:85.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Start training.\n",
            "loss:2.876066 [    0/ 2513]\n",
            "Done. Time:16.82668357700004\n",
            "Training performance: \n",
            " Avg loss:2.918174\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1507745080000404\n",
            "Validation performance: \n",
            " Avg loss:2.955460\n",
            "\n",
            "Start test.\n",
            "Done. Time:17.8518317139999\n",
            "Test performance: \n",
            " Avg WER:83.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Start training.\n",
            "loss:2.949601 [    0/ 2513]\n",
            "Done. Time:16.405401852000068\n",
            "Training performance: \n",
            " Avg loss:2.900884\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4335081870001432\n",
            "Validation performance: \n",
            " Avg loss:2.939220\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.071879967000086\n",
            "Test performance: \n",
            " Avg WER:85.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Start training.\n",
            "loss:2.914494 [    0/ 2513]\n",
            "Done. Time:16.716732000000093\n",
            "Training performance: \n",
            " Avg loss:2.884725\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.254440913000053\n",
            "Validation performance: \n",
            " Avg loss:2.931279\n",
            "\n",
            "Start test.\n",
            "Done. Time:17.432248232999882\n",
            "Test performance: \n",
            " Avg WER:84.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Start training.\n",
            "loss:2.923651 [    0/ 2513]\n",
            "Done. Time:16.26773174699997\n",
            "Training performance: \n",
            " Avg loss:2.867048\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.419636096999966\n",
            "Validation performance: \n",
            " Avg loss:2.912260\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.423765559000003\n",
            "Test performance: \n",
            " Avg WER:87.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Start training.\n",
            "loss:2.796707 [    0/ 2513]\n",
            "Done. Time:17.143341916000054\n",
            "Training performance: \n",
            " Avg loss:2.851652\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.597875138000063\n",
            "Validation performance: \n",
            " Avg loss:2.906685\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.779432391\n",
            "Test performance: \n",
            " Avg WER:96.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Start training.\n",
            "loss:2.842588 [    0/ 2513]\n",
            "Done. Time:16.74807908599996\n",
            "Training performance: \n",
            " Avg loss:2.837510\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.082190558999855\n",
            "Validation performance: \n",
            " Avg loss:2.891952\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.068130886000063\n",
            "Test performance: \n",
            " Avg WER:88.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Start training.\n",
            "loss:2.864021 [    0/ 2513]\n",
            "Done. Time:16.30554812499986\n",
            "Training performance: \n",
            " Avg loss:2.825021\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4218853429999854\n",
            "Validation performance: \n",
            " Avg loss:2.889270\n",
            "\n",
            "Start test.\n",
            "Done. Time:17.85034640899994\n",
            "Test performance: \n",
            " Avg WER:85.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Start training.\n",
            "loss:2.786388 [    0/ 2513]\n",
            "Done. Time:16.54665531799992\n",
            "Training performance: \n",
            " Avg loss:2.813808\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1550690880001184\n",
            "Validation performance: \n",
            " Avg loss:2.874272\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.322989434999954\n",
            "Test performance: \n",
            " Avg WER:89.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Start training.\n",
            "loss:2.789234 [    0/ 2513]\n",
            "Done. Time:16.38985135200005\n",
            "Training performance: \n",
            " Avg loss:2.799410\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.443845667000005\n",
            "Validation performance: \n",
            " Avg loss:2.861613\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.93412106100004\n",
            "Test performance: \n",
            " Avg WER:100.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Start training.\n",
            "loss:2.788844 [    0/ 2513]\n",
            "Done. Time:16.772853097000052\n",
            "Training performance: \n",
            " Avg loss:2.786921\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4488772079998853\n",
            "Validation performance: \n",
            " Avg loss:2.846917\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.889164435999874\n",
            "Test performance: \n",
            " Avg WER:88.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Start training.\n",
            "loss:2.791092 [    0/ 2513]\n",
            "Done. Time:16.277756580000187\n",
            "Training performance: \n",
            " Avg loss:2.775084\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.6905829790000553\n",
            "Validation performance: \n",
            " Avg loss:2.838566\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.569349912999996\n",
            "Test performance: \n",
            " Avg WER:86.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Start training.\n",
            "loss:2.745268 [    0/ 2513]\n",
            "Done. Time:16.29797538699995\n",
            "Training performance: \n",
            " Avg loss:2.762730\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4508243320001384\n",
            "Validation performance: \n",
            " Avg loss:2.832830\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.238714072999983\n",
            "Test performance: \n",
            " Avg WER:84.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Start training.\n",
            "loss:2.778494 [    0/ 2513]\n",
            "Done. Time:16.455904431000135\n",
            "Training performance: \n",
            " Avg loss:2.751690\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.2260973239999657\n",
            "Validation performance: \n",
            " Avg loss:2.825506\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.582010573999924\n",
            "Test performance: \n",
            " Avg WER:86.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Start training.\n",
            "loss:2.784081 [    0/ 2513]\n",
            "Done. Time:16.721178304999967\n",
            "Training performance: \n",
            " Avg loss:2.742130\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4121647639999537\n",
            "Validation performance: \n",
            " Avg loss:2.814582\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.5783875489999\n",
            "Test performance: \n",
            " Avg WER:100.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Start training.\n",
            "loss:2.738782 [    0/ 2513]\n",
            "Done. Time:16.375331481999865\n",
            "Training performance: \n",
            " Avg loss:2.731590\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.420870702000002\n",
            "Validation performance: \n",
            " Avg loss:2.805858\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.347887162000006\n",
            "Test performance: \n",
            " Avg WER:86.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Start training.\n",
            "loss:2.708797 [    0/ 2513]\n",
            "Done. Time:16.37260904599998\n",
            "Training performance: \n",
            " Avg loss:2.720156\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4336280689999512\n",
            "Validation performance: \n",
            " Avg loss:2.802241\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.733728164000013\n",
            "Test performance: \n",
            " Avg WER:92.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Start training.\n",
            "loss:2.697371 [    0/ 2513]\n",
            "Done. Time:16.39372681100008\n",
            "Training performance: \n",
            " Avg loss:2.709472\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4230022180001924\n",
            "Validation performance: \n",
            " Avg loss:2.801486\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.089089812999873\n",
            "Test performance: \n",
            " Avg WER:95.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Start training.\n",
            "loss:2.796399 [    0/ 2513]\n",
            "Done. Time:17.204133247000073\n",
            "Training performance: \n",
            " Avg loss:2.698481\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4622007319999284\n",
            "Validation performance: \n",
            " Avg loss:2.790142\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.723705175000077\n",
            "Test performance: \n",
            " Avg WER:94.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Start training.\n",
            "loss:2.715989 [    0/ 2513]\n",
            "Done. Time:16.257780227000012\n",
            "Training performance: \n",
            " Avg loss:2.687514\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.8670082880000791\n",
            "Validation performance: \n",
            " Avg loss:2.787462\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.545072836000145\n",
            "Test performance: \n",
            " Avg WER:92.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Start training.\n",
            "loss:2.709015 [    0/ 2513]\n",
            "Done. Time:16.37689694100004\n",
            "Training performance: \n",
            " Avg loss:2.677238\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.442922906999911\n",
            "Validation performance: \n",
            " Avg loss:2.772089\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.27998537999997\n",
            "Test performance: \n",
            " Avg WER:91.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Start training.\n",
            "loss:2.663367 [    0/ 2513]\n",
            "Done. Time:16.67413916800001\n",
            "Training performance: \n",
            " Avg loss:2.665105\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4390786080000453\n",
            "Validation performance: \n",
            " Avg loss:2.762936\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.016196412\n",
            "Test performance: \n",
            " Avg WER:88.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Start training.\n",
            "loss:2.620747 [    0/ 2513]\n",
            "Done. Time:16.525263478999932\n",
            "Training performance: \n",
            " Avg loss:2.658308\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1645849369999723\n",
            "Validation performance: \n",
            " Avg loss:2.761761\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.435473988999775\n",
            "Test performance: \n",
            " Avg WER:89.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Start training.\n",
            "loss:2.637015 [    0/ 2513]\n",
            "Done. Time:16.405488619999915\n",
            "Training performance: \n",
            " Avg loss:2.646143\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.468217606000053\n",
            "Validation performance: \n",
            " Avg loss:2.755165\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.915299855000058\n",
            "Test performance: \n",
            " Avg WER:84.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Start training.\n",
            "loss:2.710723 [    0/ 2513]\n",
            "Done. Time:17.146734514999935\n",
            "Training performance: \n",
            " Avg loss:2.635763\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4636333180001202\n",
            "Validation performance: \n",
            " Avg loss:2.748069\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.288100487000065\n",
            "Test performance: \n",
            " Avg WER:96.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Start training.\n",
            "loss:2.589118 [    0/ 2513]\n",
            "Done. Time:16.961123080000107\n",
            "Training performance: \n",
            " Avg loss:2.626635\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.0941395280001416\n",
            "Validation performance: \n",
            " Avg loss:2.741638\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.71929800099997\n",
            "Test performance: \n",
            " Avg WER:86.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Start training.\n",
            "loss:2.723295 [    0/ 2513]\n",
            "Done. Time:16.251340375999916\n",
            "Training performance: \n",
            " Avg loss:2.615254\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.409577884999635\n",
            "Validation performance: \n",
            " Avg loss:2.743255\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.884449874999973\n",
            "Test performance: \n",
            " Avg WER:85.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Start training.\n",
            "loss:2.676351 [    0/ 2513]\n",
            "Done. Time:17.095603409999967\n",
            "Training performance: \n",
            " Avg loss:2.608894\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.6334257670000625\n",
            "Validation performance: \n",
            " Avg loss:2.735379\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.68586104399992\n",
            "Test performance: \n",
            " Avg WER:88.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Start training.\n",
            "loss:2.576494 [    0/ 2513]\n",
            "Done. Time:16.334348819999832\n",
            "Training performance: \n",
            " Avg loss:2.598470\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5693742789999305\n",
            "Validation performance: \n",
            " Avg loss:2.730676\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.69468563999999\n",
            "Test performance: \n",
            " Avg WER:93.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Start training.\n",
            "loss:2.487960 [    0/ 2513]\n",
            "Done. Time:16.306991546000063\n",
            "Training performance: \n",
            " Avg loss:2.587961\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.438314691999949\n",
            "Validation performance: \n",
            " Avg loss:2.723655\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.332876183999815\n",
            "Test performance: \n",
            " Avg WER:86.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Start training.\n",
            "loss:2.607262 [    0/ 2513]\n",
            "Done. Time:16.39945154199995\n",
            "Training performance: \n",
            " Avg loss:2.579279\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4388787909997518\n",
            "Validation performance: \n",
            " Avg loss:2.716284\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.3935079820003\n",
            "Test performance: \n",
            " Avg WER:85.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Start training.\n",
            "loss:2.348014 [    0/ 2513]\n",
            "Done. Time:17.136829579999812\n",
            "Training performance: \n",
            " Avg loss:2.571017\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.3374714219999078\n",
            "Validation performance: \n",
            " Avg loss:2.716423\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.468336636999993\n",
            "Test performance: \n",
            " Avg WER:89.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Start training.\n",
            "loss:2.609846 [    0/ 2513]\n",
            "Done. Time:16.284544366999853\n",
            "Training performance: \n",
            " Avg loss:2.564713\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4252721259999817\n",
            "Validation performance: \n",
            " Avg loss:2.720480\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.07725835800011\n",
            "Test performance: \n",
            " Avg WER:90.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Start training.\n",
            "loss:2.575177 [    0/ 2513]\n",
            "Done. Time:16.389702541000133\n",
            "Training performance: \n",
            " Avg loss:2.551484\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4367330660002153\n",
            "Validation performance: \n",
            " Avg loss:2.710793\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.164021602000048\n",
            "Test performance: \n",
            " Avg WER:91.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Start training.\n",
            "loss:2.614090 [    0/ 2513]\n",
            "Done. Time:16.566590012000233\n",
            "Training performance: \n",
            " Avg loss:2.541115\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.501034874999732\n",
            "Validation performance: \n",
            " Avg loss:2.705567\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.987165571999867\n",
            "Test performance: \n",
            " Avg WER:86.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Start training.\n",
            "loss:2.450420 [    0/ 2513]\n",
            "Done. Time:17.08783800099991\n",
            "Training performance: \n",
            " Avg loss:2.533669\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4217854580001585\n",
            "Validation performance: \n",
            " Avg loss:2.701821\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.15462778300025\n",
            "Test performance: \n",
            " Avg WER:87.7%\n",
            "\n",
            "Minimum validation loss:2.701820507645607 at 50 epoch.\n",
            "Minimum WER:83.86901598236466 at 16 epoch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot result"
      ],
      "metadata": {
        "id": "bI_5zh65kfss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.grid(axis=\"y\", linestyle=\"dotted\", color=\"k\")\n",
        "\n",
        "xs = np.arange(1, len(val_losses_rnn)+1)\n",
        "plt.plot(xs, val_losses_rnn, label=\"RNN\", marker=\".\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.ylim([0.0, 5.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "kQ1seXMDkiHj",
        "outputId": "ad739636-4a24-4d94-f555-f4c90e830676"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid(axis=\"y\", linestyle=\"dotted\", color=\"k\")\n",
        "\n",
        "xs = np.arange(1, len(test_wers_rnn)+1)\n",
        "plt.plot(xs, test_wers_rnn, label=\"RNN\", marker=\".\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"WER\")\n",
        "plt.ylim([0.0, 100.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "4Uq2CQVDk9ol",
        "outputId": "a241efe9-d998-4593-a0bf-527282bea40a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}