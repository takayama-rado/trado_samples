{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOV0/UxcSP/WzjEcI96piyt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takayama-rado/trado_samples/blob/main/colab_files/gafs_rnn_encoder_decoder_cslr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download dataset and modules"
      ],
      "metadata": {
        "id": "eQgl6G6nE7D0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADnfnnDUE0t1",
        "outputId": "9028c09d-2b95-4956-b6af-4c244669f4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to local.\n",
        "!cp ./drive/MyDrive/Datasets/gafs_dataset_very_small.zip gafs_dataset.zip"
      ],
      "metadata": {
        "id": "1qMzlk1ilsWk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o gafs_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnzLCqD8FR7T",
        "outputId": "45f07afa-bb99-42f1-a556-6ee54fe110c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gafs_dataset.zip\n",
            "   creating: gafs_dataset_very_small/\n",
            "  inflating: gafs_dataset_very_small/0.hdf5  \n",
            "  inflating: gafs_dataset_very_small/1.hdf5  \n",
            "  inflating: gafs_dataset_very_small/10.hdf5  \n",
            "  inflating: gafs_dataset_very_small/102.hdf5  \n",
            "  inflating: gafs_dataset_very_small/105.hdf5  \n",
            "  inflating: gafs_dataset_very_small/107.hdf5  \n",
            "  inflating: gafs_dataset_very_small/109.hdf5  \n",
            "  inflating: gafs_dataset_very_small/112.hdf5  \n",
            "  inflating: gafs_dataset_very_small/113.hdf5  \n",
            "  inflating: gafs_dataset_very_small/115.hdf5  \n",
            "  inflating: gafs_dataset_very_small/117.hdf5  \n",
            "  inflating: gafs_dataset_very_small/121.hdf5  \n",
            "  inflating: gafs_dataset_very_small/122.hdf5  \n",
            "  inflating: gafs_dataset_very_small/125.hdf5  \n",
            "  inflating: gafs_dataset_very_small/128.hdf5  \n",
            "  inflating: gafs_dataset_very_small/13.hdf5  \n",
            "  inflating: gafs_dataset_very_small/135.hdf5  \n",
            "  inflating: gafs_dataset_very_small/136.hdf5  \n",
            "  inflating: gafs_dataset_very_small/138.hdf5  \n",
            "  inflating: gafs_dataset_very_small/141.hdf5  \n",
            "  inflating: gafs_dataset_very_small/143.hdf5  \n",
            "  inflating: gafs_dataset_very_small/145.hdf5  \n",
            "  inflating: gafs_dataset_very_small/147.hdf5  \n",
            "  inflating: gafs_dataset_very_small/15.hdf5  \n",
            "  inflating: gafs_dataset_very_small/151.hdf5  \n",
            "  inflating: gafs_dataset_very_small/153.hdf5  \n",
            "  inflating: gafs_dataset_very_small/154.hdf5  \n",
            "  inflating: gafs_dataset_very_small/157.hdf5  \n",
            "  inflating: gafs_dataset_very_small/158.hdf5  \n",
            "  inflating: gafs_dataset_very_small/160.hdf5  \n",
            "  inflating: gafs_dataset_very_small/161.hdf5  \n",
            "  inflating: gafs_dataset_very_small/168.hdf5  \n",
            "  inflating: gafs_dataset_very_small/169.hdf5  \n",
            "  inflating: gafs_dataset_very_small/171.hdf5  \n",
            "  inflating: gafs_dataset_very_small/176.hdf5  \n",
            "  inflating: gafs_dataset_very_small/178.hdf5  \n",
            "  inflating: gafs_dataset_very_small/18.hdf5  \n",
            "  inflating: gafs_dataset_very_small/181.hdf5  \n",
            "  inflating: gafs_dataset_very_small/186.hdf5  \n",
            "  inflating: gafs_dataset_very_small/187.hdf5  \n",
            "  inflating: gafs_dataset_very_small/188.hdf5  \n",
            "  inflating: gafs_dataset_very_small/192.hdf5  \n",
            "  inflating: gafs_dataset_very_small/196.hdf5  \n",
            "  inflating: gafs_dataset_very_small/2.hdf5  \n",
            "  inflating: gafs_dataset_very_small/20.hdf5  \n",
            "  inflating: gafs_dataset_very_small/202.hdf5  \n",
            "  inflating: gafs_dataset_very_small/203.hdf5  \n",
            "  inflating: gafs_dataset_very_small/21.hdf5  \n",
            "  inflating: gafs_dataset_very_small/216.hdf5  \n",
            "  inflating: gafs_dataset_very_small/217.hdf5  \n",
            "  inflating: gafs_dataset_very_small/219.hdf5  \n",
            "  inflating: gafs_dataset_very_small/223.hdf5  \n",
            "  inflating: gafs_dataset_very_small/225.hdf5  \n",
            "  inflating: gafs_dataset_very_small/227.hdf5  \n",
            "  inflating: gafs_dataset_very_small/230.hdf5  \n",
            "  inflating: gafs_dataset_very_small/231.hdf5  \n",
            "  inflating: gafs_dataset_very_small/233.hdf5  \n",
            "  inflating: gafs_dataset_very_small/236.hdf5  \n",
            "  inflating: gafs_dataset_very_small/239.hdf5  \n",
            "  inflating: gafs_dataset_very_small/24.hdf5  \n",
            "  inflating: gafs_dataset_very_small/241.hdf5  \n",
            "  inflating: gafs_dataset_very_small/242.hdf5  \n",
            "  inflating: gafs_dataset_very_small/246.hdf5  \n",
            "  inflating: gafs_dataset_very_small/25.hdf5  \n",
            "  inflating: gafs_dataset_very_small/251.hdf5  \n",
            "  inflating: gafs_dataset_very_small/254.hdf5  \n",
            "  inflating: gafs_dataset_very_small/27.hdf5  \n",
            "  inflating: gafs_dataset_very_small/36.hdf5  \n",
            "  inflating: gafs_dataset_very_small/38.hdf5  \n",
            "  inflating: gafs_dataset_very_small/4.hdf5  \n",
            "  inflating: gafs_dataset_very_small/40.hdf5  \n",
            "  inflating: gafs_dataset_very_small/43.hdf5  \n",
            "  inflating: gafs_dataset_very_small/53.hdf5  \n",
            "  inflating: gafs_dataset_very_small/56.hdf5  \n",
            "  inflating: gafs_dataset_very_small/59.hdf5  \n",
            "  inflating: gafs_dataset_very_small/6.hdf5  \n",
            "  inflating: gafs_dataset_very_small/63.hdf5  \n",
            "  inflating: gafs_dataset_very_small/68.hdf5  \n",
            "  inflating: gafs_dataset_very_small/70.hdf5  \n",
            "  inflating: gafs_dataset_very_small/71.hdf5  \n",
            "  inflating: gafs_dataset_very_small/72.hdf5  \n",
            "  inflating: gafs_dataset_very_small/73.hdf5  \n",
            "  inflating: gafs_dataset_very_small/74.hdf5  \n",
            "  inflating: gafs_dataset_very_small/76.hdf5  \n",
            "  inflating: gafs_dataset_very_small/80.hdf5  \n",
            "  inflating: gafs_dataset_very_small/81.hdf5  \n",
            "  inflating: gafs_dataset_very_small/88.hdf5  \n",
            "  inflating: gafs_dataset_very_small/89.hdf5  \n",
            "  inflating: gafs_dataset_very_small/9.hdf5  \n",
            "  inflating: gafs_dataset_very_small/92.hdf5  \n",
            "  inflating: gafs_dataset_very_small/93.hdf5  \n",
            "  inflating: gafs_dataset_very_small/95.hdf5  \n",
            "  inflating: gafs_dataset_very_small/character_to_prediction_index.json  \n",
            "  inflating: gafs_dataset_very_small/LICENSE.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gafs_dataset_very_small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ZWwV56FWVD",
        "outputId": "b48aa9d9-44be-424f-8806-a7cb44ad13b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.hdf5\t  135.hdf5  160.hdf5  1.hdf5\t236.hdf5  43.hdf5  80.hdf5\n",
            "102.hdf5  136.hdf5  161.hdf5  202.hdf5\t239.hdf5  4.hdf5   81.hdf5\n",
            "105.hdf5  138.hdf5  168.hdf5  203.hdf5\t241.hdf5  53.hdf5  88.hdf5\n",
            "107.hdf5  13.hdf5   169.hdf5  20.hdf5\t242.hdf5  56.hdf5  89.hdf5\n",
            "109.hdf5  141.hdf5  171.hdf5  216.hdf5\t246.hdf5  59.hdf5  92.hdf5\n",
            "10.hdf5   143.hdf5  176.hdf5  217.hdf5\t24.hdf5   63.hdf5  93.hdf5\n",
            "112.hdf5  145.hdf5  178.hdf5  219.hdf5\t251.hdf5  68.hdf5  95.hdf5\n",
            "113.hdf5  147.hdf5  181.hdf5  21.hdf5\t254.hdf5  6.hdf5   9.hdf5\n",
            "115.hdf5  151.hdf5  186.hdf5  223.hdf5\t25.hdf5   70.hdf5  character_to_prediction_index.json\n",
            "117.hdf5  153.hdf5  187.hdf5  225.hdf5\t27.hdf5   71.hdf5  LICENSE.txt\n",
            "121.hdf5  154.hdf5  188.hdf5  227.hdf5\t2.hdf5\t  72.hdf5\n",
            "122.hdf5  157.hdf5  18.hdf5   230.hdf5\t36.hdf5   73.hdf5\n",
            "125.hdf5  158.hdf5  192.hdf5  231.hdf5\t38.hdf5   74.hdf5\n",
            "128.hdf5  15.hdf5   196.hdf5  233.hdf5\t40.hdf5   76.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gafs_dataset_very_small/character_to_prediction_index.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLP9RpORFaLL",
        "outputId": "5bede925-a132-4a4e-82f7-549d97e732df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\" \":0,\"!\":1,\"#\":2,\"$\":3,\"%\":4,\"&\":5,\"'\":6,\"(\":7,\")\":8,\"*\":9,\"+\":10,\",\":11,\"-\":12,\".\":13,\"\\/\":14,\"0\":15,\"1\":16,\"2\":17,\"3\":18,\"4\":19,\"5\":20,\"6\":21,\"7\":22,\"8\":23,\"9\":24,\":\":25,\";\":26,\"=\":27,\"?\":28,\"@\":29,\"[\":30,\"_\":31,\"a\":32,\"b\":33,\"c\":34,\"d\":35,\"e\":36,\"f\":37,\"g\":38,\"h\":39,\"i\":40,\"j\":41,\"k\":42,\"l\":43,\"m\":44,\"n\":45,\"o\":46,\"p\":47,\"q\":48,\"r\":49,\"s\":50,\"t\":51,\"u\":52,\"v\":53,\"w\":54,\"x\":55,\"y\":56,\"z\":57,\"~\":58}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gafs_dataset_very_small/LICENSE.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iQlJCDtU4d8",
        "outputId": "338209bf-9bb3-4264-c115-69871882372c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset provided by Natsuki Takayama (Takayama Research and Development Office) is licensed under CC-BY 4.0.\r\n",
            "Author: Copyright 2024 Natsuki Takayama\r\n",
            "Title: GASF very small dataset\r\n",
            "Original licenser: Google LLC\r\n",
            "Modification\r\n",
            "- Extract only 3 parquet file.\r\n",
            "- Packaged into HDF5 format.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "with h5py.File(\"gafs_dataset_very_small/0.hdf5\", \"r\") as fread:\n",
        "    keys = list(fread.keys())\n",
        "    print(keys[:10])\n",
        "    group = fread[keys[0]]\n",
        "    print(group.keys())\n",
        "    feature = group[\"feature\"][:]\n",
        "    token = group[\"token\"][:]\n",
        "    print(feature.shape)\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37SBiPQeVSB7",
        "outputId": "c7666839-7177-47d5-9989-b88d8cec46ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1720198121', '1722303176', '1723157122', '1731934631', '1737624109', '1739256200', '1743069372', '1743412187', '1744795751', '1746320345']\n",
            "<KeysViewHDF5 ['feature', 'token']>\n",
            "(2, 271, 543)\n",
            "[14 38 32 45 44 36 40 32 43 43 36 56 14 43 40 45 32 12 34 32 49 50 51 36\n",
            " 45 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/takayama-rado/trado_samples/archive/refs/tags/v0.3.4.zip -O master.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjcUxrq2VmlE",
        "outputId": "a09e5786-b075-40bc-88f6-afe6c53bdcc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-12 14:01:00--  https://github.com/takayama-rado/trado_samples/archive/refs/tags/v0.3.4.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/takayama-rado/trado_samples/zip/refs/tags/v0.3.4 [following]\n",
            "--2024-09-12 14:01:00--  https://codeload.github.com/takayama-rado/trado_samples/zip/refs/tags/v0.3.4\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [               <=>  ]  76.54M  17.6MB/s    in 4.3s    \n",
            "\n",
            "2024-09-12 14:01:05 (17.6 MB/s) - ‘master.zip’ saved [80254068]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o master.zip -d master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUTwKBOMVw-j",
        "outputId": "90ee309b-c984-40a4-85d2-e95e3392094d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  master.zip\n",
            "3406d5a0072e08879272e622ff8efdc1c7b78ee8\n",
            "   creating: master/trado_samples-0.3.4/\n",
            "  inflating: master/trado_samples-0.3.4/.gitignore  \n",
            "  inflating: master/trado_samples-0.3.4/LICENSE  \n",
            "  inflating: master/trado_samples-0.3.4/README.md  \n",
            "   creating: master/trado_samples-0.3.4/colab_files/\n",
            " extracting: master/trado_samples-0.3.4/colab_files/.gitkeep  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_affine_np_einsum.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_jax_static.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_mpholistic_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_mpothers_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_jax.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_numpy.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_torch.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_jax.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_numpy_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_numpy_1.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_torch.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gafs_create_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_access_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_conformer_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_create_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_macaronnet_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_normalize_landmarks.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_1.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_2.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_3.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_select_landmarks.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_simple_attention.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_simple_islr_model.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_drop.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_hflip.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_resize.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_saffine.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_snoise.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_tclip.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_tinterp.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_twarping.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_w_ls.ipynb  \n",
            "   creating: master/trado_samples-0.3.4/src/\n",
            "   creating: master/trado_samples-0.3.4/src/modules_gislr/\n",
            " extracting: master/trado_samples-0.3.4/src/modules_gislr/__init__.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/activation.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/dataset.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/defines.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/draw_functions.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/layers.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/train_functions.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/transforms.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/utils.py  \n",
            "   creating: master/trado_samples-0.3.4/test_data/\n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static_affine.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static_interp.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_middle0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_near0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/hand_only.mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv master/trado_samples-0.3.4/src/modules_gislr ."
      ],
      "metadata": {
        "id": "yXsIhVAWVyej"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf master master.zip gafs_dataset_very_small.zip"
      ],
      "metadata": {
        "id": "ohykNs7zV3TL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koeHiooRV7qr",
        "outputId": "b756fdf2-8024-4d61-947d-b59ac4afb8dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  gafs_dataset_very_small\tgafs_dataset.zip  modules_gislr  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load library"
      ],
      "metadata": {
        "id": "ddZ2NhLDV8yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "# Third party's modules\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import (\n",
        "    DataLoader)\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "# Local modules\n",
        "sys.path.append(\"modules_gislr\")\n",
        "from modules_gislr.dataset import (\n",
        "    HDF5Dataset,\n",
        "    merge)\n",
        "from modules_gislr.defines import (\n",
        "    get_fullbody_landmarks\n",
        ")\n",
        "from modules_gislr.layers import (\n",
        "    RNNEncoder,\n",
        "    apply_norm,\n",
        "    create_norm\n",
        ")\n",
        "from modules_gislr.train_functions import (\n",
        "    LabelSmoothingCrossEntropyLoss\n",
        ")\n",
        "from modules_gislr.transforms import (\n",
        "    PartsBasedNormalization,\n",
        "    ReplaceNan,\n",
        "    SelectLandmarksAndFeature,\n",
        "    ToTensor\n",
        ")\n",
        "from modules_gislr.utils import (\n",
        "    select_reluwise_activation\n",
        ")"
      ],
      "metadata": {
        "id": "8K-wtRChV-n7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implement RNN Encoder-Decoder CSLR model"
      ],
      "metadata": {
        "id": "Klup1x0iWlHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention module"
      ],
      "metadata": {
        "id": "aPHXLRYMXE9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttentionEnergy(nn.Module):\n",
        "    def __init__(self,\n",
        "                 key_dim,\n",
        "                 query_dim,\n",
        "                 att_dim,\n",
        "                 add_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_key = nn.Linear(key_dim, att_dim, bias=add_bias)\n",
        "        self.w_query = nn.Linear(query_dim, att_dim, bias=add_bias)\n",
        "        self.w_out = nn.Linear(att_dim, 1, bias=add_bias)\n",
        "\n",
        "    def forward(self, key, query):\n",
        "        # key: `[N, key_len, key_dim]`\n",
        "        # query: `[N, 1, query_dim]`\n",
        "        key = self.w_key(key)\n",
        "        query = self.w_query(query)\n",
        "        # Adding with broadcasting.\n",
        "        # key: `[N, key_len, key_dim]`\n",
        "        # query: `[N, 1, query_dim]`\n",
        "        # query should be broadcasted to `[N, key_len, query_dim]`\n",
        "        temp = key + query\n",
        "        # `[N, key_len, att_dim] -> [N, key_len, 1] -> [N, 1, key_len]`\n",
        "        energy = self.w_out(torch.tanh(temp))\n",
        "        energy = torch.permute(energy, [0, 2, 1])\n",
        "        return energy"
      ],
      "metadata": {
        "id": "9AClEx01WqmU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 key_dim,\n",
        "                 query_dim,\n",
        "                 att_dim,\n",
        "                 add_bias):\n",
        "        super().__init__()\n",
        "\n",
        "        self.att_energy = BahdanauAttentionEnergy(\n",
        "            key_dim=key_dim,\n",
        "            query_dim=query_dim,\n",
        "            att_dim=att_dim,\n",
        "            add_bias=add_bias)\n",
        "\n",
        "        self.neg_inf = None\n",
        "\n",
        "    def forward(self,\n",
        "                key,\n",
        "                value,\n",
        "                query,\n",
        "                mask=None):\n",
        "        if self.neg_inf is None:\n",
        "            self.neg_inf = float(np.finfo(\n",
        "                torch.tensor(0, dtype=key.dtype).numpy().dtype).min)\n",
        "\n",
        "        batch, klen, kdim = key.shape\n",
        "        _, qlen, qdim = query.shape\n",
        "        energy = self.att_energy(key=key, query=query)\n",
        "        assert energy.shape == (batch, qlen, klen)\n",
        "\n",
        "        # Apply mask.\n",
        "        if mask is not None:\n",
        "            if len(mask.shape) == 2:\n",
        "                # `[N, klen] -> [N, qlen(=1), klen]`\n",
        "                mask = mask.unsqueeze(1)\n",
        "            # Negative infinity should be 0 in softmax.\n",
        "            energy = energy.masked_fill_(mask==0, self.neg_inf)\n",
        "\n",
        "        # Compute attention mask.\n",
        "        attw = torch.softmax(energy, dim=-1)\n",
        "        # attw: `[N, qlen, klen]`\n",
        "        # value: `[N, klen, kdim]`\n",
        "        # bmm: `[N, qlen, klen] x [N, klen, kdim] -> [N, qlen, kdim]`\n",
        "        cvec = torch.bmm(attw, value)\n",
        "        return cvec, attw"
      ],
      "metadata": {
        "id": "VEy2_ljtXNEz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Decoder"
      ],
      "metadata": {
        "id": "IRNyq0OkX8Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauRNNDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 hidden_channels,\n",
        "                 out_channels,\n",
        "                 emb_channels,\n",
        "                 att_dim,\n",
        "                 att_add_bias,\n",
        "                 rnn_type,\n",
        "                 num_layers,\n",
        "                 activation,\n",
        "                 dropout,\n",
        "                 padding_val,\n",
        "                 proj_size=0):\n",
        "        super().__init__()\n",
        "        assert rnn_type in [\"srnn\", \"lstm\", \"gru\"]\n",
        "\n",
        "        self.emb_layer = nn.Embedding(\n",
        "            num_embeddings=out_channels,\n",
        "            embedding_dim=emb_channels,\n",
        "            padding_idx=padding_val)\n",
        "\n",
        "        self.att_layer = SingleHeadAttention(\n",
        "            key_dim=in_channels,\n",
        "            query_dim=hidden_channels,\n",
        "            att_dim=att_dim,\n",
        "            add_bias=att_add_bias)\n",
        "\n",
        "        if rnn_type == \"srnn\":\n",
        "            self.rnn = nn.RNN(input_size=in_channels + emb_channels,\n",
        "                              hidden_size=hidden_channels,\n",
        "                              num_layers=num_layers,\n",
        "                              nonlinearity=activation,\n",
        "                              batch_first=True,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=False)\n",
        "        elif rnn_type == \"lstm\":\n",
        "            self.rnn = nn.LSTM(input_size=in_channels + emb_channels,\n",
        "                               hidden_size=hidden_channels,\n",
        "                               num_layers=num_layers,\n",
        "                               batch_first=True,\n",
        "                               dropout=dropout,\n",
        "                               bidirectional=False,\n",
        "                               proj_size=proj_size)\n",
        "        elif rnn_type == \"gru\":\n",
        "            self.rnn = nn.GRU(input_size=in_channels + emb_channels,\n",
        "                              hidden_size=hidden_channels,\n",
        "                              num_layers=num_layers,\n",
        "                              batch_first=True,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=False)\n",
        "        self.head = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.dec_hstate = None\n",
        "        self.attw = None\n",
        "\n",
        "    def init_dec_hstate(self, enc_hstate, init_as_zero=False):\n",
        "        if init_as_zero:\n",
        "            dec_hstate = torch.zeros_like(enc_hstate)\n",
        "        else:\n",
        "            dec_hstate = enc_hstate\n",
        "        # To avoid error at RNN layer.\n",
        "        self.dec_hstate = dec_hstate.contiguous()\n",
        "\n",
        "    def forward(self,\n",
        "                dec_inputs,\n",
        "                enc_seqs,\n",
        "                enc_mask):\n",
        "        assert self.dec_hstate is not None, f\"dec_hstate has not been initialized.\"\n",
        "        dec_hstate = self.dec_hstate\n",
        "\n",
        "        # Attention layer requires hidden state of 2nd rnn layer.\n",
        "        # as `[N, 1, C]`\n",
        "        query = dec_hstate[-1].unsqueeze(1)\n",
        "        cvec, self.attw = self.att_layer(\n",
        "            key=enc_seqs,\n",
        "            value=enc_seqs,\n",
        "            query=query,\n",
        "            mask=enc_mask)\n",
        "\n",
        "        emb_out = self.emb_layer(dec_inputs)\n",
        "        # `[N, C] -> [N, 1, C]`\n",
        "        emb_out = emb_out.reshape([-1, 1, emb_out.shape[-1]])\n",
        "        feature = torch.cat([cvec, emb_out], dim=-1)\n",
        "        if isinstance(self.rnn, nn.LSTM):\n",
        "            hidden_seqs, (last_hstate, last_cstate) = self.rnn(feature,\n",
        "                                                               dec_hstate)\n",
        "        else:\n",
        "            hidden_seqs, last_hstate = self.rnn(feature,\n",
        "                                                dec_hstate)\n",
        "            last_cstate = None\n",
        "\n",
        "        output_dec = self.head(hidden_seqs)\n",
        "        self.dec_hstate = last_hstate\n",
        "        return output_dec"
      ],
      "metadata": {
        "id": "UZ4Me3ZRX-GL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN CSLR model"
      ],
      "metadata": {
        "id": "IvVTsyTsYPOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCSLR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_in_channels,\n",
        "                 enc_hidden_channels,\n",
        "                 enc_rnn_type,\n",
        "                 enc_num_layers,\n",
        "                 enc_activation,\n",
        "                 enc_bidir,\n",
        "                 enc_dropout,\n",
        "                 enc_apply_mask,\n",
        "                 enc_proj_size,\n",
        "                 dec_in_channels,\n",
        "                 dec_hidden_channels,\n",
        "                 dec_out_channels,\n",
        "                 dec_emb_channels,\n",
        "                 dec_att_dim,\n",
        "                 dec_att_add_bias,\n",
        "                 dec_rnn_type,\n",
        "                 dec_num_layers,\n",
        "                 dec_activation,\n",
        "                 dec_dropout,\n",
        "                 dec_padding_val,\n",
        "                 dec_proj_size):\n",
        "        super().__init__()\n",
        "        self.enc_bidir = enc_bidir\n",
        "\n",
        "        self.linear = nn.Linear(enc_in_channels, enc_hidden_channels)\n",
        "        self.enc_activation = nn.ReLU()\n",
        "\n",
        "        self.encoder = RNNEncoder(\n",
        "            in_channels=enc_hidden_channels,\n",
        "            out_channels=enc_hidden_channels,\n",
        "            rnn_type=enc_rnn_type,\n",
        "            num_layers=enc_num_layers,\n",
        "            activation=enc_activation,\n",
        "            bidir=enc_bidir,\n",
        "            dropout=enc_dropout,\n",
        "            apply_mask=enc_apply_mask,\n",
        "            proj_size=enc_proj_size)\n",
        "\n",
        "        if enc_bidir:\n",
        "            dec_in_channels *= 2\n",
        "            dec_hidden_channels *= 2\n",
        "            dec_att_dim *= 2\n",
        "\n",
        "        self.decoder = BahdanauRNNDecoder(\n",
        "            in_channels=dec_in_channels,\n",
        "            hidden_channels=dec_hidden_channels,\n",
        "            out_channels=dec_out_channels,\n",
        "            emb_channels=dec_emb_channels,\n",
        "            att_dim=dec_att_dim,\n",
        "            att_add_bias=dec_att_add_bias,\n",
        "            rnn_type=dec_rnn_type,\n",
        "            num_layers=dec_num_layers,\n",
        "            activation=dec_activation,\n",
        "            dropout=dec_dropout,\n",
        "            padding_val=dec_padding_val,\n",
        "            proj_size=dec_proj_size)\n",
        "\n",
        "    def _apply_encoder(self, feature, feature_pad_mask=None):\n",
        "        # Feature extraction.\n",
        "        # `[N, C, T, J] -> [N, T, C, J] -> [N, T, C*J] -> [N, T, C']`\n",
        "        N, C, T, J = feature.shape\n",
        "        feature = feature.permute([0, 2, 1, 3])\n",
        "        feature = feature.reshape(N, T, -1)\n",
        "\n",
        "        feature = self.linear(feature)\n",
        "        feature = self.enc_activation(feature)\n",
        "\n",
        "        # Apply encoder.\n",
        "        enc_seqs, enc_hstate = self.encoder(feature, feature_pad_mask)[:2]\n",
        "\n",
        "        # Basically, decoder should not be bidirectional.\n",
        "        # So, we should concatenate backwarded feature.\n",
        "        if self.enc_bidir:\n",
        "            # `[2*layers, N, C] -> [layers, N, 2*C]`\n",
        "            enc_hstate = torch.permute(enc_hstate, [1, 0, 2])\n",
        "            enc_hstate = enc_hstate.reshape([enc_hstate.shape[0],\n",
        "                                             enc_hstate.shape[1] // 2,\n",
        "                                             -1])\n",
        "            enc_hstate = torch.permute(enc_hstate, [1, 0, 2])\n",
        "        return enc_seqs, enc_hstate\n",
        "\n",
        "    def forward(self,\n",
        "                feature, tokens,\n",
        "                feature_pad_mask=None, tokens_pad_mask=None):\n",
        "        \"\"\"Forward computation for train.\n",
        "        \"\"\"\n",
        "        enc_seqs, enc_hstate = self._apply_encoder(feature, feature_pad_mask)\n",
        "\n",
        "        # Apply decoder.\n",
        "        self.decoder.init_dec_hstate(enc_hstate)\n",
        "        dec_inputs = tokens[:, 0:1]\n",
        "        preds = None\n",
        "        for t_index in range(1, tokens.shape[-1]):\n",
        "            pred = self.decoder(\n",
        "                dec_inputs=dec_inputs,\n",
        "                enc_seqs=enc_seqs,\n",
        "                enc_mask=feature_pad_mask)\n",
        "            if preds is None:\n",
        "                preds = pred\n",
        "            else:\n",
        "                # `[N, T, C]`\n",
        "                preds = torch.cat([preds, pred], dim=1)\n",
        "\n",
        "            # Teacher forcing.\n",
        "            dec_inputs = tokens[:, t_index:t_index+1]\n",
        "        return preds\n",
        "\n",
        "    def inference(self,\n",
        "                  feature,\n",
        "                  start_id,\n",
        "                  end_id,\n",
        "                  feature_pad_mask=None,\n",
        "                  max_seqlen=62):\n",
        "        \"\"\"Forward computation for test.\n",
        "        \"\"\"\n",
        "        enc_seqs, enc_hstate = self._apply_encoder(feature, feature_pad_mask)\n",
        "\n",
        "        # Apply decoder.\n",
        "        self.decoder.init_dec_hstate(enc_hstate)\n",
        "        dec_inputs = torch.tensor([start_id]).to(feature.device)\n",
        "        # `[N, T]`\n",
        "        dec_inputs = dec_inputs.reshape([1, 1])\n",
        "        preds = None\n",
        "        pred_ids = [start_id]\n",
        "        for _ in range(max_seqlen):\n",
        "            pred = self.decoder(\n",
        "                dec_inputs=dec_inputs,\n",
        "                enc_seqs=enc_seqs,\n",
        "                enc_mask=feature_pad_mask)\n",
        "            if preds is None:\n",
        "                preds = pred\n",
        "            else:\n",
        "                # `[N, T, C]`\n",
        "                preds = torch.cat([preds, pred], dim=1)\n",
        "\n",
        "            pid = torch.argmax(pred, dim=-1)\n",
        "            dec_inputs = pid\n",
        "\n",
        "            pid = pid.reshape([1]).detach().cpu().numpy()[0]\n",
        "            pred_ids.append(int(pid))\n",
        "            if int(pid) == end_id:\n",
        "                break\n",
        "\n",
        "        # `[N, T]`\n",
        "        pred_ids = np.array([pred_ids])\n",
        "        return pred_ids, preds"
      ],
      "metadata": {
        "id": "QHuAwB3VYRfE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Update to train CSLR model"
      ],
      "metadata": {
        "id": "6vRfzdWzaRFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add transformation to insert keywords"
      ],
      "metadata": {
        "id": "ho8wlf3UcNCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InsertTokensForS2S():\n",
        "    def __init__(self,\n",
        "                 sos_token,\n",
        "                 eos_token,\n",
        "                 error_at_exist=False):\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.error_at_exist = error_at_exist\n",
        "\n",
        "    def check_format(self, tokens):\n",
        "        insert_sos = False\n",
        "        if tokens[0] != self.sos_token:\n",
        "            insert_sos = True\n",
        "        elif self.error_at_exist:\n",
        "            message = f\"The sos_token:{self.sos_token} is exist in {tokens}.\" \\\n",
        "                + \"Please check the format.\"\n",
        "            raise ValueError(message)\n",
        "        insert_eos = False\n",
        "        if tokens[-1] != self.eos_token:\n",
        "            insert_eos = True\n",
        "        elif self.error_at_exist:\n",
        "            message = f\"The eos_token:{self.eos_token} is exist in {tokens}.\" \\\n",
        "                + \"Please check the format.\"\n",
        "            raise ValueError(message)\n",
        "        return insert_sos, insert_eos\n",
        "\n",
        "    def __call__(self, data):\n",
        "\n",
        "        tokens = data[\"token\"]\n",
        "        dtype = tokens.dtype\n",
        "\n",
        "        insert_sos, insert_eos = self.check_format(tokens)\n",
        "        # Insert.\n",
        "        new_tokens = []\n",
        "        if insert_sos:\n",
        "            new_tokens.append(self.sos_token)\n",
        "        new_tokens += tokens.tolist()\n",
        "        if insert_eos:\n",
        "            new_tokens.append(self.eos_token)\n",
        "        new_tokens = np.array(new_tokens, dtype=dtype)\n",
        "        data[\"token\"] = new_tokens\n",
        "        return data"
      ],
      "metadata": {
        "id": "5A9SURRYaW9d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update merge function to create padded token"
      ],
      "metadata": {
        "id": "pOioAkXmgg2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_padded_batch(batch,\n",
        "                       feature_shape,\n",
        "                       token_shape,\n",
        "                       feature_padding_val=0,\n",
        "                       token_padding_val=0):\n",
        "    feature_batch = [sample[\"feature\"] for sample in batch]\n",
        "    token_batch = [sample[\"token\"] for sample in batch]\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge feature.\n",
        "    # ==========================================================\n",
        "    # `[B, C, T, J]`\n",
        "    merged_shape = [len(batch), *feature_shape]\n",
        "    # Use maximum frame length in a batch as padded length.\n",
        "    if merged_shape[2] == -1:\n",
        "        tlen = max([feature.shape[1] for feature in feature_batch])\n",
        "        merged_shape[2] = tlen\n",
        "    merged_feature = merge(feature_batch, merged_shape, padding_val=feature_padding_val)\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge tocken.\n",
        "    # ==========================================================\n",
        "    # `[B, L]`\n",
        "    merged_shape = [len(batch), *token_shape]\n",
        "    # Use maximum token length in a batch as padded length.\n",
        "    if merged_shape[1] == -1:\n",
        "        tlen = max([token.shape[0] for token in token_batch])\n",
        "        merged_shape[1] = tlen\n",
        "    merged_token = merge(token_batch, merged_shape, padding_val=token_padding_val)\n",
        "\n",
        "    # Generate padding mask.\n",
        "    # Pad: 0, Signal: 1\n",
        "    # The frames which all channels and landmarks are equals to padding value\n",
        "    # should be padded.\n",
        "    feature_pad_mask = merged_feature == feature_padding_val\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=1)\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=-1)\n",
        "    feature_pad_mask = torch.logical_not(feature_pad_mask)\n",
        "    token_pad_mask = torch.logical_not(merged_token == token_padding_val)\n",
        "\n",
        "    retval = {\n",
        "        \"feature\": merged_feature,\n",
        "        \"token\": merged_token,\n",
        "        \"feature_pad_mask\": feature_pad_mask,\n",
        "        \"token_pad_mask\": token_pad_mask}\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Lw2jeDOTgtbc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update train, validation, and test loops"
      ],
      "metadata": {
        "id": "CmD69vQahEMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(model, feature, tokens, feature_pad_mask, tokens_pad_mask):\n",
        "    if isinstance(model, RNNCSLR):\n",
        "        preds = model(feature,\n",
        "                      tokens,\n",
        "                      feature_pad_mask=feature_pad_mask,\n",
        "                      tokens_pad_mask=tokens_pad_mask)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model type:{type(model)}.\")\n",
        "    return preds"
      ],
      "metadata": {
        "id": "r-CoDrCOhKy8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, feature, start_id, end_id, max_seqlen=62):\n",
        "    if isinstance(model, RNNCSLR):\n",
        "        pred_ids, _ = model.inference(feature,\n",
        "                                      start_id,\n",
        "                                      end_id,\n",
        "                                      max_seqlen=max_seqlen)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model type:{type(model)}.\")\n",
        "    return pred_ids"
      ],
      "metadata": {
        "id": "CryTqBaihbgE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_tokens_format(tokens, tokens_pad_mask, start_id, end_id):\n",
        "    # Check token's format.\n",
        "    end_indices0 = np.arange(len(tokens))\n",
        "    end_indices1 = tokens_pad_mask.sum(dim=-1).detach().cpu().numpy() - 1\n",
        "    message = \"The start and/or end ids are not included in tokens. \" \\\n",
        "        f\"Please check data format. start_id:{start_id}, \" \\\n",
        "        f\"end_id:{end_id}, enc_indices:{end_indices1}, tokens:{tokens}\"\n",
        "    ref_tokens = tokens.detach().cpu().numpy()\n",
        "    assert (ref_tokens[:, 0] == start_id).all(), message\n",
        "    assert (ref_tokens[end_indices0, end_indices1] == end_id).all(), message"
      ],
      "metadata": {
        "id": "Rchi8Hx2hZCs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_csir_s2s(dataloader,\n",
        "                        model,\n",
        "                        loss_fn,\n",
        "                        optimizer,\n",
        "                        device,\n",
        "                        start_id,\n",
        "                        end_id,\n",
        "                        return_pred_times=False):\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss = 0\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to training mode.\n",
        "    model.train()\n",
        "    # Main loop.\n",
        "    print(\"Start training.\")\n",
        "    start = time.perf_counter()\n",
        "    for batch_idx, batch_sample in enumerate(dataloader):\n",
        "        feature = batch_sample[\"feature\"]\n",
        "        feature_pad_mask = batch_sample[\"feature_pad_mask\"]\n",
        "        tokens = batch_sample[\"token\"]\n",
        "        tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "        check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "        feature = feature.to(device)\n",
        "        feature_pad_mask = feature_pad_mask.to(device)\n",
        "        tokens = tokens.to(device)\n",
        "        tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "        frames = feature.shape[-2]\n",
        "\n",
        "        # Predict.\n",
        "        pred_start = time.perf_counter()\n",
        "        preds = forward(model, feature, tokens,\n",
        "                        feature_pad_mask, tokens_pad_mask)\n",
        "        pred_end = time.perf_counter()\n",
        "        pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "        # Compute loss.\n",
        "        # Preds do not include <start>, so skip that of tokens.\n",
        "        loss = 0\n",
        "        if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "            for t_index in range(1, tokens.shape[-1]):\n",
        "                pred = preds[:, t_index-1, :]\n",
        "                token = tokens[:, t_index]\n",
        "                loss += loss_fn(pred, token)\n",
        "            loss /= tokens.shape[-1]\n",
        "        # LabelSmoothingCrossEntropyLoss\n",
        "        else:\n",
        "            # `[N, T, C] -> [N, C, T]`\n",
        "            preds = preds.permute([0, 2, 1])\n",
        "            # Remove prediction after the last token.\n",
        "            if preds.shape[-1] == tokens.shape[-1]:\n",
        "                preds = preds[:, :, :-1]\n",
        "            loss = loss_fn(preds, tokens[:, 1:])\n",
        "\n",
        "        # Back propagation.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Print current loss per 100 steps.\n",
        "        if batch_idx % 100 == 0:\n",
        "            loss = loss.item()\n",
        "            steps = batch_idx * len(feature)\n",
        "            print(f\"loss:{loss:>7f} [{steps:>5d}/{size:>5d}]\")\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "    # Average loss.\n",
        "    train_loss /= num_batches\n",
        "    print(\"Training performance: \\n\",\n",
        "          f\"Avg loss:{train_loss:>8f}\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (train_loss, pred_times) if return_pred_times else train_loss\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Fd3z-wjohe1d"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_loop_csir_s2s(dataloader,\n",
        "                      model,\n",
        "                      loss_fn,\n",
        "                      device,\n",
        "                      start_id,\n",
        "                      end_id,\n",
        "                      return_pred_times=False):\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss = 0\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to evaluation mode.\n",
        "    model.eval()\n",
        "    # Main loop.\n",
        "    print(\"Start validation.\")\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_sample in enumerate(dataloader):\n",
        "            feature = batch_sample[\"feature\"]\n",
        "            feature_pad_mask = batch_sample[\"feature_pad_mask\"]\n",
        "            tokens = batch_sample[\"token\"]\n",
        "            tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "            check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "            feature = feature.to(device)\n",
        "            feature_pad_mask = feature_pad_mask.to(device)\n",
        "            tokens = tokens.to(device)\n",
        "            tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "            frames = feature.shape[-2]\n",
        "\n",
        "            # Predict.\n",
        "            pred_start = time.perf_counter()\n",
        "            preds = forward(model, feature, tokens,\n",
        "                            feature_pad_mask, tokens_pad_mask)\n",
        "            pred_end = time.perf_counter()\n",
        "            pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "            # Compute loss.\n",
        "            # Preds do not include <start>, so skip that of tokens.\n",
        "            loss = 0\n",
        "            if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "                for t_index in range(1, tokens.shape[-1]):\n",
        "                    pred = preds[:, t_index-1, :]\n",
        "                    token = tokens[:, t_index]\n",
        "                    loss += loss_fn(pred, token)\n",
        "                loss /= tokens.shape[-1]\n",
        "            # LabelSmoothingCrossEntropyLoss\n",
        "            else:\n",
        "                # `[N, T, C] -> [N, C, T]`\n",
        "                preds = preds.permute([0, 2, 1])\n",
        "                # Remove prediction after the last token.\n",
        "                if preds.shape[-1] == tokens.shape[-1]:\n",
        "                    preds = preds[:, :, :-1]\n",
        "                loss = loss_fn(preds, tokens[:, 1:])\n",
        "\n",
        "            val_loss += loss.item()\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average loss.\n",
        "    val_loss /= num_batches\n",
        "    print(\"Validation performance: \\n\",\n",
        "          f\"Avg loss:{val_loss:>8f}\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (val_loss, pred_times) if return_pred_times else val_loss\n",
        "    return retval"
      ],
      "metadata": {
        "id": "ewn5yeIYhj0d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop_csir_s2s(dataloader,\n",
        "                       model,\n",
        "                       device,\n",
        "                       start_id,\n",
        "                       end_id,\n",
        "                       return_pred_times=False,\n",
        "                       max_seqlen=62):\n",
        "    size = len(dataloader.dataset)\n",
        "    total_wer = 0\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to evaluation mode.\n",
        "    model.eval()\n",
        "    # Main loop.\n",
        "    print(\"Start test.\")\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_sample in enumerate(dataloader):\n",
        "            feature = batch_sample[\"feature\"]\n",
        "            tokens = batch_sample[\"token\"]\n",
        "            tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "            check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "            feature = feature.to(device)\n",
        "            tokens = tokens.to(device)\n",
        "            tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "            frames = feature.shape[-2]\n",
        "\n",
        "            # Predict.\n",
        "            pred_start = time.perf_counter()\n",
        "            pred_ids = inference(model, feature, start_id, end_id, max_seqlen=max_seqlen)\n",
        "            pred_end = time.perf_counter()\n",
        "            pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "            # Compute WER.\n",
        "            # <sos> and <eos> should be removed because they may boost performance.\n",
        "            # print(tokens)\n",
        "            # print(pred_ids)\n",
        "            tokens = tokens[0, 1:-1]\n",
        "            # pred_ids = pred_ids[0, 1:-1]\n",
        "            pred_ids = [pid for pid in pred_ids[0] if pid not in [start_id, end_id]]\n",
        "            ref_length = len(tokens)\n",
        "            wer = edit_distance(tokens, pred_ids)\n",
        "            wer /= ref_length\n",
        "            total_wer += wer\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average WER.\n",
        "    awer = total_wer / size * 100\n",
        "    print(\"Test performance: \\n\",\n",
        "          f\"Avg WER:{awer:>0.1f}%\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (awer, pred_times) if return_pred_times else awer\n",
        "    return retval"
      ],
      "metadata": {
        "id": "55N9jLUGhmpd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sanity check"
      ],
      "metadata": {
        "id": "nMXYp4-gZVRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access check.\n",
        "dataset_dir = Path(\"gafs_dataset_very_small\")\n",
        "files = list(dataset_dir.iterdir())\n",
        "dictionary = [fin for fin in files if \".json\" in fin.name][0]\n",
        "hdf5_files = [fin for fin in files if \".hdf5\" in fin.name]\n",
        "\n",
        "print(dictionary)\n",
        "print(hdf5_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc19b-2pZXQE",
        "outputId": "d3a312fe-9880-4fd1-bab1-099ba88abdcf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gafs_dataset_very_small/character_to_prediction_index.json\n",
            "[PosixPath('gafs_dataset_very_small/153.hdf5'), PosixPath('gafs_dataset_very_small/176.hdf5'), PosixPath('gafs_dataset_very_small/231.hdf5'), PosixPath('gafs_dataset_very_small/13.hdf5'), PosixPath('gafs_dataset_very_small/141.hdf5'), PosixPath('gafs_dataset_very_small/192.hdf5'), PosixPath('gafs_dataset_very_small/145.hdf5'), PosixPath('gafs_dataset_very_small/157.hdf5'), PosixPath('gafs_dataset_very_small/74.hdf5'), PosixPath('gafs_dataset_very_small/158.hdf5'), PosixPath('gafs_dataset_very_small/0.hdf5'), PosixPath('gafs_dataset_very_small/72.hdf5'), PosixPath('gafs_dataset_very_small/10.hdf5'), PosixPath('gafs_dataset_very_small/63.hdf5'), PosixPath('gafs_dataset_very_small/188.hdf5'), PosixPath('gafs_dataset_very_small/151.hdf5'), PosixPath('gafs_dataset_very_small/117.hdf5'), PosixPath('gafs_dataset_very_small/202.hdf5'), PosixPath('gafs_dataset_very_small/105.hdf5'), PosixPath('gafs_dataset_very_small/92.hdf5'), PosixPath('gafs_dataset_very_small/236.hdf5'), PosixPath('gafs_dataset_very_small/233.hdf5'), PosixPath('gafs_dataset_very_small/143.hdf5'), PosixPath('gafs_dataset_very_small/169.hdf5'), PosixPath('gafs_dataset_very_small/4.hdf5'), PosixPath('gafs_dataset_very_small/20.hdf5'), PosixPath('gafs_dataset_very_small/251.hdf5'), PosixPath('gafs_dataset_very_small/128.hdf5'), PosixPath('gafs_dataset_very_small/6.hdf5'), PosixPath('gafs_dataset_very_small/125.hdf5'), PosixPath('gafs_dataset_very_small/161.hdf5'), PosixPath('gafs_dataset_very_small/59.hdf5'), PosixPath('gafs_dataset_very_small/225.hdf5'), PosixPath('gafs_dataset_very_small/227.hdf5'), PosixPath('gafs_dataset_very_small/241.hdf5'), PosixPath('gafs_dataset_very_small/24.hdf5'), PosixPath('gafs_dataset_very_small/121.hdf5'), PosixPath('gafs_dataset_very_small/219.hdf5'), PosixPath('gafs_dataset_very_small/181.hdf5'), PosixPath('gafs_dataset_very_small/135.hdf5'), PosixPath('gafs_dataset_very_small/109.hdf5'), PosixPath('gafs_dataset_very_small/239.hdf5'), PosixPath('gafs_dataset_very_small/81.hdf5'), PosixPath('gafs_dataset_very_small/113.hdf5'), PosixPath('gafs_dataset_very_small/56.hdf5'), PosixPath('gafs_dataset_very_small/122.hdf5'), PosixPath('gafs_dataset_very_small/115.hdf5'), PosixPath('gafs_dataset_very_small/21.hdf5'), PosixPath('gafs_dataset_very_small/27.hdf5'), PosixPath('gafs_dataset_very_small/160.hdf5'), PosixPath('gafs_dataset_very_small/68.hdf5'), PosixPath('gafs_dataset_very_small/38.hdf5'), PosixPath('gafs_dataset_very_small/203.hdf5'), PosixPath('gafs_dataset_very_small/102.hdf5'), PosixPath('gafs_dataset_very_small/18.hdf5'), PosixPath('gafs_dataset_very_small/88.hdf5'), PosixPath('gafs_dataset_very_small/254.hdf5'), PosixPath('gafs_dataset_very_small/71.hdf5'), PosixPath('gafs_dataset_very_small/15.hdf5'), PosixPath('gafs_dataset_very_small/95.hdf5'), PosixPath('gafs_dataset_very_small/73.hdf5'), PosixPath('gafs_dataset_very_small/196.hdf5'), PosixPath('gafs_dataset_very_small/53.hdf5'), PosixPath('gafs_dataset_very_small/1.hdf5'), PosixPath('gafs_dataset_very_small/70.hdf5'), PosixPath('gafs_dataset_very_small/242.hdf5'), PosixPath('gafs_dataset_very_small/136.hdf5'), PosixPath('gafs_dataset_very_small/43.hdf5'), PosixPath('gafs_dataset_very_small/154.hdf5'), PosixPath('gafs_dataset_very_small/9.hdf5'), PosixPath('gafs_dataset_very_small/138.hdf5'), PosixPath('gafs_dataset_very_small/112.hdf5'), PosixPath('gafs_dataset_very_small/76.hdf5'), PosixPath('gafs_dataset_very_small/25.hdf5'), PosixPath('gafs_dataset_very_small/178.hdf5'), PosixPath('gafs_dataset_very_small/216.hdf5'), PosixPath('gafs_dataset_very_small/230.hdf5'), PosixPath('gafs_dataset_very_small/80.hdf5'), PosixPath('gafs_dataset_very_small/2.hdf5'), PosixPath('gafs_dataset_very_small/93.hdf5'), PosixPath('gafs_dataset_very_small/89.hdf5'), PosixPath('gafs_dataset_very_small/187.hdf5'), PosixPath('gafs_dataset_very_small/246.hdf5'), PosixPath('gafs_dataset_very_small/147.hdf5'), PosixPath('gafs_dataset_very_small/107.hdf5'), PosixPath('gafs_dataset_very_small/168.hdf5'), PosixPath('gafs_dataset_very_small/186.hdf5'), PosixPath('gafs_dataset_very_small/217.hdf5'), PosixPath('gafs_dataset_very_small/171.hdf5'), PosixPath('gafs_dataset_very_small/36.hdf5'), PosixPath('gafs_dataset_very_small/223.hdf5'), PosixPath('gafs_dataset_very_small/40.hdf5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dictionary.\n",
        "with open(dictionary, \"r\") as fread:\n",
        "    key2token = json.load(fread)\n",
        "\n",
        "VOCAB = len(key2token)\n",
        "# Add keywords.\n",
        "key2token[\"<sos>\"] = VOCAB\n",
        "key2token[\"<eos>\"] = VOCAB + 1\n",
        "key2token[\"<pad>\"] = VOCAB + 2\n",
        "# Reset.\n",
        "VOCAB = len(key2token)"
      ],
      "metadata": {
        "id": "BPoxLXpDZchs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, use_landmarks = get_fullbody_landmarks()\n",
        "use_features = [\"x\", \"y\"]\n",
        "\n",
        "trans_select_feature = SelectLandmarksAndFeature(landmarks=use_landmarks, features=use_features)\n",
        "trans_repnan = ReplaceNan()\n",
        "trans_norm = PartsBasedNormalization(align_mode=\"framewise\", scale_mode=\"unique\")\n",
        "trans_insert_token = InsertTokensForS2S(sos_token=key2token[\"<sos>\"], eos_token=key2token[\"<eos>\"])\n",
        "\n",
        "pre_transforms = Compose([\n",
        "    trans_select_feature,\n",
        "    trans_repnan,\n",
        "    trans_insert_token,\n",
        "    trans_norm\n",
        "])\n",
        "\n",
        "train_transforms = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "val_transforms = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = Compose([\n",
        "    ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "I6KEUSeqZf2U"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "feature_shape = (len(use_features), -1, len(use_landmarks))\n",
        "token_shape = (-1,)\n",
        "merge_fn = partial(merge_padded_batch,\n",
        "                   feature_shape=feature_shape,\n",
        "                   token_shape=token_shape,\n",
        "                   feature_padding_val=0.0,\n",
        "                   token_padding_val=key2token[\"<pad>\"])\n",
        "\n",
        "dataset = HDF5Dataset(hdf5_files, pre_transforms=pre_transforms, transforms=train_transforms)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=merge_fn)\n",
        "try:\n",
        "    data = next(iter(dataloader))\n",
        "    feature_origin = data[\"feature\"]\n",
        "    tokens_origin = data[\"token\"]\n",
        "\n",
        "    print(feature_origin.shape)\n",
        "    print(tokens_origin)\n",
        "except Exception as inst:\n",
        "    print(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rExP4cCiGAm",
        "outputId": "bee29deb-ff50-4250-aceb-82ee2ca990f3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 130, 130])\n",
            "tensor([[59, 17, 19, 16, 21,  0, 33, 46, 52, 45, 51, 56,  0, 43, 46, 46, 47, 60,\n",
            "         61],\n",
            "        [59, 23, 19, 23, 21, 20,  0, 49, 46, 40, 43, 36, 56,  0, 43, 32, 45, 36,\n",
            "         60]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model.\n",
        "# in_channels: J * C (130*2=260)\n",
        "#   J: use_landmarks (130)\n",
        "#   C: use_channels (2)\n",
        "# out_channels: 10\n",
        "in_channels = len(use_landmarks) * len(use_features)\n",
        "out_channels = VOCAB\n",
        "enc_hidden_channels = 64\n",
        "dec_hidden_channels = 64\n",
        "dec_emb_channels = 4\n",
        "dec_att_dim = 64\n",
        "model = RNNCSLR(\n",
        "    enc_in_channels=in_channels,\n",
        "    enc_hidden_channels=enc_hidden_channels,\n",
        "    enc_rnn_type=\"gru\",\n",
        "    enc_num_layers=2,\n",
        "    enc_activation=\"relu\",\n",
        "    enc_bidir=True,\n",
        "    enc_dropout=0.1,\n",
        "    enc_apply_mask=True,\n",
        "    enc_proj_size=0,\n",
        "    dec_in_channels=enc_hidden_channels,\n",
        "    dec_hidden_channels=dec_hidden_channels,\n",
        "    dec_out_channels=out_channels,\n",
        "    dec_emb_channels=dec_emb_channels,\n",
        "    dec_att_dim=dec_att_dim,\n",
        "    dec_att_add_bias=True,\n",
        "    dec_rnn_type=\"gru\",\n",
        "    dec_num_layers=2,\n",
        "    dec_activation=\"relu\",\n",
        "    dec_dropout=0.1,\n",
        "    dec_padding_val=key2token[\"<pad>\"],\n",
        "    dec_proj_size=0)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Sanity check.\n",
        "sample = next(iter(dataloader))\n",
        "logit = model(sample[\"feature\"],\n",
        "              tokens=sample[\"token\"],\n",
        "              feature_pad_mask=sample[\"feature_pad_mask\"])\n",
        "print(logit.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yQKHqQtiUTE",
        "outputId": "8c51f0d8-416f-4b2f-f2fa-4d2e6eee084b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNCSLR(\n",
            "  (linear): Linear(in_features=260, out_features=64, bias=True)\n",
            "  (enc_activation): ReLU()\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (decoder): BahdanauRNNDecoder(\n",
            "    (emb_layer): Embedding(62, 4, padding_idx=61)\n",
            "    (att_layer): SingleHeadAttention(\n",
            "      (att_energy): BahdanauAttentionEnergy(\n",
            "        (w_key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_out): Linear(in_features=128, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (rnn): GRU(132, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (head): Linear(in_features=128, out_features=62, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([2, 18, 62])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train and evaluation"
      ],
      "metadata": {
        "id": "r4o9bbOMjG_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Set common parameters"
      ],
      "metadata": {
        "id": "22_3Ne9ojKLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set common parameters.\n",
        "batch_size = 32\n",
        "load_into_ram = True\n",
        "test_pid = 0\n",
        "num_workers = os.cpu_count()\n",
        "print(f\"Using {num_workers} cores for data loading.\")\n",
        "lr = 3e-4\n",
        "label_smoothing = 0.1\n",
        "sos_token = key2token[\"<sos>\"]\n",
        "eos_token = key2token[\"<eos>\"]\n",
        "pad_token = key2token[\"<pad>\"]\n",
        "max_seqlen = 31\n",
        "\n",
        "epochs = 50\n",
        "eval_every_n_epochs = 1\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} for computation.\")\n",
        "\n",
        "train_hdf5files = [fin for fin in hdf5_files if str(test_pid) not in fin.name]\n",
        "val_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "test_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "\n",
        "_, use_landmarks = get_fullbody_landmarks()\n",
        "use_features = [\"x\", \"y\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3UpkewijNd0",
        "outputId": "42f35eff-2ca6-4d0f-9ef9-19736186f646"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2 cores for data loading.\n",
            "Using cuda for computation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataloaders.\n",
        "train_dataset = HDF5Dataset(train_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=train_transforms, load_into_ram=load_into_ram)\n",
        "val_dataset = HDF5Dataset(val_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=val_transforms, load_into_ram=load_into_ram)\n",
        "test_dataset = HDF5Dataset(test_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=test_transforms, load_into_ram=load_into_ram)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=merge_fn, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "RiTJhuSSjW8k"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Run training process"
      ],
      "metadata": {
        "id": "idjHfhW4jerd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = RNNCSLR(\n",
        "    enc_in_channels=in_channels,\n",
        "    enc_hidden_channels=enc_hidden_channels,\n",
        "    enc_rnn_type=\"gru\",\n",
        "    enc_num_layers=2,\n",
        "    enc_activation=\"relu\",\n",
        "    enc_bidir=True,\n",
        "    enc_dropout=0.1,\n",
        "    enc_apply_mask=True,\n",
        "    enc_proj_size=0,\n",
        "    dec_in_channels=enc_hidden_channels,\n",
        "    dec_hidden_channels=dec_hidden_channels,\n",
        "    dec_out_channels=out_channels,\n",
        "    dec_emb_channels=dec_emb_channels,\n",
        "    dec_att_dim=dec_att_dim,\n",
        "    dec_att_add_bias=True,\n",
        "    dec_rnn_type=\"gru\",\n",
        "    dec_num_layers=2,\n",
        "    dec_activation=\"relu\",\n",
        "    dec_dropout=0.1,\n",
        "    dec_padding_val=pad_token,\n",
        "    dec_proj_size=0)\n",
        "\n",
        "print(model_rnn)\n",
        "\n",
        "loss_fn = LabelSmoothingCrossEntropyLoss(\n",
        "    ignore_indices=pad_token, reduction=\"mean_temporal_prior\",\n",
        "    label_smoothing=label_smoothing)\n",
        "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAXnUtz5jheb",
        "outputId": "9ecb8e6f-e4f5-4598-90fd-b0c0430a6b57"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNCSLR(\n",
            "  (linear): Linear(in_features=260, out_features=64, bias=True)\n",
            "  (enc_activation): ReLU()\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (decoder): BahdanauRNNDecoder(\n",
            "    (emb_layer): Embedding(62, 4, padding_idx=61)\n",
            "    (att_layer): SingleHeadAttention(\n",
            "      (att_energy): BahdanauAttentionEnergy(\n",
            "        (w_key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_out): Linear(in_features=128, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (rnn): GRU(132, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (head): Linear(in_features=128, out_features=62, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, validation, and evaluation.\n",
        "model_rnn.to(device)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_wers = []\n",
        "print(\"Start training.\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "    train_loss, train_times = train_loop_csir_s2s(\n",
        "        train_dataloader, model_rnn, loss_fn, optimizer, device,\n",
        "        sos_token, eos_token,\n",
        "        return_pred_times=True)\n",
        "    val_loss, val_times = val_loop_csir_s2s(\n",
        "        val_dataloader, model_rnn, loss_fn, device,\n",
        "        sos_token, eos_token,\n",
        "        return_pred_times=True)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if (epoch+1) % eval_every_n_epochs == 0:\n",
        "        wer, test_times = test_loop_csir_s2s(\n",
        "            test_dataloader, model_rnn, device,\n",
        "            sos_token, eos_token,\n",
        "            return_pred_times=True,\n",
        "            max_seqlen=max_seqlen)\n",
        "        test_wers.append(wer)\n",
        "train_losses_rnn = np.array(train_losses)\n",
        "val_losses_rnn = np.array(val_losses)\n",
        "test_wers_rnn = np.array(test_wers)\n",
        "\n",
        "val_losses_rnn = np.array(val_losses_rnn)\n",
        "test_wers_rnn = np.array(test_wers_rnn)\n",
        "print(f\"Minimum validation loss:{val_losses_rnn.min()} at {np.argmin(val_losses_rnn)+1} epoch.\")\n",
        "print(f\"Minimum WER:{test_wers_rnn.min()} at {np.argmin(test_wers_rnn)*eval_every_n_epochs+1} epoch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSxf_6kBjzAk",
        "outputId": "f29c0ce8-4806-4f57-cb37-0f7c0274f54a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training.\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1\n",
            "Start training.\n",
            "loss:4.121930 [    0/ 2513]\n",
            "Done. Time:18.338289789000015\n",
            "Training performance: \n",
            " Avg loss:3.739936\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3446640120000097\n",
            "Validation performance: \n",
            " Avg loss:3.596616\n",
            "\n",
            "Start test.\n",
            "Done. Time:13.198322569000027\n",
            "Test performance: \n",
            " Avg WER:91.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Start training.\n",
            "loss:3.595436 [    0/ 2513]\n",
            "Done. Time:15.546841942000015\n",
            "Training performance: \n",
            " Avg loss:3.585216\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3367734139999925\n",
            "Validation performance: \n",
            " Avg loss:3.573433\n",
            "\n",
            "Start test.\n",
            "Done. Time:15.55787591699999\n",
            "Test performance: \n",
            " Avg WER:92.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Start training.\n",
            "loss:3.595432 [    0/ 2513]\n",
            "Done. Time:15.787991280999961\n",
            "Training performance: \n",
            " Avg loss:3.558898\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.334750079999992\n",
            "Validation performance: \n",
            " Avg loss:3.520789\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.706548564000002\n",
            "Test performance: \n",
            " Avg WER:126.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Start training.\n",
            "loss:3.533038 [    0/ 2513]\n",
            "Done. Time:15.588439795999989\n",
            "Training performance: \n",
            " Avg loss:3.437632\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3546836419999977\n",
            "Validation performance: \n",
            " Avg loss:3.346727\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.350375688000042\n",
            "Test performance: \n",
            " Avg WER:118.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Start training.\n",
            "loss:3.264567 [    0/ 2513]\n",
            "Done. Time:15.806740041000012\n",
            "Training performance: \n",
            " Avg loss:3.309385\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.136633503999974\n",
            "Validation performance: \n",
            " Avg loss:3.290513\n",
            "\n",
            "Start test.\n",
            "Done. Time:17.700002467000047\n",
            "Test performance: \n",
            " Avg WER:97.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Start training.\n",
            "loss:3.276779 [    0/ 2513]\n",
            "Done. Time:15.520803376999993\n",
            "Training performance: \n",
            " Avg loss:3.255078\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3481005660000278\n",
            "Validation performance: \n",
            " Avg loss:3.230819\n",
            "\n",
            "Start test.\n",
            "Done. Time:15.51876748899997\n",
            "Test performance: \n",
            " Avg WER:92.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Start training.\n",
            "loss:3.277709 [    0/ 2513]\n",
            "Done. Time:15.527839816999972\n",
            "Training performance: \n",
            " Avg loss:3.215930\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3575449649999882\n",
            "Validation performance: \n",
            " Avg loss:3.199277\n",
            "\n",
            "Start test.\n",
            "Done. Time:17.215576228999964\n",
            "Test performance: \n",
            " Avg WER:92.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Start training.\n",
            "loss:3.146217 [    0/ 2513]\n",
            "Done. Time:15.60065631599997\n",
            "Training performance: \n",
            " Avg loss:3.180021\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.375674914000001\n",
            "Validation performance: \n",
            " Avg loss:3.167315\n",
            "\n",
            "Start test.\n",
            "Done. Time:15.549877572000014\n",
            "Test performance: \n",
            " Avg WER:88.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Start training.\n",
            "loss:3.060927 [    0/ 2513]\n",
            "Done. Time:15.432306729999937\n",
            "Training performance: \n",
            " Avg loss:3.146296\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.429261292000092\n",
            "Validation performance: \n",
            " Avg loss:3.130596\n",
            "\n",
            "Start test.\n",
            "Done. Time:17.18106646199999\n",
            "Test performance: \n",
            " Avg WER:89.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Start training.\n",
            "loss:3.306554 [    0/ 2513]\n",
            "Done. Time:15.885684065000078\n",
            "Training performance: \n",
            " Avg loss:3.113964\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.237480077999976\n",
            "Validation performance: \n",
            " Avg loss:3.125143\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.60545877499999\n",
            "Test performance: \n",
            " Avg WER:96.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Start training.\n",
            "loss:3.064988 [    0/ 2513]\n",
            "Done. Time:15.96061505199998\n",
            "Training performance: \n",
            " Avg loss:3.076815\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3644921000000068\n",
            "Validation performance: \n",
            " Avg loss:3.067281\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.603867874000002\n",
            "Test performance: \n",
            " Avg WER:91.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Start training.\n",
            "loss:3.092510 [    0/ 2513]\n",
            "Done. Time:15.412113424999916\n",
            "Training performance: \n",
            " Avg loss:3.043439\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3610960480000358\n",
            "Validation performance: \n",
            " Avg loss:3.030142\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.047905345000004\n",
            "Test performance: \n",
            " Avg WER:88.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Start training.\n",
            "loss:2.878265 [    0/ 2513]\n",
            "Done. Time:15.77875097499998\n",
            "Training performance: \n",
            " Avg loss:3.012131\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5172497619998921\n",
            "Validation performance: \n",
            " Avg loss:3.005320\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.607686482000076\n",
            "Test performance: \n",
            " Avg WER:87.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Start training.\n",
            "loss:3.006450 [    0/ 2513]\n",
            "Done. Time:15.664608656000041\n",
            "Training performance: \n",
            " Avg loss:2.984498\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3475041940000665\n",
            "Validation performance: \n",
            " Avg loss:2.977901\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.769240929000034\n",
            "Test performance: \n",
            " Avg WER:91.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Start training.\n",
            "loss:2.905471 [    0/ 2513]\n",
            "Done. Time:16.27583345000005\n",
            "Training performance: \n",
            " Avg loss:2.958918\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.152049140000031\n",
            "Validation performance: \n",
            " Avg loss:2.956463\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.242409093000106\n",
            "Test performance: \n",
            " Avg WER:88.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Start training.\n",
            "loss:2.979843 [    0/ 2513]\n",
            "Done. Time:15.656525311999985\n",
            "Training performance: \n",
            " Avg loss:2.939753\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3539097189999438\n",
            "Validation performance: \n",
            " Avg loss:2.937601\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.37266950700007\n",
            "Test performance: \n",
            " Avg WER:91.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Start training.\n",
            "loss:2.883225 [    0/ 2513]\n",
            "Done. Time:15.781027965000021\n",
            "Training performance: \n",
            " Avg loss:2.919081\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3579210099999273\n",
            "Validation performance: \n",
            " Avg loss:2.922428\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.295829375999915\n",
            "Test performance: \n",
            " Avg WER:90.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Start training.\n",
            "loss:2.989004 [    0/ 2513]\n",
            "Done. Time:17.84971410000003\n",
            "Training performance: \n",
            " Avg loss:2.900404\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.7490761750000274\n",
            "Validation performance: \n",
            " Avg loss:2.912207\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.78156182700002\n",
            "Test performance: \n",
            " Avg WER:94.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Start training.\n",
            "loss:2.873226 [    0/ 2513]\n",
            "Done. Time:15.705936377999933\n",
            "Training performance: \n",
            " Avg loss:2.885429\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3861662240000214\n",
            "Validation performance: \n",
            " Avg loss:2.890654\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.944510885\n",
            "Test performance: \n",
            " Avg WER:91.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Start training.\n",
            "loss:2.782708 [    0/ 2513]\n",
            "Done. Time:15.734229424999967\n",
            "Training performance: \n",
            " Avg loss:2.867496\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.363323701000013\n",
            "Validation performance: \n",
            " Avg loss:2.881040\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.78095943400001\n",
            "Test performance: \n",
            " Avg WER:91.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Start training.\n",
            "loss:2.927285 [    0/ 2513]\n",
            "Done. Time:15.60564135200002\n",
            "Training performance: \n",
            " Avg loss:2.852314\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4671065349999708\n",
            "Validation performance: \n",
            " Avg loss:2.875950\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.01838701500003\n",
            "Test performance: \n",
            " Avg WER:93.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Start training.\n",
            "loss:2.819701 [    0/ 2513]\n",
            "Done. Time:15.60832622800001\n",
            "Training performance: \n",
            " Avg loss:2.842627\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3504480850001528\n",
            "Validation performance: \n",
            " Avg loss:2.855825\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.674353005000057\n",
            "Test performance: \n",
            " Avg WER:91.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Start training.\n",
            "loss:2.797743 [    0/ 2513]\n",
            "Done. Time:15.883284715999935\n",
            "Training performance: \n",
            " Avg loss:2.826402\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.400727991000167\n",
            "Validation performance: \n",
            " Avg loss:2.845021\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.59829314600006\n",
            "Test performance: \n",
            " Avg WER:88.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Start training.\n",
            "loss:2.791048 [    0/ 2513]\n",
            "Done. Time:15.962830166000003\n",
            "Training performance: \n",
            " Avg loss:2.814849\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.560547243999963\n",
            "Validation performance: \n",
            " Avg loss:2.840325\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.430173036000042\n",
            "Test performance: \n",
            " Avg WER:95.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Start training.\n",
            "loss:2.682765 [    0/ 2513]\n",
            "Done. Time:17.73384081900008\n",
            "Training performance: \n",
            " Avg loss:2.804218\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3810646349998024\n",
            "Validation performance: \n",
            " Avg loss:2.824874\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.05374218199995\n",
            "Test performance: \n",
            " Avg WER:88.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Start training.\n",
            "loss:2.817242 [    0/ 2513]\n",
            "Done. Time:15.953999195000051\n",
            "Training performance: \n",
            " Avg loss:2.792046\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3632333980001476\n",
            "Validation performance: \n",
            " Avg loss:2.820613\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.244316629999958\n",
            "Test performance: \n",
            " Avg WER:86.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Start training.\n",
            "loss:2.947366 [    0/ 2513]\n",
            "Done. Time:16.309091336999927\n",
            "Training performance: \n",
            " Avg loss:2.781226\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1551371349999044\n",
            "Validation performance: \n",
            " Avg loss:2.808587\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.532230721999895\n",
            "Test performance: \n",
            " Avg WER:86.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Start training.\n",
            "loss:2.758286 [    0/ 2513]\n",
            "Done. Time:15.85469681099994\n",
            "Training performance: \n",
            " Avg loss:2.771381\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3740168920000997\n",
            "Validation performance: \n",
            " Avg loss:2.798760\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.35935342400012\n",
            "Test performance: \n",
            " Avg WER:88.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Start training.\n",
            "loss:2.626563 [    0/ 2513]\n",
            "Done. Time:15.991319927999939\n",
            "Training performance: \n",
            " Avg loss:2.758604\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3835143590001735\n",
            "Validation performance: \n",
            " Avg loss:2.799759\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.957640087000073\n",
            "Test performance: \n",
            " Avg WER:87.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Start training.\n",
            "loss:2.712250 [    0/ 2513]\n",
            "Done. Time:15.677614701000039\n",
            "Training performance: \n",
            " Avg loss:2.749649\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.824528558000111\n",
            "Validation performance: \n",
            " Avg loss:2.784232\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.79401782700006\n",
            "Test performance: \n",
            " Avg WER:88.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Start training.\n",
            "loss:2.703509 [    0/ 2513]\n",
            "Done. Time:16.051540931999853\n",
            "Training performance: \n",
            " Avg loss:2.737612\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3691038759998264\n",
            "Validation performance: \n",
            " Avg loss:2.776419\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.752287719999913\n",
            "Test performance: \n",
            " Avg WER:87.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Start training.\n",
            "loss:2.701334 [    0/ 2513]\n",
            "Done. Time:16.49058870099998\n",
            "Training performance: \n",
            " Avg loss:2.727994\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1285764460001246\n",
            "Validation performance: \n",
            " Avg loss:2.767068\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.644373074999976\n",
            "Test performance: \n",
            " Avg WER:88.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Start training.\n",
            "loss:2.687929 [    0/ 2513]\n",
            "Done. Time:15.836345944999948\n",
            "Training performance: \n",
            " Avg loss:2.718048\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5490462970001317\n",
            "Validation performance: \n",
            " Avg loss:2.762759\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.16688239699988\n",
            "Test performance: \n",
            " Avg WER:85.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Start training.\n",
            "loss:2.670069 [    0/ 2513]\n",
            "Done. Time:15.810869161000028\n",
            "Training performance: \n",
            " Avg loss:2.709818\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.359166230000028\n",
            "Validation performance: \n",
            " Avg loss:2.756864\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.53166736000003\n",
            "Test performance: \n",
            " Avg WER:86.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Start training.\n",
            "loss:2.773003 [    0/ 2513]\n",
            "Done. Time:16.078631534000124\n",
            "Training performance: \n",
            " Avg loss:2.700020\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.0352875109999786\n",
            "Validation performance: \n",
            " Avg loss:2.749777\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.160826228000133\n",
            "Test performance: \n",
            " Avg WER:89.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Start training.\n",
            "loss:2.811329 [    0/ 2513]\n",
            "Done. Time:15.629238722000082\n",
            "Training performance: \n",
            " Avg loss:2.690629\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3963161189999482\n",
            "Validation performance: \n",
            " Avg loss:2.740151\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.033259718999943\n",
            "Test performance: \n",
            " Avg WER:90.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Start training.\n",
            "loss:2.756131 [    0/ 2513]\n",
            "Done. Time:15.823118309000165\n",
            "Training performance: \n",
            " Avg loss:2.679384\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.361688709000191\n",
            "Validation performance: \n",
            " Avg loss:2.735013\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.068398991999857\n",
            "Test performance: \n",
            " Avg WER:85.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Start training.\n",
            "loss:2.607864 [    0/ 2513]\n",
            "Done. Time:15.599597989999893\n",
            "Training performance: \n",
            " Avg loss:2.671109\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.7270569239999531\n",
            "Validation performance: \n",
            " Avg loss:2.735820\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.984179326000003\n",
            "Test performance: \n",
            " Avg WER:82.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Start training.\n",
            "loss:2.607127 [    0/ 2513]\n",
            "Done. Time:15.584635785000046\n",
            "Training performance: \n",
            " Avg loss:2.659979\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3491184549998252\n",
            "Validation performance: \n",
            " Avg loss:2.719281\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.163674390000097\n",
            "Test performance: \n",
            " Avg WER:86.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Start training.\n",
            "loss:2.601132 [    0/ 2513]\n",
            "Done. Time:16.002905370999997\n",
            "Training performance: \n",
            " Avg loss:2.648625\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.538867345999961\n",
            "Validation performance: \n",
            " Avg loss:2.714928\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.614325721000114\n",
            "Test performance: \n",
            " Avg WER:87.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Start training.\n",
            "loss:2.589705 [    0/ 2513]\n",
            "Done. Time:15.634897977000037\n",
            "Training performance: \n",
            " Avg loss:2.639730\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.361002874000178\n",
            "Validation performance: \n",
            " Avg loss:2.706146\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.216755538000143\n",
            "Test performance: \n",
            " Avg WER:86.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Start training.\n",
            "loss:2.612699 [    0/ 2513]\n",
            "Done. Time:15.569812008000099\n",
            "Training performance: \n",
            " Avg loss:2.632125\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3713868229999662\n",
            "Validation performance: \n",
            " Avg loss:2.703905\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.58701263199987\n",
            "Test performance: \n",
            " Avg WER:85.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Start training.\n",
            "loss:2.668662 [    0/ 2513]\n",
            "Done. Time:15.606934569000032\n",
            "Training performance: \n",
            " Avg loss:2.618927\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.8992884410001807\n",
            "Validation performance: \n",
            " Avg loss:2.697477\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.683555324000054\n",
            "Test performance: \n",
            " Avg WER:87.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Start training.\n",
            "loss:2.638704 [    0/ 2513]\n",
            "Done. Time:15.578969971000106\n",
            "Training performance: \n",
            " Avg loss:2.607865\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3574118879998878\n",
            "Validation performance: \n",
            " Avg loss:2.687964\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.767735938999976\n",
            "Test performance: \n",
            " Avg WER:85.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Start training.\n",
            "loss:2.553202 [    0/ 2513]\n",
            "Done. Time:16.190791906999948\n",
            "Training performance: \n",
            " Avg loss:2.599330\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.508296800999915\n",
            "Validation performance: \n",
            " Avg loss:2.682555\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.117326236000054\n",
            "Test performance: \n",
            " Avg WER:85.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Start training.\n",
            "loss:2.545847 [    0/ 2513]\n",
            "Done. Time:15.464112916999966\n",
            "Training performance: \n",
            " Avg loss:2.591554\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3486508260000392\n",
            "Validation performance: \n",
            " Avg loss:2.677080\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.606662788999984\n",
            "Test performance: \n",
            " Avg WER:84.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Start training.\n",
            "loss:2.598696 [    0/ 2513]\n",
            "Done. Time:17.50432560599984\n",
            "Training performance: \n",
            " Avg loss:2.579762\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3690431469999567\n",
            "Validation performance: \n",
            " Avg loss:2.672265\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.494651986000008\n",
            "Test performance: \n",
            " Avg WER:84.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Start training.\n",
            "loss:2.459656 [    0/ 2513]\n",
            "Done. Time:16.13904221200005\n",
            "Training performance: \n",
            " Avg loss:2.570955\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1523759649999192\n",
            "Validation performance: \n",
            " Avg loss:2.667380\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.01365014199996\n",
            "Test performance: \n",
            " Avg WER:84.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Start training.\n",
            "loss:2.552823 [    0/ 2513]\n",
            "Done. Time:15.576077899999973\n",
            "Training performance: \n",
            " Avg loss:2.560557\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.349800340999991\n",
            "Validation performance: \n",
            " Avg loss:2.663042\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.871716702999947\n",
            "Test performance: \n",
            " Avg WER:82.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Start training.\n",
            "loss:2.489989 [    0/ 2513]\n",
            "Done. Time:15.80501011699971\n",
            "Training performance: \n",
            " Avg loss:2.550732\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3715531919997375\n",
            "Validation performance: \n",
            " Avg loss:2.662228\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.242761181000333\n",
            "Test performance: \n",
            " Avg WER:88.6%\n",
            "\n",
            "Minimum validation loss:2.662227764725685 at 50 epoch.\n",
            "Minimum WER:82.62594280168555 at 49 epoch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot result"
      ],
      "metadata": {
        "id": "bI_5zh65kfss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.grid(axis=\"y\", linestyle=\"dotted\", color=\"k\")\n",
        "\n",
        "xs = np.arange(1, len(val_losses_rnn)+1)\n",
        "plt.plot(xs, val_losses_rnn, label=\"RNN\", marker=\".\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.ylim([0.0, 5.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "kQ1seXMDkiHj",
        "outputId": "5a9da7bf-5607-497a-f500-259403d61912"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid(axis=\"y\", linestyle=\"dotted\", color=\"k\")\n",
        "\n",
        "xs = np.arange(1, len(test_wers_rnn)+1)\n",
        "plt.plot(xs, test_wers_rnn, label=\"RNN\", marker=\".\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"WER\")\n",
        "plt.ylim([0.0, 100.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "4Uq2CQVDk9ol",
        "outputId": "176665c3-a83e-4c2d-8acb-3e140a118bea"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}