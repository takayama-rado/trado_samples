{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPj23wBMzksSiUx1HPso+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takayama-rado/trado_samples/blob/main/colab_files/gafs_rnn_encoder_decoder_cslr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download dataset and modules"
      ],
      "metadata": {
        "id": "eQgl6G6nE7D0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADnfnnDUE0t1",
        "outputId": "018cd076-ecea-4199-807b-98d131938f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to local.\n",
        "!cp ./drive/MyDrive/Datasets/gafs_dataset_very_small.zip gafs_dataset.zip"
      ],
      "metadata": {
        "id": "1qMzlk1ilsWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o gafs_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnzLCqD8FR7T",
        "outputId": "2c97b26c-dff6-4eca-c072-456bdf354b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gafs_dataset.zip\n",
            "   creating: gafs_dataset_very_small/\n",
            "  inflating: gafs_dataset_very_small/0.hdf5  \n",
            "  inflating: gafs_dataset_very_small/1.hdf5  \n",
            "  inflating: gafs_dataset_very_small/10.hdf5  \n",
            "  inflating: gafs_dataset_very_small/102.hdf5  \n",
            "  inflating: gafs_dataset_very_small/105.hdf5  \n",
            "  inflating: gafs_dataset_very_small/107.hdf5  \n",
            "  inflating: gafs_dataset_very_small/109.hdf5  \n",
            "  inflating: gafs_dataset_very_small/112.hdf5  \n",
            "  inflating: gafs_dataset_very_small/113.hdf5  \n",
            "  inflating: gafs_dataset_very_small/115.hdf5  \n",
            "  inflating: gafs_dataset_very_small/117.hdf5  \n",
            "  inflating: gafs_dataset_very_small/121.hdf5  \n",
            "  inflating: gafs_dataset_very_small/122.hdf5  \n",
            "  inflating: gafs_dataset_very_small/125.hdf5  \n",
            "  inflating: gafs_dataset_very_small/128.hdf5  \n",
            "  inflating: gafs_dataset_very_small/13.hdf5  \n",
            "  inflating: gafs_dataset_very_small/135.hdf5  \n",
            "  inflating: gafs_dataset_very_small/136.hdf5  \n",
            "  inflating: gafs_dataset_very_small/138.hdf5  \n",
            "  inflating: gafs_dataset_very_small/141.hdf5  \n",
            "  inflating: gafs_dataset_very_small/143.hdf5  \n",
            "  inflating: gafs_dataset_very_small/145.hdf5  \n",
            "  inflating: gafs_dataset_very_small/147.hdf5  \n",
            "  inflating: gafs_dataset_very_small/15.hdf5  \n",
            "  inflating: gafs_dataset_very_small/151.hdf5  \n",
            "  inflating: gafs_dataset_very_small/153.hdf5  \n",
            "  inflating: gafs_dataset_very_small/154.hdf5  \n",
            "  inflating: gafs_dataset_very_small/157.hdf5  \n",
            "  inflating: gafs_dataset_very_small/158.hdf5  \n",
            "  inflating: gafs_dataset_very_small/160.hdf5  \n",
            "  inflating: gafs_dataset_very_small/161.hdf5  \n",
            "  inflating: gafs_dataset_very_small/168.hdf5  \n",
            "  inflating: gafs_dataset_very_small/169.hdf5  \n",
            "  inflating: gafs_dataset_very_small/171.hdf5  \n",
            "  inflating: gafs_dataset_very_small/176.hdf5  \n",
            "  inflating: gafs_dataset_very_small/178.hdf5  \n",
            "  inflating: gafs_dataset_very_small/18.hdf5  \n",
            "  inflating: gafs_dataset_very_small/181.hdf5  \n",
            "  inflating: gafs_dataset_very_small/186.hdf5  \n",
            "  inflating: gafs_dataset_very_small/187.hdf5  \n",
            "  inflating: gafs_dataset_very_small/188.hdf5  \n",
            "  inflating: gafs_dataset_very_small/192.hdf5  \n",
            "  inflating: gafs_dataset_very_small/196.hdf5  \n",
            "  inflating: gafs_dataset_very_small/2.hdf5  \n",
            "  inflating: gafs_dataset_very_small/20.hdf5  \n",
            "  inflating: gafs_dataset_very_small/202.hdf5  \n",
            "  inflating: gafs_dataset_very_small/203.hdf5  \n",
            "  inflating: gafs_dataset_very_small/21.hdf5  \n",
            "  inflating: gafs_dataset_very_small/216.hdf5  \n",
            "  inflating: gafs_dataset_very_small/217.hdf5  \n",
            "  inflating: gafs_dataset_very_small/219.hdf5  \n",
            "  inflating: gafs_dataset_very_small/223.hdf5  \n",
            "  inflating: gafs_dataset_very_small/225.hdf5  \n",
            "  inflating: gafs_dataset_very_small/227.hdf5  \n",
            "  inflating: gafs_dataset_very_small/230.hdf5  \n",
            "  inflating: gafs_dataset_very_small/231.hdf5  \n",
            "  inflating: gafs_dataset_very_small/233.hdf5  \n",
            "  inflating: gafs_dataset_very_small/236.hdf5  \n",
            "  inflating: gafs_dataset_very_small/239.hdf5  \n",
            "  inflating: gafs_dataset_very_small/24.hdf5  \n",
            "  inflating: gafs_dataset_very_small/241.hdf5  \n",
            "  inflating: gafs_dataset_very_small/242.hdf5  \n",
            "  inflating: gafs_dataset_very_small/246.hdf5  \n",
            "  inflating: gafs_dataset_very_small/25.hdf5  \n",
            "  inflating: gafs_dataset_very_small/251.hdf5  \n",
            "  inflating: gafs_dataset_very_small/254.hdf5  \n",
            "  inflating: gafs_dataset_very_small/27.hdf5  \n",
            "  inflating: gafs_dataset_very_small/36.hdf5  \n",
            "  inflating: gafs_dataset_very_small/38.hdf5  \n",
            "  inflating: gafs_dataset_very_small/4.hdf5  \n",
            "  inflating: gafs_dataset_very_small/40.hdf5  \n",
            "  inflating: gafs_dataset_very_small/43.hdf5  \n",
            "  inflating: gafs_dataset_very_small/53.hdf5  \n",
            "  inflating: gafs_dataset_very_small/56.hdf5  \n",
            "  inflating: gafs_dataset_very_small/59.hdf5  \n",
            "  inflating: gafs_dataset_very_small/6.hdf5  \n",
            "  inflating: gafs_dataset_very_small/63.hdf5  \n",
            "  inflating: gafs_dataset_very_small/68.hdf5  \n",
            "  inflating: gafs_dataset_very_small/70.hdf5  \n",
            "  inflating: gafs_dataset_very_small/71.hdf5  \n",
            "  inflating: gafs_dataset_very_small/72.hdf5  \n",
            "  inflating: gafs_dataset_very_small/73.hdf5  \n",
            "  inflating: gafs_dataset_very_small/74.hdf5  \n",
            "  inflating: gafs_dataset_very_small/76.hdf5  \n",
            "  inflating: gafs_dataset_very_small/80.hdf5  \n",
            "  inflating: gafs_dataset_very_small/81.hdf5  \n",
            "  inflating: gafs_dataset_very_small/88.hdf5  \n",
            "  inflating: gafs_dataset_very_small/89.hdf5  \n",
            "  inflating: gafs_dataset_very_small/9.hdf5  \n",
            "  inflating: gafs_dataset_very_small/92.hdf5  \n",
            "  inflating: gafs_dataset_very_small/93.hdf5  \n",
            "  inflating: gafs_dataset_very_small/95.hdf5  \n",
            "  inflating: gafs_dataset_very_small/character_to_prediction_index.json  \n",
            "  inflating: gafs_dataset_very_small/LICENSE.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gafs_dataset_very_small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ZWwV56FWVD",
        "outputId": "59abf39b-b33b-4f08-b52d-90560abf6a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.hdf5\t  135.hdf5  160.hdf5  1.hdf5\t236.hdf5  43.hdf5  80.hdf5\n",
            "102.hdf5  136.hdf5  161.hdf5  202.hdf5\t239.hdf5  4.hdf5   81.hdf5\n",
            "105.hdf5  138.hdf5  168.hdf5  203.hdf5\t241.hdf5  53.hdf5  88.hdf5\n",
            "107.hdf5  13.hdf5   169.hdf5  20.hdf5\t242.hdf5  56.hdf5  89.hdf5\n",
            "109.hdf5  141.hdf5  171.hdf5  216.hdf5\t246.hdf5  59.hdf5  92.hdf5\n",
            "10.hdf5   143.hdf5  176.hdf5  217.hdf5\t24.hdf5   63.hdf5  93.hdf5\n",
            "112.hdf5  145.hdf5  178.hdf5  219.hdf5\t251.hdf5  68.hdf5  95.hdf5\n",
            "113.hdf5  147.hdf5  181.hdf5  21.hdf5\t254.hdf5  6.hdf5   9.hdf5\n",
            "115.hdf5  151.hdf5  186.hdf5  223.hdf5\t25.hdf5   70.hdf5  character_to_prediction_index.json\n",
            "117.hdf5  153.hdf5  187.hdf5  225.hdf5\t27.hdf5   71.hdf5  LICENSE.txt\n",
            "121.hdf5  154.hdf5  188.hdf5  227.hdf5\t2.hdf5\t  72.hdf5\n",
            "122.hdf5  157.hdf5  18.hdf5   230.hdf5\t36.hdf5   73.hdf5\n",
            "125.hdf5  158.hdf5  192.hdf5  231.hdf5\t38.hdf5   74.hdf5\n",
            "128.hdf5  15.hdf5   196.hdf5  233.hdf5\t40.hdf5   76.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gafs_dataset_very_small/character_to_prediction_index.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLP9RpORFaLL",
        "outputId": "124da41d-9a56-47d0-80c0-a6b78e0f4781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\" \":0,\"!\":1,\"#\":2,\"$\":3,\"%\":4,\"&\":5,\"'\":6,\"(\":7,\")\":8,\"*\":9,\"+\":10,\",\":11,\"-\":12,\".\":13,\"\\/\":14,\"0\":15,\"1\":16,\"2\":17,\"3\":18,\"4\":19,\"5\":20,\"6\":21,\"7\":22,\"8\":23,\"9\":24,\":\":25,\";\":26,\"=\":27,\"?\":28,\"@\":29,\"[\":30,\"_\":31,\"a\":32,\"b\":33,\"c\":34,\"d\":35,\"e\":36,\"f\":37,\"g\":38,\"h\":39,\"i\":40,\"j\":41,\"k\":42,\"l\":43,\"m\":44,\"n\":45,\"o\":46,\"p\":47,\"q\":48,\"r\":49,\"s\":50,\"t\":51,\"u\":52,\"v\":53,\"w\":54,\"x\":55,\"y\":56,\"z\":57,\"~\":58}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gafs_dataset_very_small/LICENSE.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iQlJCDtU4d8",
        "outputId": "f436c7e2-b349-4ce8-f6c2-fdf421f8dcc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset provided by Natsuki Takayama (Takayama Research and Development Office) is licensed under CC-BY 4.0.\r\n",
            "Author: Copyright 2024 Natsuki Takayama\r\n",
            "Title: GASF very small dataset\r\n",
            "Original licenser: Google LLC\r\n",
            "Modification\r\n",
            "- Extract only 1 parquet file.\r\n",
            "- Packaged into HDF5 format.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "with h5py.File(\"gafs_dataset_very_small/0.hdf5\", \"r\") as fread:\n",
        "    keys = list(fread.keys())\n",
        "    print(keys[:10])\n",
        "    group = fread[keys[0]]\n",
        "    print(group.keys())\n",
        "    feature = group[\"feature\"][:]\n",
        "    token = group[\"token\"][:]\n",
        "    print(feature.shape)\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37SBiPQeVSB7",
        "outputId": "4b164c51-b8de-44cc-c4e6-4a01ea29498e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1720198121', '1722303176', '1723157122', '1731934631', '1737624109', '1739256200', '1743069372', '1743412187', '1744795751', '1746320345']\n",
            "<KeysViewHDF5 ['feature', 'token']>\n",
            "(2, 271, 543)\n",
            "[14 38 32 45 44 36 40 32 43 43 36 56 14 43 40 45 32 12 34 32 49 50 51 36\n",
            " 45 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/takayama-rado/trado_samples/archive/refs/tags/v0.3.4.zip -O master.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjcUxrq2VmlE",
        "outputId": "890a67e4-0fad-4e48-ae9b-03d3c46507e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-10 06:19:15--  https://github.com/takayama-rado/trado_samples/archive/refs/tags/v0.3.4.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/takayama-rado/trado_samples/zip/refs/tags/v0.3.4 [following]\n",
            "--2024-09-10 06:19:15--  https://codeload.github.com/takayama-rado/trado_samples/zip/refs/tags/v0.3.4\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.116.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.116.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [           <=>      ]  76.54M  17.8MB/s    in 4.3s    \n",
            "\n",
            "2024-09-10 06:19:20 (17.8 MB/s) - ‘master.zip’ saved [80254068]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o master.zip -d master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUTwKBOMVw-j",
        "outputId": "6f725a79-691f-4dd3-abf9-672997750c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  master.zip\n",
            "3406d5a0072e08879272e622ff8efdc1c7b78ee8\n",
            "   creating: master/trado_samples-0.3.4/\n",
            "  inflating: master/trado_samples-0.3.4/.gitignore  \n",
            "  inflating: master/trado_samples-0.3.4/LICENSE  \n",
            "  inflating: master/trado_samples-0.3.4/README.md  \n",
            "   creating: master/trado_samples-0.3.4/colab_files/\n",
            " extracting: master/trado_samples-0.3.4/colab_files/.gitkeep  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_affine_np_einsum.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_jax_static.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_mpholistic_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_mpothers_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_jax.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_numpy.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_torch.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_jax.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_numpy_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_numpy_1.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_torch.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gafs_create_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_access_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_conformer_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_create_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_macaronnet_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_normalize_landmarks.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_1.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_2.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_3.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_select_landmarks.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_simple_attention.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_simple_islr_model.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_drop.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_hflip.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_resize.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_saffine.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_snoise.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_tclip.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_tinterp.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_twarping.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_w_ls.ipynb  \n",
            "   creating: master/trado_samples-0.3.4/src/\n",
            "   creating: master/trado_samples-0.3.4/src/modules_gislr/\n",
            " extracting: master/trado_samples-0.3.4/src/modules_gislr/__init__.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/activation.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/dataset.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/defines.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/draw_functions.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/layers.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/train_functions.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/transforms.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/utils.py  \n",
            "   creating: master/trado_samples-0.3.4/test_data/\n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static_affine.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static_interp.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_middle0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_near0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/hand_only.mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv master/trado_samples-0.3.4/src/modules_gislr ."
      ],
      "metadata": {
        "id": "yXsIhVAWVyej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf master master.zip gafs_dataset_very_small.zip"
      ],
      "metadata": {
        "id": "ohykNs7zV3TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koeHiooRV7qr",
        "outputId": "85abe146-910b-4dd6-ff26-2582ada4babe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  gafs_dataset_very_small\tgafs_dataset.zip  modules_gislr  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load library"
      ],
      "metadata": {
        "id": "ddZ2NhLDV8yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from functools import partial\n",
        "from inspect import signature\n",
        "from pathlib import Path\n",
        "\n",
        "# Third party's modules\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import (\n",
        "    DataLoader)\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "# Local modules\n",
        "sys.path.append(\"modules_gislr\")\n",
        "from modules_gislr.dataset import (\n",
        "    HDF5Dataset,\n",
        "    merge)\n",
        "from modules_gislr.defines import (\n",
        "    get_fullbody_landmarks\n",
        ")\n",
        "from modules_gislr.layers import (\n",
        "    RNNEncoder,\n",
        "    apply_norm,\n",
        "    create_norm\n",
        ")\n",
        "from modules_gislr.train_functions import (\n",
        "    LabelSmoothingCrossEntropyLoss\n",
        ")\n",
        "from modules_gislr.transforms import (\n",
        "    PartsBasedNormalization,\n",
        "    ReplaceNan,\n",
        "    SelectLandmarksAndFeature,\n",
        "    ToTensor\n",
        ")\n",
        "from modules_gislr.utils import (\n",
        "    select_reluwise_activation\n",
        ")"
      ],
      "metadata": {
        "id": "8K-wtRChV-n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implement RNN Encoder-Decoder CSLR model"
      ],
      "metadata": {
        "id": "Klup1x0iWlHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention module"
      ],
      "metadata": {
        "id": "aPHXLRYMXE9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttentionEnergy(nn.Module):\n",
        "    def __init__(self,\n",
        "                 key_dim,\n",
        "                 query_dim,\n",
        "                 att_dim,\n",
        "                 add_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_key = nn.Linear(key_dim, att_dim, bias=add_bias)\n",
        "        self.w_query = nn.Linear(query_dim, att_dim, bias=add_bias)\n",
        "        self.w_out = nn.Linear(att_dim, 1, bias=add_bias)\n",
        "\n",
        "    def forward(self, key, query):\n",
        "        # key: `[N, key_len, key_dim]`\n",
        "        # query: `[N, 1, query_dim]`\n",
        "        key = self.w_key(key)\n",
        "        query = self.w_query(query)\n",
        "        # Adding with broadcasting.\n",
        "        # key: `[N, key_len, key_dim]`\n",
        "        # query: `[N, 1, query_dim]`\n",
        "        # query should be broadcasted to `[N, key_len, query_dim]`\n",
        "        temp = key + query\n",
        "        # `[N, key_len, att_dim] -> [N, key_len, 1] -> [N, 1, key_len]`\n",
        "        energy = self.w_out(torch.tanh(temp))\n",
        "        energy = torch.permute(energy, [0, 2, 1])\n",
        "        return energy"
      ],
      "metadata": {
        "id": "9AClEx01WqmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 key_dim,\n",
        "                 query_dim,\n",
        "                 att_dim,\n",
        "                 add_bias):\n",
        "        super().__init__()\n",
        "\n",
        "        self.att_energy = BahdanauAttentionEnergy(\n",
        "            key_dim=key_dim,\n",
        "            query_dim=query_dim,\n",
        "            att_dim=att_dim,\n",
        "            add_bias=add_bias)\n",
        "\n",
        "        self.neg_inf = None\n",
        "\n",
        "    def forward(self,\n",
        "                key,\n",
        "                value,\n",
        "                query,\n",
        "                mask=None):\n",
        "        if self.neg_inf is None:\n",
        "            self.neg_inf = float(np.finfo(\n",
        "                torch.tensor(0, dtype=key.dtype).numpy().dtype).min)\n",
        "\n",
        "        batch, klen, kdim = key.shape\n",
        "        _, qlen, qdim = query.shape\n",
        "        energy = self.att_energy(key=key, query=query)\n",
        "        assert energy.shape == (batch, qlen, klen)\n",
        "\n",
        "        # Apply mask.\n",
        "        if mask is not None:\n",
        "            if len(mask.shape) == 2:\n",
        "                # `[N, klen] -> [N, qlen(=1), klen]`\n",
        "                mask = mask.unsqueeze(1)\n",
        "            # Negative infinity should be 0 in softmax.\n",
        "            energy = energy.masked_fill_(mask==0, self.neg_inf)\n",
        "\n",
        "        # Compute attention mask.\n",
        "        attw = torch.softmax(energy, dim=-1)\n",
        "        # attw: `[N, qlen, klen]`\n",
        "        # value: `[N, klen, kdim]`\n",
        "        # bmm: `[N, qlen, klen] x [N, klen, kdim] -> [N, qlen, kdim]`\n",
        "        cvec = torch.bmm(attw, value)\n",
        "        return cvec, attw"
      ],
      "metadata": {
        "id": "VEy2_ljtXNEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Decoder"
      ],
      "metadata": {
        "id": "IRNyq0OkX8Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauRNNDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 hidden_channels,\n",
        "                 out_channels,\n",
        "                 emb_channels,\n",
        "                 att_dim,\n",
        "                 att_add_bias,\n",
        "                 rnn_type,\n",
        "                 num_layers,\n",
        "                 activation,\n",
        "                 dropout,\n",
        "                 padding_val,\n",
        "                 proj_size=0):\n",
        "        super().__init__()\n",
        "        assert rnn_type in [\"srnn\", \"lstm\", \"gru\"]\n",
        "\n",
        "        self.emb_layer = nn.Embedding(\n",
        "            num_embeddings=out_channels,\n",
        "            embedding_dim=emb_channels,\n",
        "            padding_idx=padding_val)\n",
        "\n",
        "        self.att_layer = SingleHeadAttention(\n",
        "            key_dim=in_channels,\n",
        "            query_dim=hidden_channels,\n",
        "            att_dim=att_dim,\n",
        "            add_bias=att_add_bias)\n",
        "\n",
        "        if rnn_type == \"srnn\":\n",
        "            self.rnn = nn.RNN(input_size=in_channels + emb_channels,\n",
        "                              hidden_size=hidden_channels,\n",
        "                              num_layers=num_layers,\n",
        "                              nonlinearity=activation,\n",
        "                              batch_first=True,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=False)\n",
        "        elif rnn_type == \"lstm\":\n",
        "            self.rnn = nn.LSTM(input_size=in_channels + emb_channels,\n",
        "                               hidden_size=hidden_channels,\n",
        "                               num_layers=num_layers,\n",
        "                               batch_first=True,\n",
        "                               dropout=dropout,\n",
        "                               bidirectional=False,\n",
        "                               proj_size=proj_size)\n",
        "        elif rnn_type == \"gru\":\n",
        "            self.rnn = nn.GRU(input_size=in_channels + emb_channels,\n",
        "                              hidden_size=hidden_channels,\n",
        "                              num_layers=num_layers,\n",
        "                              batch_first=True,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=False)\n",
        "        self.head = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.dec_hstate = None\n",
        "        self.attw = None\n",
        "\n",
        "    def init_dec_hstate(self, enc_hstate, init_as_zero=False):\n",
        "        if init_as_zero:\n",
        "            dec_hstate = torch.zeros_like(enc_hstate)\n",
        "        else:\n",
        "            dec_hstate = enc_hstate\n",
        "        # To avoid error at RNN layer.\n",
        "        self.dec_hstate = dec_hstate.contiguous()\n",
        "\n",
        "    def forward(self,\n",
        "                dec_inputs,\n",
        "                enc_seqs,\n",
        "                enc_mask):\n",
        "        assert self.dec_hstate is not None, f\"dec_hstate has not been initialized.\"\n",
        "        dec_hstate = self.dec_hstate\n",
        "\n",
        "        # Attention layer requires hidden state of 2nd rnn layer.\n",
        "        # as `[N, 1, C]`\n",
        "        query = dec_hstate[-1].unsqueeze(1)\n",
        "        cvec, self.attw = self.att_layer(\n",
        "            key=enc_seqs,\n",
        "            value=enc_seqs,\n",
        "            query=query,\n",
        "            mask=enc_mask)\n",
        "\n",
        "        emb_out = self.emb_layer(dec_inputs)\n",
        "        # `[N, C] -> [N, 1, C]`\n",
        "        emb_out = emb_out.reshape([-1, 1, emb_out.shape[-1]])\n",
        "        feature = torch.cat([cvec, emb_out], dim=-1)\n",
        "        if isinstance(self.rnn, nn.LSTM):\n",
        "            hidden_seqs, (last_hstate, last_cstate) = self.rnn(feature,\n",
        "                                                               dec_hstate)\n",
        "        else:\n",
        "            hidden_seqs, last_hstate = self.rnn(feature,\n",
        "                                                dec_hstate)\n",
        "            last_cstate = None\n",
        "\n",
        "        output_dec = self.head(hidden_seqs)\n",
        "        self.dec_hstate = last_hstate\n",
        "        return output_dec"
      ],
      "metadata": {
        "id": "UZ4Me3ZRX-GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN CSLR model"
      ],
      "metadata": {
        "id": "IvVTsyTsYPOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCSLR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_in_channels,\n",
        "                 enc_hidden_channels,\n",
        "                 enc_rnn_type,\n",
        "                 enc_num_layers,\n",
        "                 enc_activation,\n",
        "                 enc_bidir,\n",
        "                 enc_dropout,\n",
        "                 enc_apply_mask,\n",
        "                 enc_proj_size,\n",
        "                 dec_in_channels,\n",
        "                 dec_hidden_channels,\n",
        "                 dec_out_channels,\n",
        "                 dec_emb_channels,\n",
        "                 dec_att_dim,\n",
        "                 dec_att_add_bias,\n",
        "                 dec_rnn_type,\n",
        "                 dec_num_layers,\n",
        "                 dec_activation,\n",
        "                 dec_dropout,\n",
        "                 dec_padding_val,\n",
        "                 dec_proj_size):\n",
        "        super().__init__()\n",
        "        self.enc_bidir = enc_bidir\n",
        "\n",
        "        self.linear = nn.Linear(enc_in_channels, enc_hidden_channels)\n",
        "        self.enc_activation = nn.ReLU()\n",
        "\n",
        "        self.encoder = RNNEncoder(\n",
        "            in_channels=enc_hidden_channels,\n",
        "            out_channels=enc_hidden_channels,\n",
        "            rnn_type=enc_rnn_type,\n",
        "            num_layers=enc_num_layers,\n",
        "            activation=enc_activation,\n",
        "            bidir=enc_bidir,\n",
        "            dropout=enc_dropout,\n",
        "            apply_mask=enc_apply_mask,\n",
        "            proj_size=enc_proj_size)\n",
        "\n",
        "        if enc_bidir:\n",
        "            dec_in_channels *= 2\n",
        "            dec_hidden_channels *= 2\n",
        "            dec_att_dim *= 2\n",
        "\n",
        "        self.decoder = BahdanauRNNDecoder(\n",
        "            in_channels=dec_in_channels,\n",
        "            hidden_channels=dec_hidden_channels,\n",
        "            out_channels=dec_out_channels,\n",
        "            emb_channels=dec_emb_channels,\n",
        "            att_dim=dec_att_dim,\n",
        "            att_add_bias=dec_att_add_bias,\n",
        "            rnn_type=dec_rnn_type,\n",
        "            num_layers=dec_num_layers,\n",
        "            activation=dec_activation,\n",
        "            dropout=dec_dropout,\n",
        "            padding_val=dec_padding_val,\n",
        "            proj_size=dec_proj_size)\n",
        "\n",
        "    def _apply_encoder(self, feature, feature_pad_mask=None):\n",
        "        # Feature extraction.\n",
        "        # `[N, C, T, J] -> [N, T, C, J] -> [N, T, C*J] -> [N, T, C']`\n",
        "        N, C, T, J = feature.shape\n",
        "        feature = feature.permute([0, 2, 1, 3])\n",
        "        feature = feature.reshape(N, T, -1)\n",
        "\n",
        "        feature = self.linear(feature)\n",
        "        feature = self.enc_activation(feature)\n",
        "\n",
        "        # Apply encoder.\n",
        "        enc_seqs, enc_hstate = self.encoder(feature, feature_pad_mask)[:2]\n",
        "\n",
        "        # Basically, decoder should not be bidirectional.\n",
        "        # So, we should concatenate backwarded feature.\n",
        "        if self.enc_bidir:\n",
        "            # `[2*layers, N, C] -> [layers, N, 2*C]`\n",
        "            enc_hstate = torch.permute(enc_hstate, [1, 0, 2])\n",
        "            enc_hstate = enc_hstate.reshape([enc_hstate.shape[0],\n",
        "                                             enc_hstate.shape[1] // 2,\n",
        "                                             -1])\n",
        "            enc_hstate = torch.permute(enc_hstate, [1, 0, 2])\n",
        "        return enc_seqs, enc_hstate\n",
        "\n",
        "    def forward(self,\n",
        "                feature, tokens,\n",
        "                feature_pad_mask=None, tokens_pad_mask=None):\n",
        "        \"\"\"Forward computation for train.\n",
        "        \"\"\"\n",
        "        enc_seqs, enc_hstate = self._apply_encoder(feature, feature_pad_mask)\n",
        "\n",
        "        # Apply decoder.\n",
        "        self.decoder.init_dec_hstate(enc_hstate)\n",
        "        dec_inputs = tokens[:, 0:1]\n",
        "        preds = None\n",
        "        for t_index in range(1, tokens.shape[-1]):\n",
        "            pred = self.decoder(\n",
        "                dec_inputs=dec_inputs,\n",
        "                enc_seqs=enc_seqs,\n",
        "                enc_mask=feature_pad_mask)\n",
        "            if preds is None:\n",
        "                preds = pred\n",
        "            else:\n",
        "                # `[N, T, C]`\n",
        "                preds = torch.cat([preds, pred], dim=1)\n",
        "\n",
        "            # Teacher forcing.\n",
        "            dec_inputs = tokens[:, t_index:t_index+1]\n",
        "        return preds\n",
        "\n",
        "    def inference(self,\n",
        "                  feature,\n",
        "                  start_id,\n",
        "                  end_id,\n",
        "                  feature_pad_mask=None,\n",
        "                  max_seqlen=62):\n",
        "        \"\"\"Forward computation for test.\n",
        "        \"\"\"\n",
        "        enc_seqs, enc_hstate = self._apply_encoder(feature, feature_pad_mask)\n",
        "\n",
        "        # Apply decoder.\n",
        "        self.decoder.init_dec_hstate(enc_hstate)\n",
        "        dec_inputs = torch.tensor([start_id]).to(feature.device)\n",
        "        # `[N, T]`\n",
        "        dec_inputs = dec_inputs.reshape([1, 1])\n",
        "        preds = None\n",
        "        pred_ids = [start_id]\n",
        "        for _ in range(max_seqlen):\n",
        "            pred = self.decoder(\n",
        "                dec_inputs=dec_inputs,\n",
        "                enc_seqs=enc_seqs,\n",
        "                enc_mask=feature_pad_mask)\n",
        "            if preds is None:\n",
        "                preds = pred\n",
        "            else:\n",
        "                # `[N, T, C]`\n",
        "                preds = torch.cat([preds, pred], dim=1)\n",
        "\n",
        "            pid = torch.argmax(pred, dim=-1)\n",
        "            dec_inputs = pid\n",
        "\n",
        "            pid = pid.reshape([1]).detach().cpu().numpy()[0]\n",
        "            pred_ids.append(int(pid))\n",
        "            if int(pid) == end_id:\n",
        "                break\n",
        "\n",
        "        # `[N, T]`\n",
        "        pred_ids = np.array([pred_ids])\n",
        "        return pred_ids, preds"
      ],
      "metadata": {
        "id": "QHuAwB3VYRfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Update to train CSLR model"
      ],
      "metadata": {
        "id": "6vRfzdWzaRFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add transformation to insert keywords"
      ],
      "metadata": {
        "id": "ho8wlf3UcNCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InsertTokensForS2S():\n",
        "    def __init__(self,\n",
        "                 sos_token,\n",
        "                 eos_token,\n",
        "                 error_at_exist=False):\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.error_at_exist = error_at_exist\n",
        "\n",
        "    def check_format(self, tokens):\n",
        "        insert_sos = False\n",
        "        if tokens[0] != self.sos_token:\n",
        "            insert_sos = True\n",
        "        elif self.error_at_exist:\n",
        "            message = f\"The sos_token:{self.sos_token} is exist in {tokens}.\" \\\n",
        "                + \"Please check the format.\"\n",
        "            raise ValueError(message)\n",
        "        insert_eos = False\n",
        "        if tokens[-1] != self.eos_token:\n",
        "            insert_eos = True\n",
        "        elif self.error_at_exist:\n",
        "            message = f\"The eos_token:{self.eos_token} is exist in {tokens}.\" \\\n",
        "                + \"Please check the format.\"\n",
        "            raise ValueError(message)\n",
        "        return insert_sos, insert_eos\n",
        "\n",
        "    def __call__(self, data):\n",
        "\n",
        "        tokens = data[\"token\"]\n",
        "        dtype = tokens.dtype\n",
        "\n",
        "        insert_sos, insert_eos = self.check_format(tokens)\n",
        "        # Insert.\n",
        "        new_tokens = []\n",
        "        if insert_sos:\n",
        "            new_tokens.append(self.sos_token)\n",
        "        new_tokens += tokens.tolist()\n",
        "        if insert_eos:\n",
        "            new_tokens.append(self.eos_token)\n",
        "        new_tokens = np.array(new_tokens, dtype=dtype)\n",
        "        data[\"token\"] = new_tokens\n",
        "        return data"
      ],
      "metadata": {
        "id": "5A9SURRYaW9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update merge function to create padded token"
      ],
      "metadata": {
        "id": "pOioAkXmgg2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_padded_batch(batch,\n",
        "                       feature_shape,\n",
        "                       token_shape,\n",
        "                       feature_padding_val=0,\n",
        "                       token_padding_val=0):\n",
        "    feature_batch = [sample[\"feature\"] for sample in batch]\n",
        "    token_batch = [sample[\"token\"] for sample in batch]\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge feature.\n",
        "    # ==========================================================\n",
        "    # `[B, C, T, J]`\n",
        "    merged_shape = [len(batch), *feature_shape]\n",
        "    # Use maximum frame length in a batch as padded length.\n",
        "    if merged_shape[2] == -1:\n",
        "        tlen = max([feature.shape[1] for feature in feature_batch])\n",
        "        merged_shape[2] = tlen\n",
        "    merged_feature = merge(feature_batch, merged_shape, padding_val=feature_padding_val)\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge tocken.\n",
        "    # ==========================================================\n",
        "    # `[B, L]`\n",
        "    merged_shape = [len(batch), *token_shape]\n",
        "    # Use maximum token length in a batch as padded length.\n",
        "    if merged_shape[1] == -1:\n",
        "        tlen = max([token.shape[0] for token in token_batch])\n",
        "        merged_shape[1] = tlen\n",
        "    merged_token = merge(token_batch, merged_shape, padding_val=token_padding_val)\n",
        "\n",
        "    # Generate padding mask.\n",
        "    # Pad: 0, Signal: 1\n",
        "    # The frames which all channels and landmarks are equals to padding value\n",
        "    # should be padded.\n",
        "    feature_pad_mask = merged_feature == feature_padding_val\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=1)\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=-1)\n",
        "    feature_pad_mask = torch.logical_not(feature_pad_mask)\n",
        "    token_pad_mask = torch.logical_not(merged_token == token_padding_val)\n",
        "\n",
        "    retval = {\n",
        "        \"feature\": merged_feature,\n",
        "        \"token\": merged_token,\n",
        "        \"feature_pad_mask\": feature_pad_mask,\n",
        "        \"token_pad_mask\": token_pad_mask}\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Lw2jeDOTgtbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update train, validation, and test loops"
      ],
      "metadata": {
        "id": "CmD69vQahEMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(model, feature, tokens, feature_pad_mask, tokens_pad_mask):\n",
        "    if isinstance(model, RNNCSLR):\n",
        "        preds = model(feature,\n",
        "                      tokens,\n",
        "                      feature_pad_mask=feature_pad_mask,\n",
        "                      tokens_pad_mask=tokens_pad_mask)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model type:{type(model)}.\")\n",
        "    return preds"
      ],
      "metadata": {
        "id": "r-CoDrCOhKy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, feature, start_id, end_id, max_seqlen=62):\n",
        "    if isinstance(model, RNNCSLR):\n",
        "        pred_ids, _ = model.inference(feature,\n",
        "                                      start_id,\n",
        "                                      end_id,\n",
        "                                      max_seqlen=max_seqlen)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model type:{type(model)}.\")\n",
        "    return pred_ids"
      ],
      "metadata": {
        "id": "CryTqBaihbgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_tokens_format(tokens, tokens_pad_mask, start_id, end_id):\n",
        "    # Check token's format.\n",
        "    end_indices0 = np.arange(len(tokens))\n",
        "    end_indices1 = tokens_pad_mask.sum(dim=-1).detach().cpu().numpy() - 1\n",
        "    message = \"The start and/or end ids are not included in tokens. \" \\\n",
        "        f\"Please check data format. start_id:{start_id}, \" \\\n",
        "        f\"end_id:{end_id}, enc_indices:{end_indices1}, tokens:{tokens}\"\n",
        "    ref_tokens = tokens.detach().cpu().numpy()\n",
        "    assert (ref_tokens[:, 0] == start_id).all(), message\n",
        "    assert (ref_tokens[end_indices0, end_indices1] == end_id).all(), message"
      ],
      "metadata": {
        "id": "Rchi8Hx2hZCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_csir_s2s(dataloader,\n",
        "                        model,\n",
        "                        loss_fn,\n",
        "                        optimizer,\n",
        "                        device,\n",
        "                        start_id,\n",
        "                        end_id,\n",
        "                        return_pred_times=False):\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss = 0\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to training mode.\n",
        "    model.train()\n",
        "    # Main loop.\n",
        "    print(\"Start training.\")\n",
        "    start = time.perf_counter()\n",
        "    for batch_idx, batch_sample in enumerate(dataloader):\n",
        "        feature = batch_sample[\"feature\"]\n",
        "        feature_pad_mask = batch_sample[\"feature_pad_mask\"]\n",
        "        tokens = batch_sample[\"token\"]\n",
        "        tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "        check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "        feature = feature.to(device)\n",
        "        feature_pad_mask = feature_pad_mask.to(device)\n",
        "        tokens = tokens.to(device)\n",
        "        tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "        frames = feature.shape[-2]\n",
        "\n",
        "        # Predict.\n",
        "        pred_start = time.perf_counter()\n",
        "        preds = forward(model, feature, tokens,\n",
        "                        feature_pad_mask, tokens_pad_mask)\n",
        "        pred_end = time.perf_counter()\n",
        "        pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "        # Compute loss.\n",
        "        # Preds do not include <start>, so skip that of tokens.\n",
        "        loss = 0\n",
        "        if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "            for t_index in range(1, tokens.shape[-1]):\n",
        "                pred = preds[:, t_index-1, :]\n",
        "                token = tokens[:, t_index]\n",
        "                loss += loss_fn(pred, token)\n",
        "            loss /= tokens.shape[-1]\n",
        "        # LabelSmoothingCrossEntropyLoss\n",
        "        else:\n",
        "            # `[N, T, C] -> [N, C, T]`\n",
        "            preds = preds.permute([0, 2, 1])\n",
        "            # Remove prediction after the last token.\n",
        "            if preds.shape[-1] == tokens.shape[-1]:\n",
        "                preds = preds[:, :, :-1]\n",
        "            loss = loss_fn(preds, tokens[:, 1:])\n",
        "\n",
        "        # Back propagation.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Print current loss per 100 steps.\n",
        "        if batch_idx % 100 == 0:\n",
        "            loss = loss.item()\n",
        "            steps = batch_idx * len(feature)\n",
        "            print(f\"loss:{loss:>7f} [{steps:>5d}/{size:>5d}]\")\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "    # Average loss.\n",
        "    train_loss /= num_batches\n",
        "    print(\"Training performance: \\n\",\n",
        "          f\"Avg loss:{train_loss:>8f}\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (train_loss, pred_times) if return_pred_times else train_loss\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Fd3z-wjohe1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_loop_csir_s2s(dataloader,\n",
        "                      model,\n",
        "                      loss_fn,\n",
        "                      device,\n",
        "                      start_id,\n",
        "                      end_id,\n",
        "                      return_pred_times=False):\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss = 0\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to training mode.\n",
        "    model.train()\n",
        "    # Main loop.\n",
        "    print(\"Start validation.\")\n",
        "    start = time.perf_counter()\n",
        "    for batch_idx, batch_sample in enumerate(dataloader):\n",
        "        feature = batch_sample[\"feature\"]\n",
        "        feature_pad_mask = batch_sample[\"feature_pad_mask\"]\n",
        "        tokens = batch_sample[\"token\"]\n",
        "        tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "        check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "        feature = feature.to(device)\n",
        "        feature_pad_mask = feature_pad_mask.to(device)\n",
        "        tokens = tokens.to(device)\n",
        "        tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "        frames = feature.shape[-2]\n",
        "\n",
        "        # Predict.\n",
        "        pred_start = time.perf_counter()\n",
        "        preds = forward(model, feature, tokens,\n",
        "                        feature_pad_mask, tokens_pad_mask)\n",
        "        pred_end = time.perf_counter()\n",
        "        pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "        # Compute loss.\n",
        "        # Preds do not include <start>, so skip that of tokens.\n",
        "        loss = 0\n",
        "        if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "            for t_index in range(1, tokens.shape[-1]):\n",
        "                pred = preds[:, t_index-1, :]\n",
        "                token = tokens[:, t_index]\n",
        "                loss += loss_fn(pred, token)\n",
        "            loss /= tokens.shape[-1]\n",
        "        # LabelSmoothingCrossEntropyLoss\n",
        "        else:\n",
        "            # `[N, T, C] -> [N, C, T]`\n",
        "            preds = preds.permute([0, 2, 1])\n",
        "            # Remove prediction after the last token.\n",
        "            if preds.shape[-1] == tokens.shape[-1]:\n",
        "                preds = preds[:, :, :-1]\n",
        "            loss = loss_fn(preds, tokens[:, 1:])\n",
        "\n",
        "        val_loss += loss.item()\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average loss.\n",
        "    val_loss /= num_batches\n",
        "    print(\"Validation performance: \\n\",\n",
        "          f\"Avg loss:{val_loss:>8f}\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (val_loss, pred_times) if return_pred_times else val_loss\n",
        "    return retval"
      ],
      "metadata": {
        "id": "ewn5yeIYhj0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop_csir_s2s(dataloader,\n",
        "                       model,\n",
        "                       device,\n",
        "                       start_id,\n",
        "                       end_id,\n",
        "                       use_normalized_wer=False,\n",
        "                       return_pred_times=False,\n",
        "                       max_seqlen=62):\n",
        "    size = len(dataloader.dataset)\n",
        "    total_wer = 0\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to training mode.\n",
        "    model.train()\n",
        "    # Main loop.\n",
        "    print(\"Start test.\")\n",
        "    start = time.perf_counter()\n",
        "    for batch_idx, batch_sample in enumerate(dataloader):\n",
        "        feature = batch_sample[\"feature\"]\n",
        "        tokens = batch_sample[\"token\"]\n",
        "        tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "        check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "        feature = feature.to(device)\n",
        "        tokens = tokens.to(device)\n",
        "        tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "        frames = feature.shape[-2]\n",
        "\n",
        "        # Predict.\n",
        "        pred_start = time.perf_counter()\n",
        "        pred_ids = inference(model, feature, start_id, end_id, max_seqlen=max_seqlen)\n",
        "        pred_end = time.perf_counter()\n",
        "        pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "        # Compute WER.\n",
        "        # <sos> and <eos> should be removed because they may boost performance.\n",
        "        # print(tokens)\n",
        "        # print(pred_ids)\n",
        "        tokens = tokens[0, 1:-1]\n",
        "        # pred_ids = pred_ids[0, 1:-1]\n",
        "        pred_ids = [pid for pid in pred_ids[0] if pid not in [start_id, end_id]]\n",
        "        if use_normalized_wer:\n",
        "            ref_length = max(len(pred_ids), len(tokens))\n",
        "        else:\n",
        "            ref_length = len(tokens)\n",
        "        wer = edit_distance(tokens, pred_ids)\n",
        "        wer /= ref_length\n",
        "        total_wer += wer\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average WER.\n",
        "    awer = total_wer / size * 100\n",
        "    print(\"Test performance: \\n\",\n",
        "          f\"Avg WER:{awer:>0.1f}%\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (awer, pred_times) if return_pred_times else awer\n",
        "    return retval"
      ],
      "metadata": {
        "id": "55N9jLUGhmpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sanity check"
      ],
      "metadata": {
        "id": "nMXYp4-gZVRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access check.\n",
        "dataset_dir = Path(\"gafs_dataset_very_small\")\n",
        "files = list(dataset_dir.iterdir())\n",
        "dictionary = [fin for fin in files if \".json\" in fin.name][0]\n",
        "hdf5_files = [fin for fin in files if \".hdf5\" in fin.name]\n",
        "\n",
        "print(dictionary)\n",
        "print(hdf5_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc19b-2pZXQE",
        "outputId": "ff2c6016-86f2-487a-e2a9-3c34da9fec1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gafs_dataset_very_small/character_to_prediction_index.json\n",
            "[PosixPath('gafs_dataset_very_small/10.hdf5'), PosixPath('gafs_dataset_very_small/138.hdf5'), PosixPath('gafs_dataset_very_small/225.hdf5'), PosixPath('gafs_dataset_very_small/145.hdf5'), PosixPath('gafs_dataset_very_small/136.hdf5'), PosixPath('gafs_dataset_very_small/202.hdf5'), PosixPath('gafs_dataset_very_small/178.hdf5'), PosixPath('gafs_dataset_very_small/233.hdf5'), PosixPath('gafs_dataset_very_small/59.hdf5'), PosixPath('gafs_dataset_very_small/53.hdf5'), PosixPath('gafs_dataset_very_small/80.hdf5'), PosixPath('gafs_dataset_very_small/223.hdf5'), PosixPath('gafs_dataset_very_small/9.hdf5'), PosixPath('gafs_dataset_very_small/216.hdf5'), PosixPath('gafs_dataset_very_small/13.hdf5'), PosixPath('gafs_dataset_very_small/38.hdf5'), PosixPath('gafs_dataset_very_small/192.hdf5'), PosixPath('gafs_dataset_very_small/112.hdf5'), PosixPath('gafs_dataset_very_small/43.hdf5'), PosixPath('gafs_dataset_very_small/109.hdf5'), PosixPath('gafs_dataset_very_small/18.hdf5'), PosixPath('gafs_dataset_very_small/188.hdf5'), PosixPath('gafs_dataset_very_small/203.hdf5'), PosixPath('gafs_dataset_very_small/157.hdf5'), PosixPath('gafs_dataset_very_small/92.hdf5'), PosixPath('gafs_dataset_very_small/76.hdf5'), PosixPath('gafs_dataset_very_small/93.hdf5'), PosixPath('gafs_dataset_very_small/36.hdf5'), PosixPath('gafs_dataset_very_small/0.hdf5'), PosixPath('gafs_dataset_very_small/6.hdf5'), PosixPath('gafs_dataset_very_small/160.hdf5'), PosixPath('gafs_dataset_very_small/1.hdf5'), PosixPath('gafs_dataset_very_small/27.hdf5'), PosixPath('gafs_dataset_very_small/117.hdf5'), PosixPath('gafs_dataset_very_small/231.hdf5'), PosixPath('gafs_dataset_very_small/56.hdf5'), PosixPath('gafs_dataset_very_small/168.hdf5'), PosixPath('gafs_dataset_very_small/171.hdf5'), PosixPath('gafs_dataset_very_small/158.hdf5'), PosixPath('gafs_dataset_very_small/128.hdf5'), PosixPath('gafs_dataset_very_small/143.hdf5'), PosixPath('gafs_dataset_very_small/95.hdf5'), PosixPath('gafs_dataset_very_small/251.hdf5'), PosixPath('gafs_dataset_very_small/15.hdf5'), PosixPath('gafs_dataset_very_small/219.hdf5'), PosixPath('gafs_dataset_very_small/141.hdf5'), PosixPath('gafs_dataset_very_small/21.hdf5'), PosixPath('gafs_dataset_very_small/73.hdf5'), PosixPath('gafs_dataset_very_small/151.hdf5'), PosixPath('gafs_dataset_very_small/71.hdf5'), PosixPath('gafs_dataset_very_small/70.hdf5'), PosixPath('gafs_dataset_very_small/72.hdf5'), PosixPath('gafs_dataset_very_small/125.hdf5'), PosixPath('gafs_dataset_very_small/154.hdf5'), PosixPath('gafs_dataset_very_small/239.hdf5'), PosixPath('gafs_dataset_very_small/186.hdf5'), PosixPath('gafs_dataset_very_small/181.hdf5'), PosixPath('gafs_dataset_very_small/187.hdf5'), PosixPath('gafs_dataset_very_small/153.hdf5'), PosixPath('gafs_dataset_very_small/246.hdf5'), PosixPath('gafs_dataset_very_small/115.hdf5'), PosixPath('gafs_dataset_very_small/161.hdf5'), PosixPath('gafs_dataset_very_small/2.hdf5'), PosixPath('gafs_dataset_very_small/107.hdf5'), PosixPath('gafs_dataset_very_small/81.hdf5'), PosixPath('gafs_dataset_very_small/24.hdf5'), PosixPath('gafs_dataset_very_small/105.hdf5'), PosixPath('gafs_dataset_very_small/122.hdf5'), PosixPath('gafs_dataset_very_small/230.hdf5'), PosixPath('gafs_dataset_very_small/242.hdf5'), PosixPath('gafs_dataset_very_small/20.hdf5'), PosixPath('gafs_dataset_very_small/227.hdf5'), PosixPath('gafs_dataset_very_small/89.hdf5'), PosixPath('gafs_dataset_very_small/102.hdf5'), PosixPath('gafs_dataset_very_small/176.hdf5'), PosixPath('gafs_dataset_very_small/241.hdf5'), PosixPath('gafs_dataset_very_small/88.hdf5'), PosixPath('gafs_dataset_very_small/254.hdf5'), PosixPath('gafs_dataset_very_small/169.hdf5'), PosixPath('gafs_dataset_very_small/147.hdf5'), PosixPath('gafs_dataset_very_small/113.hdf5'), PosixPath('gafs_dataset_very_small/40.hdf5'), PosixPath('gafs_dataset_very_small/135.hdf5'), PosixPath('gafs_dataset_very_small/236.hdf5'), PosixPath('gafs_dataset_very_small/74.hdf5'), PosixPath('gafs_dataset_very_small/196.hdf5'), PosixPath('gafs_dataset_very_small/217.hdf5'), PosixPath('gafs_dataset_very_small/25.hdf5'), PosixPath('gafs_dataset_very_small/63.hdf5'), PosixPath('gafs_dataset_very_small/4.hdf5'), PosixPath('gafs_dataset_very_small/121.hdf5'), PosixPath('gafs_dataset_very_small/68.hdf5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dictionary.\n",
        "with open(dictionary, \"r\") as fread:\n",
        "    key2token = json.load(fread)\n",
        "\n",
        "VOCAB = len(key2token)\n",
        "# Add keywords.\n",
        "key2token[\"<sos>\"] = VOCAB\n",
        "key2token[\"<eos>\"] = VOCAB + 1\n",
        "key2token[\"<pad>\"] = VOCAB + 2\n",
        "# Reset.\n",
        "VOCAB = len(key2token)"
      ],
      "metadata": {
        "id": "BPoxLXpDZchs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, use_landmarks = get_fullbody_landmarks()\n",
        "use_features = [\"x\", \"y\"]\n",
        "\n",
        "trans_select_feature = SelectLandmarksAndFeature(landmarks=use_landmarks, features=use_features)\n",
        "trans_repnan = ReplaceNan()\n",
        "trans_norm = PartsBasedNormalization(align_mode=\"framewise\", scale_mode=\"unique\")\n",
        "trans_insert_token = InsertTokensForS2S(sos_token=key2token[\"<sos>\"], eos_token=key2token[\"<eos>\"])\n",
        "\n",
        "pre_transforms = Compose([\n",
        "    trans_select_feature,\n",
        "    trans_repnan,\n",
        "    trans_insert_token,\n",
        "    trans_norm\n",
        "])\n",
        "\n",
        "train_transforms = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "val_transforms = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = Compose([\n",
        "    ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "I6KEUSeqZf2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "feature_shape = (len(use_features), -1, len(use_landmarks))\n",
        "token_shape = (-1,)\n",
        "merge_fn = partial(merge_padded_batch,\n",
        "                   feature_shape=feature_shape,\n",
        "                   token_shape=token_shape,\n",
        "                   feature_padding_val=0.0,\n",
        "                   token_padding_val=key2token[\"<pad>\"])\n",
        "\n",
        "dataset = HDF5Dataset(hdf5_files, pre_transforms=pre_transforms, transforms=train_transforms)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=merge_fn)\n",
        "try:\n",
        "    data = next(iter(dataloader))\n",
        "    feature_origin = data[\"feature\"]\n",
        "    tokens_origin = data[\"token\"]\n",
        "\n",
        "    print(feature_origin.shape)\n",
        "    print(tokens_origin)\n",
        "except Exception as inst:\n",
        "    print(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rExP4cCiGAm",
        "outputId": "b322ca3f-b3d9-4921-93eb-233727817478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 177, 130])\n",
            "tensor([[59, 23, 17,  0, 46, 42, 43, 32, 54, 32, 39, 32,  0, 43, 32, 45, 36, 60,\n",
            "         61, 61, 61, 61],\n",
            "        [59, 16, 21, 21, 23,  0, 43, 32, 33, 52, 49, 45, 52, 44,  0, 50, 51, 49,\n",
            "         36, 36, 51, 60]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model.\n",
        "# in_channels: J * C (130*2=260)\n",
        "#   J: use_landmarks (130)\n",
        "#   C: use_channels (2)\n",
        "# out_channels: 10\n",
        "in_channels = len(use_landmarks) * len(use_features)\n",
        "out_channels = VOCAB\n",
        "enc_hidden_channels = 64\n",
        "dec_hidden_channels = 64\n",
        "dec_emb_channels = 4\n",
        "dec_att_dim = 64\n",
        "model = RNNCSLR(\n",
        "    enc_in_channels=in_channels,\n",
        "    enc_hidden_channels=enc_hidden_channels,\n",
        "    enc_rnn_type=\"gru\",\n",
        "    enc_num_layers=2,\n",
        "    enc_activation=\"relu\",\n",
        "    enc_bidir=True,\n",
        "    enc_dropout=0.1,\n",
        "    enc_apply_mask=True,\n",
        "    enc_proj_size=0,\n",
        "    dec_in_channels=enc_hidden_channels,\n",
        "    dec_hidden_channels=dec_hidden_channels,\n",
        "    dec_out_channels=out_channels,\n",
        "    dec_emb_channels=dec_emb_channels,\n",
        "    dec_att_dim=dec_att_dim,\n",
        "    dec_att_add_bias=True,\n",
        "    dec_rnn_type=\"gru\",\n",
        "    dec_num_layers=2,\n",
        "    dec_activation=\"relu\",\n",
        "    dec_dropout=0.1,\n",
        "    dec_padding_val=key2token[\"<pad>\"],\n",
        "    dec_proj_size=0)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Sanity check.\n",
        "sample = next(iter(dataloader))\n",
        "logit = model(sample[\"feature\"],\n",
        "              tokens=sample[\"token\"],\n",
        "              feature_pad_mask=sample[\"feature_pad_mask\"])\n",
        "print(logit.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yQKHqQtiUTE",
        "outputId": "4872d0d7-66f9-47ca-f5bb-7e31af1b2005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNCSLR(\n",
            "  (linear): Linear(in_features=260, out_features=64, bias=True)\n",
            "  (enc_activation): ReLU()\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (decoder): BahdanauRNNDecoder(\n",
            "    (emb_layer): Embedding(62, 4, padding_idx=61)\n",
            "    (att_layer): SingleHeadAttention(\n",
            "      (att_energy): BahdanauAttentionEnergy(\n",
            "        (w_key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_out): Linear(in_features=128, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (rnn): GRU(132, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (head): Linear(in_features=128, out_features=62, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([2, 21, 62])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train and evaluation"
      ],
      "metadata": {
        "id": "r4o9bbOMjG_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Set common parameters"
      ],
      "metadata": {
        "id": "22_3Ne9ojKLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set common parameters.\n",
        "batch_size = 32\n",
        "load_into_ram = True\n",
        "test_pid = 0\n",
        "num_workers = os.cpu_count()\n",
        "print(f\"Using {num_workers} cores for data loading.\")\n",
        "lr = 3e-4\n",
        "label_smoothing = 0.1\n",
        "sos_token = key2token[\"<sos>\"]\n",
        "eos_token = key2token[\"<eos>\"]\n",
        "pad_token = key2token[\"<pad>\"]\n",
        "max_seqlen = 31\n",
        "\n",
        "epochs = 50\n",
        "eval_every_n_epochs = 1\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} for computation.\")\n",
        "\n",
        "train_hdf5files = [fin for fin in hdf5_files if str(test_pid) not in fin.name]\n",
        "val_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "test_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "\n",
        "_, use_landmarks = get_fullbody_landmarks()\n",
        "use_features = [\"x\", \"y\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3UpkewijNd0",
        "outputId": "9863da22-21e2-4ea0-edd4-47caff92a8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2 cores for data loading.\n",
            "Using cuda for computation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataloaders.\n",
        "train_dataset = HDF5Dataset(train_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=train_transforms, load_into_ram=load_into_ram)\n",
        "val_dataset = HDF5Dataset(val_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=val_transforms, load_into_ram=load_into_ram)\n",
        "test_dataset = HDF5Dataset(test_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=test_transforms, load_into_ram=load_into_ram)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=merge_fn, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "RiTJhuSSjW8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Run training process"
      ],
      "metadata": {
        "id": "idjHfhW4jerd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = RNNCSLR(\n",
        "    enc_in_channels=in_channels,\n",
        "    enc_hidden_channels=enc_hidden_channels,\n",
        "    enc_rnn_type=\"gru\",\n",
        "    enc_num_layers=2,\n",
        "    enc_activation=\"relu\",\n",
        "    enc_bidir=True,\n",
        "    enc_dropout=0.1,\n",
        "    enc_apply_mask=True,\n",
        "    enc_proj_size=0,\n",
        "    dec_in_channels=enc_hidden_channels,\n",
        "    dec_hidden_channels=dec_hidden_channels,\n",
        "    dec_out_channels=out_channels,\n",
        "    dec_emb_channels=dec_emb_channels,\n",
        "    dec_att_dim=dec_att_dim,\n",
        "    dec_att_add_bias=True,\n",
        "    dec_rnn_type=\"gru\",\n",
        "    dec_num_layers=2,\n",
        "    dec_activation=\"relu\",\n",
        "    dec_dropout=0.1,\n",
        "    dec_padding_val=pad_token,\n",
        "    dec_proj_size=0)\n",
        "\n",
        "print(model_rnn)\n",
        "\n",
        "loss_fn = LabelSmoothingCrossEntropyLoss(\n",
        "    ignore_indices=pad_token, reduction=\"mean_temporal_prior\",\n",
        "    label_smoothing=label_smoothing)\n",
        "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAXnUtz5jheb",
        "outputId": "2bdf2b86-4ce0-41a1-cc09-8927394b6a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNCSLR(\n",
            "  (linear): Linear(in_features=260, out_features=64, bias=True)\n",
            "  (enc_activation): ReLU()\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (decoder): BahdanauRNNDecoder(\n",
            "    (emb_layer): Embedding(62, 4, padding_idx=61)\n",
            "    (att_layer): SingleHeadAttention(\n",
            "      (att_energy): BahdanauAttentionEnergy(\n",
            "        (w_key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_out): Linear(in_features=128, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (rnn): GRU(132, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (head): Linear(in_features=128, out_features=62, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, validation, and evaluation.\n",
        "model_rnn.to(device)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_wers = []\n",
        "print(\"Start training.\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "    train_loss, train_times = train_loop_csir_s2s(\n",
        "        train_dataloader, model_rnn, loss_fn, optimizer, device,\n",
        "        sos_token, eos_token,\n",
        "        return_pred_times=True)\n",
        "    val_loss, val_times = val_loop_csir_s2s(\n",
        "        val_dataloader, model_rnn, loss_fn, device,\n",
        "        sos_token, eos_token,\n",
        "        return_pred_times=True)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if (epoch+1) % eval_every_n_epochs == 0:\n",
        "        wer, test_times = test_loop_csir_s2s(\n",
        "            test_dataloader, model_rnn, device,\n",
        "            sos_token, eos_token,\n",
        "            return_pred_times=True,\n",
        "            max_seqlen=max_seqlen)\n",
        "        test_wers.append(wer)\n",
        "train_losses_rnn = np.array(train_losses)\n",
        "val_losses_rnn = np.array(val_losses)\n",
        "test_wers_rnn = np.array(test_wers)\n",
        "\n",
        "val_losses_rnn = np.array(val_losses_rnn)\n",
        "test_wers_rnn = np.array(test_wers_rnn)\n",
        "print(f\"Minimum validation loss:{val_losses_rnn.min()} at {np.argmin(val_losses_rnn)+1} epoch.\")\n",
        "print(f\"Minimum WER:{test_wers_rnn.min()} at {np.argmin(test_wers_rnn)*eval_every_n_epochs+1} epoch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSxf_6kBjzAk",
        "outputId": "bdde2e36-ebd8-4932-b4d2-5172f6f4db5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training.\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1\n",
            "Start training.\n",
            "loss:4.141368 [    0/ 2513]\n",
            "Done. Time:17.759382293999977\n",
            "Training performance: \n",
            " Avg loss:3.753373\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5249640609999915\n",
            "Validation performance: \n",
            " Avg loss:3.604697\n",
            "\n",
            "Start test.\n",
            "Done. Time:13.60654139999997\n",
            "Test performance: \n",
            " Avg WER:91.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Start training.\n",
            "loss:3.596135 [    0/ 2513]\n",
            "Done. Time:15.70272884100001\n",
            "Training performance: \n",
            " Avg loss:3.587052\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.6988455949999661\n",
            "Validation performance: \n",
            " Avg loss:3.575021\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.370194218999984\n",
            "Test performance: \n",
            " Avg WER:91.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Start training.\n",
            "loss:3.566780 [    0/ 2513]\n",
            "Done. Time:15.90304077899998\n",
            "Training performance: \n",
            " Avg loss:3.556449\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1964069299999665\n",
            "Validation performance: \n",
            " Avg loss:3.524821\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.078060171999994\n",
            "Test performance: \n",
            " Avg WER:117.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Start training.\n",
            "loss:3.571022 [    0/ 2513]\n",
            "Done. Time:15.931662714999959\n",
            "Training performance: \n",
            " Avg loss:3.450517\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5163862419999532\n",
            "Validation performance: \n",
            " Avg loss:3.362342\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.290969598000004\n",
            "Test performance: \n",
            " Avg WER:103.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Start training.\n",
            "loss:3.304693 [    0/ 2513]\n",
            "Done. Time:15.971055594000006\n",
            "Training performance: \n",
            " Avg loss:3.337102\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5113909049999847\n",
            "Validation performance: \n",
            " Avg loss:3.281275\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.706274338000014\n",
            "Test performance: \n",
            " Avg WER:91.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Start training.\n",
            "loss:3.267980 [    0/ 2513]\n",
            "Done. Time:16.109019767000007\n",
            "Training performance: \n",
            " Avg loss:3.270049\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5331873190000351\n",
            "Validation performance: \n",
            " Avg loss:3.227556\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.57832790599997\n",
            "Test performance: \n",
            " Avg WER:90.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Start training.\n",
            "loss:3.321819 [    0/ 2513]\n",
            "Done. Time:16.12239260800004\n",
            "Training performance: \n",
            " Avg loss:3.222243\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5593813179999643\n",
            "Validation performance: \n",
            " Avg loss:3.182813\n",
            "\n",
            "Start test.\n",
            "Done. Time:17.69441878200007\n",
            "Test performance: \n",
            " Avg WER:89.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Start training.\n",
            "loss:3.164572 [    0/ 2513]\n",
            "Done. Time:15.983269870999948\n",
            "Training performance: \n",
            " Avg loss:3.169119\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4989903999999115\n",
            "Validation performance: \n",
            " Avg loss:3.161795\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.402787068000066\n",
            "Test performance: \n",
            " Avg WER:92.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Start training.\n",
            "loss:3.194525 [    0/ 2513]\n",
            "Done. Time:15.888244226999973\n",
            "Training performance: \n",
            " Avg loss:3.133062\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5069008379999786\n",
            "Validation performance: \n",
            " Avg loss:3.110244\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.397021706000032\n",
            "Test performance: \n",
            " Avg WER:91.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Start training.\n",
            "loss:3.119093 [    0/ 2513]\n",
            "Done. Time:16.130129323000006\n",
            "Training performance: \n",
            " Avg loss:3.101551\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.605752253999981\n",
            "Validation performance: \n",
            " Avg loss:3.081119\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.763282367999977\n",
            "Test performance: \n",
            " Avg WER:88.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Start training.\n",
            "loss:3.184865 [    0/ 2513]\n",
            "Done. Time:15.953397179000035\n",
            "Training performance: \n",
            " Avg loss:3.072371\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5313716670000304\n",
            "Validation performance: \n",
            " Avg loss:3.052065\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.688787990999913\n",
            "Test performance: \n",
            " Avg WER:89.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Start training.\n",
            "loss:3.064316 [    0/ 2513]\n",
            "Done. Time:15.894853871999999\n",
            "Training performance: \n",
            " Avg loss:3.044526\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5008622249999917\n",
            "Validation performance: \n",
            " Avg loss:3.034987\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.043676941000058\n",
            "Test performance: \n",
            " Avg WER:89.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Start training.\n",
            "loss:3.058185 [    0/ 2513]\n",
            "Done. Time:16.679944836999994\n",
            "Training performance: \n",
            " Avg loss:3.019295\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.6384037239999998\n",
            "Validation performance: \n",
            " Avg loss:3.013810\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.35032167999998\n",
            "Test performance: \n",
            " Avg WER:86.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Start training.\n",
            "loss:3.079668 [    0/ 2513]\n",
            "Done. Time:15.800374501999954\n",
            "Training performance: \n",
            " Avg loss:2.996543\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.500473859000067\n",
            "Validation performance: \n",
            " Avg loss:2.990506\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.34246096000004\n",
            "Test performance: \n",
            " Avg WER:91.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Start training.\n",
            "loss:2.956944 [    0/ 2513]\n",
            "Done. Time:15.538732466000056\n",
            "Training performance: \n",
            " Avg loss:2.975279\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5205276950000552\n",
            "Validation performance: \n",
            " Avg loss:2.978504\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.102862630999994\n",
            "Test performance: \n",
            " Avg WER:90.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Start training.\n",
            "loss:2.979929 [    0/ 2513]\n",
            "Done. Time:16.581596140999977\n",
            "Training performance: \n",
            " Avg loss:2.956005\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4739614439999968\n",
            "Validation performance: \n",
            " Avg loss:2.953194\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.81566670699999\n",
            "Test performance: \n",
            " Avg WER:88.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Start training.\n",
            "loss:2.845366 [    0/ 2513]\n",
            "Done. Time:15.914765479999915\n",
            "Training performance: \n",
            " Avg loss:2.934311\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.636238818000038\n",
            "Validation performance: \n",
            " Avg loss:2.937849\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.441943691000006\n",
            "Test performance: \n",
            " Avg WER:93.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Start training.\n",
            "loss:2.981558 [    0/ 2513]\n",
            "Done. Time:15.670402791000015\n",
            "Training performance: \n",
            " Avg loss:2.914950\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5098350900000241\n",
            "Validation performance: \n",
            " Avg loss:2.918005\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.551234036999972\n",
            "Test performance: \n",
            " Avg WER:89.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Start training.\n",
            "loss:2.988381 [    0/ 2513]\n",
            "Done. Time:15.614686149999898\n",
            "Training performance: \n",
            " Avg loss:2.893287\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.477484718000028\n",
            "Validation performance: \n",
            " Avg loss:2.902298\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.194385967000017\n",
            "Test performance: \n",
            " Avg WER:90.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Start training.\n",
            "loss:2.853385 [    0/ 2513]\n",
            "Done. Time:16.29237501099999\n",
            "Training performance: \n",
            " Avg loss:2.872563\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.317688214999862\n",
            "Validation performance: \n",
            " Avg loss:2.889019\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.662520522000023\n",
            "Test performance: \n",
            " Avg WER:93.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Start training.\n",
            "loss:2.838818 [    0/ 2513]\n",
            "Done. Time:16.068422889999965\n",
            "Training performance: \n",
            " Avg loss:2.854642\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5008070970000063\n",
            "Validation performance: \n",
            " Avg loss:2.867759\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.88551016500014\n",
            "Test performance: \n",
            " Avg WER:89.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Start training.\n",
            "loss:2.744241 [    0/ 2513]\n",
            "Done. Time:15.559427337999978\n",
            "Training performance: \n",
            " Avg loss:2.839487\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4910145950000242\n",
            "Validation performance: \n",
            " Avg loss:2.867312\n",
            "\n",
            "Start test.\n",
            "Done. Time:25.670253116999902\n",
            "Test performance: \n",
            " Avg WER:101.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Start training.\n",
            "loss:2.968571 [    0/ 2513]\n",
            "Done. Time:15.899244183000064\n",
            "Training performance: \n",
            " Avg loss:2.822213\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.2684222010000212\n",
            "Validation performance: \n",
            " Avg loss:2.843524\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.838979278999886\n",
            "Test performance: \n",
            " Avg WER:89.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Start training.\n",
            "loss:2.799435 [    0/ 2513]\n",
            "Done. Time:15.766484319000028\n",
            "Training performance: \n",
            " Avg loss:2.807996\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4865809619998345\n",
            "Validation performance: \n",
            " Avg loss:2.832041\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.247056035000014\n",
            "Test performance: \n",
            " Avg WER:92.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Start training.\n",
            "loss:2.759262 [    0/ 2513]\n",
            "Done. Time:16.40637444899994\n",
            "Training performance: \n",
            " Avg loss:2.795480\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.4377301570000327\n",
            "Validation performance: \n",
            " Avg loss:2.817645\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.14912487599986\n",
            "Test performance: \n",
            " Avg WER:94.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Start training.\n",
            "loss:2.738209 [    0/ 2513]\n",
            "Done. Time:15.914683330000116\n",
            "Training performance: \n",
            " Avg loss:2.781465\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.8641808690001653\n",
            "Validation performance: \n",
            " Avg loss:2.818959\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.424640528999817\n",
            "Test performance: \n",
            " Avg WER:92.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Start training.\n",
            "loss:2.812542 [    0/ 2513]\n",
            "Done. Time:15.921707298000001\n",
            "Training performance: \n",
            " Avg loss:2.767949\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5412154360001296\n",
            "Validation performance: \n",
            " Avg loss:2.799605\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.310321565000095\n",
            "Test performance: \n",
            " Avg WER:90.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Start training.\n",
            "loss:2.751738 [    0/ 2513]\n",
            "Done. Time:15.730928181000081\n",
            "Training performance: \n",
            " Avg loss:2.758824\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4994396849999703\n",
            "Validation performance: \n",
            " Avg loss:2.794147\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.537313942999845\n",
            "Test performance: \n",
            " Avg WER:90.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Start training.\n",
            "loss:2.734659 [    0/ 2513]\n",
            "Done. Time:16.13827937300016\n",
            "Training performance: \n",
            " Avg loss:2.746266\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4942897060000178\n",
            "Validation performance: \n",
            " Avg loss:2.784985\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.102225684999894\n",
            "Test performance: \n",
            " Avg WER:93.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Start training.\n",
            "loss:2.715821 [    0/ 2513]\n",
            "Done. Time:19.47561473099995\n",
            "Training performance: \n",
            " Avg loss:2.736280\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5158679559999655\n",
            "Validation performance: \n",
            " Avg loss:2.776502\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.667403395000065\n",
            "Test performance: \n",
            " Avg WER:90.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Start training.\n",
            "loss:2.730309 [    0/ 2513]\n",
            "Done. Time:15.840325908000068\n",
            "Training performance: \n",
            " Avg loss:2.725726\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.0495187679998708\n",
            "Validation performance: \n",
            " Avg loss:2.770320\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.44486225500009\n",
            "Test performance: \n",
            " Avg WER:95.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Start training.\n",
            "loss:2.711351 [    0/ 2513]\n",
            "Done. Time:16.116427956999814\n",
            "Training performance: \n",
            " Avg loss:2.716635\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.535494584000162\n",
            "Validation performance: \n",
            " Avg loss:2.762411\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.986224723000078\n",
            "Test performance: \n",
            " Avg WER:91.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Start training.\n",
            "loss:2.713863 [    0/ 2513]\n",
            "Done. Time:15.927048554000066\n",
            "Training performance: \n",
            " Avg loss:2.707899\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4893119199998637\n",
            "Validation performance: \n",
            " Avg loss:2.757214\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.029345643999932\n",
            "Test performance: \n",
            " Avg WER:92.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Start training.\n",
            "loss:2.825617 [    0/ 2513]\n",
            "Done. Time:16.04425179399982\n",
            "Training performance: \n",
            " Avg loss:2.696791\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.545380860000023\n",
            "Validation performance: \n",
            " Avg loss:2.745732\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.715735102000053\n",
            "Test performance: \n",
            " Avg WER:88.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Start training.\n",
            "loss:2.591888 [    0/ 2513]\n",
            "Done. Time:15.802545338000073\n",
            "Training performance: \n",
            " Avg loss:2.690198\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1907685389999187\n",
            "Validation performance: \n",
            " Avg loss:2.741932\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.29148311100016\n",
            "Test performance: \n",
            " Avg WER:89.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Start training.\n",
            "loss:2.677426 [    0/ 2513]\n",
            "Done. Time:15.915266943999995\n",
            "Training performance: \n",
            " Avg loss:2.680288\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5473290879999695\n",
            "Validation performance: \n",
            " Avg loss:2.738402\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.95250478700018\n",
            "Test performance: \n",
            " Avg WER:89.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Start training.\n",
            "loss:2.720850 [    0/ 2513]\n",
            "Done. Time:16.998532374999968\n",
            "Training performance: \n",
            " Avg loss:2.673147\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5282411129999218\n",
            "Validation performance: \n",
            " Avg loss:2.735458\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.157805500999984\n",
            "Test performance: \n",
            " Avg WER:88.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Start training.\n",
            "loss:2.730312 [    0/ 2513]\n",
            "Done. Time:16.46749445399996\n",
            "Training performance: \n",
            " Avg loss:2.665424\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.577404334999983\n",
            "Validation performance: \n",
            " Avg loss:2.736059\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.40470560299991\n",
            "Test performance: \n",
            " Avg WER:92.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Start training.\n",
            "loss:2.599684 [    0/ 2513]\n",
            "Done. Time:16.173238976999983\n",
            "Training performance: \n",
            " Avg loss:2.654634\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5281933690000642\n",
            "Validation performance: \n",
            " Avg loss:2.719844\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.50869515699992\n",
            "Test performance: \n",
            " Avg WER:87.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Start training.\n",
            "loss:2.562681 [    0/ 2513]\n",
            "Done. Time:15.950013062999915\n",
            "Training performance: \n",
            " Avg loss:2.646327\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.2121392340000057\n",
            "Validation performance: \n",
            " Avg loss:2.718908\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.58750206800005\n",
            "Test performance: \n",
            " Avg WER:89.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Start training.\n",
            "loss:2.697200 [    0/ 2513]\n",
            "Done. Time:15.68361678999986\n",
            "Training performance: \n",
            " Avg loss:2.639710\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5017427450000014\n",
            "Validation performance: \n",
            " Avg loss:2.709239\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.95386522900003\n",
            "Test performance: \n",
            " Avg WER:90.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Start training.\n",
            "loss:2.611823 [    0/ 2513]\n",
            "Done. Time:15.647641097999895\n",
            "Training performance: \n",
            " Avg loss:2.630178\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4885720130000664\n",
            "Validation performance: \n",
            " Avg loss:2.712899\n",
            "\n",
            "Start test.\n",
            "Done. Time:29.6573157979999\n",
            "Test performance: \n",
            " Avg WER:94.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Start training.\n",
            "loss:2.635214 [    0/ 2513]\n",
            "Done. Time:16.63428245399996\n",
            "Training performance: \n",
            " Avg loss:2.623138\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.488571003000061\n",
            "Validation performance: \n",
            " Avg loss:2.706749\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.036640152000018\n",
            "Test performance: \n",
            " Avg WER:87.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Start training.\n",
            "loss:2.592544 [    0/ 2513]\n",
            "Done. Time:15.89237131699997\n",
            "Training performance: \n",
            " Avg loss:2.615162\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.7916251770000144\n",
            "Validation performance: \n",
            " Avg loss:2.699172\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.812069340000107\n",
            "Test performance: \n",
            " Avg WER:89.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Start training.\n",
            "loss:2.611470 [    0/ 2513]\n",
            "Done. Time:16.07818079599997\n",
            "Training performance: \n",
            " Avg loss:2.608321\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5176749949998793\n",
            "Validation performance: \n",
            " Avg loss:2.700877\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.343437231999815\n",
            "Test performance: \n",
            " Avg WER:89.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Start training.\n",
            "loss:2.570234 [    0/ 2513]\n",
            "Done. Time:15.749781344999974\n",
            "Training performance: \n",
            " Avg loss:2.599580\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.984965308000028\n",
            "Validation performance: \n",
            " Avg loss:2.687226\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.90340706400002\n",
            "Test performance: \n",
            " Avg WER:88.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Start training.\n",
            "loss:2.643322 [    0/ 2513]\n",
            "Done. Time:15.676227490000201\n",
            "Training performance: \n",
            " Avg loss:2.591027\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.7270006040002954\n",
            "Validation performance: \n",
            " Avg loss:2.682581\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.771570268000232\n",
            "Test performance: \n",
            " Avg WER:88.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Start training.\n",
            "loss:2.678047 [    0/ 2513]\n",
            "Done. Time:15.842634292999719\n",
            "Training performance: \n",
            " Avg loss:2.582261\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.483254214000226\n",
            "Validation performance: \n",
            " Avg loss:2.687010\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.504201674999877\n",
            "Test performance: \n",
            " Avg WER:87.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Start training.\n",
            "loss:2.569041 [    0/ 2513]\n",
            "Done. Time:16.164611160000277\n",
            "Training performance: \n",
            " Avg loss:2.576327\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.3153026820000377\n",
            "Validation performance: \n",
            " Avg loss:2.679904\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.677590277000036\n",
            "Test performance: \n",
            " Avg WER:90.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Start training.\n",
            "loss:2.537554 [    0/ 2513]\n",
            "Done. Time:15.674790514000051\n",
            "Training performance: \n",
            " Avg loss:2.567390\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.669974916999763\n",
            "Validation performance: \n",
            " Avg loss:2.669603\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.861729981999815\n",
            "Test performance: \n",
            " Avg WER:87.8%\n",
            "\n",
            "Minimum validation loss:2.6696032136678696 at 50 epoch.\n",
            "Minimum WER:86.03277365443375 at 13 epoch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot result"
      ],
      "metadata": {
        "id": "bI_5zh65kfss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.grid(axis=\"y\", linestyle=\"dotted\", color=\"k\")\n",
        "\n",
        "xs = np.arange(1, len(val_losses_rnn)+1)\n",
        "plt.plot(xs, val_losses_rnn, label=\"RNN\", marker=\".\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.ylim([0.0, 5.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "kQ1seXMDkiHj",
        "outputId": "dd5a67ac-80a5-452c-b789-859114d3de75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid(axis=\"y\", linestyle=\"dotted\", color=\"k\")\n",
        "\n",
        "xs = np.arange(1, len(test_wers_rnn)+1)\n",
        "plt.plot(xs, test_wers_rnn, label=\"RNN\", marker=\".\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"WER\")\n",
        "plt.ylim([0.0, 100.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "4Uq2CQVDk9ol",
        "outputId": "47d80644-b986-4e90-b9a0-1732f9e00770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}