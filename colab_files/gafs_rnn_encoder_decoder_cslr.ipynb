{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnWmprT43QIBmPGjs3JUYI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takayama-rado/trado_samples/blob/main/colab_files/gafs_rnn_encoder_decoder_cslr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download dataset and modules"
      ],
      "metadata": {
        "id": "eQgl6G6nE7D0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADnfnnDUE0t1",
        "outputId": "b676fcae-dafc-4ed4-ce9a-75d5fd3047af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to local.\n",
        "!cp ./drive/MyDrive/Datasets/gafs_dataset_very_small.zip gafs_dataset.zip"
      ],
      "metadata": {
        "id": "1qMzlk1ilsWk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o gafs_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnzLCqD8FR7T",
        "outputId": "6ac22309-7029-42bf-b68c-ffb51a202c73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gafs_dataset.zip\n",
            "   creating: gafs_dataset_very_small/\n",
            "  inflating: gafs_dataset_very_small/0.hdf5  \n",
            "  inflating: gafs_dataset_very_small/1.hdf5  \n",
            "  inflating: gafs_dataset_very_small/10.hdf5  \n",
            "  inflating: gafs_dataset_very_small/102.hdf5  \n",
            "  inflating: gafs_dataset_very_small/105.hdf5  \n",
            "  inflating: gafs_dataset_very_small/107.hdf5  \n",
            "  inflating: gafs_dataset_very_small/109.hdf5  \n",
            "  inflating: gafs_dataset_very_small/112.hdf5  \n",
            "  inflating: gafs_dataset_very_small/113.hdf5  \n",
            "  inflating: gafs_dataset_very_small/115.hdf5  \n",
            "  inflating: gafs_dataset_very_small/117.hdf5  \n",
            "  inflating: gafs_dataset_very_small/121.hdf5  \n",
            "  inflating: gafs_dataset_very_small/122.hdf5  \n",
            "  inflating: gafs_dataset_very_small/125.hdf5  \n",
            "  inflating: gafs_dataset_very_small/128.hdf5  \n",
            "  inflating: gafs_dataset_very_small/13.hdf5  \n",
            "  inflating: gafs_dataset_very_small/135.hdf5  \n",
            "  inflating: gafs_dataset_very_small/136.hdf5  \n",
            "  inflating: gafs_dataset_very_small/138.hdf5  \n",
            "  inflating: gafs_dataset_very_small/141.hdf5  \n",
            "  inflating: gafs_dataset_very_small/143.hdf5  \n",
            "  inflating: gafs_dataset_very_small/145.hdf5  \n",
            "  inflating: gafs_dataset_very_small/147.hdf5  \n",
            "  inflating: gafs_dataset_very_small/15.hdf5  \n",
            "  inflating: gafs_dataset_very_small/151.hdf5  \n",
            "  inflating: gafs_dataset_very_small/153.hdf5  \n",
            "  inflating: gafs_dataset_very_small/154.hdf5  \n",
            "  inflating: gafs_dataset_very_small/157.hdf5  \n",
            "  inflating: gafs_dataset_very_small/158.hdf5  \n",
            "  inflating: gafs_dataset_very_small/160.hdf5  \n",
            "  inflating: gafs_dataset_very_small/161.hdf5  \n",
            "  inflating: gafs_dataset_very_small/168.hdf5  \n",
            "  inflating: gafs_dataset_very_small/169.hdf5  \n",
            "  inflating: gafs_dataset_very_small/171.hdf5  \n",
            "  inflating: gafs_dataset_very_small/176.hdf5  \n",
            "  inflating: gafs_dataset_very_small/178.hdf5  \n",
            "  inflating: gafs_dataset_very_small/18.hdf5  \n",
            "  inflating: gafs_dataset_very_small/181.hdf5  \n",
            "  inflating: gafs_dataset_very_small/186.hdf5  \n",
            "  inflating: gafs_dataset_very_small/187.hdf5  \n",
            "  inflating: gafs_dataset_very_small/188.hdf5  \n",
            "  inflating: gafs_dataset_very_small/192.hdf5  \n",
            "  inflating: gafs_dataset_very_small/196.hdf5  \n",
            "  inflating: gafs_dataset_very_small/2.hdf5  \n",
            "  inflating: gafs_dataset_very_small/20.hdf5  \n",
            "  inflating: gafs_dataset_very_small/202.hdf5  \n",
            "  inflating: gafs_dataset_very_small/203.hdf5  \n",
            "  inflating: gafs_dataset_very_small/21.hdf5  \n",
            "  inflating: gafs_dataset_very_small/216.hdf5  \n",
            "  inflating: gafs_dataset_very_small/217.hdf5  \n",
            "  inflating: gafs_dataset_very_small/219.hdf5  \n",
            "  inflating: gafs_dataset_very_small/223.hdf5  \n",
            "  inflating: gafs_dataset_very_small/225.hdf5  \n",
            "  inflating: gafs_dataset_very_small/227.hdf5  \n",
            "  inflating: gafs_dataset_very_small/230.hdf5  \n",
            "  inflating: gafs_dataset_very_small/231.hdf5  \n",
            "  inflating: gafs_dataset_very_small/233.hdf5  \n",
            "  inflating: gafs_dataset_very_small/236.hdf5  \n",
            "  inflating: gafs_dataset_very_small/239.hdf5  \n",
            "  inflating: gafs_dataset_very_small/24.hdf5  \n",
            "  inflating: gafs_dataset_very_small/241.hdf5  \n",
            "  inflating: gafs_dataset_very_small/242.hdf5  \n",
            "  inflating: gafs_dataset_very_small/246.hdf5  \n",
            "  inflating: gafs_dataset_very_small/25.hdf5  \n",
            "  inflating: gafs_dataset_very_small/251.hdf5  \n",
            "  inflating: gafs_dataset_very_small/254.hdf5  \n",
            "  inflating: gafs_dataset_very_small/27.hdf5  \n",
            "  inflating: gafs_dataset_very_small/36.hdf5  \n",
            "  inflating: gafs_dataset_very_small/38.hdf5  \n",
            "  inflating: gafs_dataset_very_small/4.hdf5  \n",
            "  inflating: gafs_dataset_very_small/40.hdf5  \n",
            "  inflating: gafs_dataset_very_small/43.hdf5  \n",
            "  inflating: gafs_dataset_very_small/53.hdf5  \n",
            "  inflating: gafs_dataset_very_small/56.hdf5  \n",
            "  inflating: gafs_dataset_very_small/59.hdf5  \n",
            "  inflating: gafs_dataset_very_small/6.hdf5  \n",
            "  inflating: gafs_dataset_very_small/63.hdf5  \n",
            "  inflating: gafs_dataset_very_small/68.hdf5  \n",
            "  inflating: gafs_dataset_very_small/70.hdf5  \n",
            "  inflating: gafs_dataset_very_small/71.hdf5  \n",
            "  inflating: gafs_dataset_very_small/72.hdf5  \n",
            "  inflating: gafs_dataset_very_small/73.hdf5  \n",
            "  inflating: gafs_dataset_very_small/74.hdf5  \n",
            "  inflating: gafs_dataset_very_small/76.hdf5  \n",
            "  inflating: gafs_dataset_very_small/80.hdf5  \n",
            "  inflating: gafs_dataset_very_small/81.hdf5  \n",
            "  inflating: gafs_dataset_very_small/88.hdf5  \n",
            "  inflating: gafs_dataset_very_small/89.hdf5  \n",
            "  inflating: gafs_dataset_very_small/9.hdf5  \n",
            "  inflating: gafs_dataset_very_small/92.hdf5  \n",
            "  inflating: gafs_dataset_very_small/93.hdf5  \n",
            "  inflating: gafs_dataset_very_small/95.hdf5  \n",
            "  inflating: gafs_dataset_very_small/character_to_prediction_index.json  \n",
            "  inflating: gafs_dataset_very_small/LICENSE.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gafs_dataset_very_small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ZWwV56FWVD",
        "outputId": "de6d4801-7fdb-40b7-f249-f7052c1e8d54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.hdf5\t  135.hdf5  160.hdf5  1.hdf5\t236.hdf5  43.hdf5  80.hdf5\n",
            "102.hdf5  136.hdf5  161.hdf5  202.hdf5\t239.hdf5  4.hdf5   81.hdf5\n",
            "105.hdf5  138.hdf5  168.hdf5  203.hdf5\t241.hdf5  53.hdf5  88.hdf5\n",
            "107.hdf5  13.hdf5   169.hdf5  20.hdf5\t242.hdf5  56.hdf5  89.hdf5\n",
            "109.hdf5  141.hdf5  171.hdf5  216.hdf5\t246.hdf5  59.hdf5  92.hdf5\n",
            "10.hdf5   143.hdf5  176.hdf5  217.hdf5\t24.hdf5   63.hdf5  93.hdf5\n",
            "112.hdf5  145.hdf5  178.hdf5  219.hdf5\t251.hdf5  68.hdf5  95.hdf5\n",
            "113.hdf5  147.hdf5  181.hdf5  21.hdf5\t254.hdf5  6.hdf5   9.hdf5\n",
            "115.hdf5  151.hdf5  186.hdf5  223.hdf5\t25.hdf5   70.hdf5  character_to_prediction_index.json\n",
            "117.hdf5  153.hdf5  187.hdf5  225.hdf5\t27.hdf5   71.hdf5  LICENSE.txt\n",
            "121.hdf5  154.hdf5  188.hdf5  227.hdf5\t2.hdf5\t  72.hdf5\n",
            "122.hdf5  157.hdf5  18.hdf5   230.hdf5\t36.hdf5   73.hdf5\n",
            "125.hdf5  158.hdf5  192.hdf5  231.hdf5\t38.hdf5   74.hdf5\n",
            "128.hdf5  15.hdf5   196.hdf5  233.hdf5\t40.hdf5   76.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gafs_dataset_very_small/character_to_prediction_index.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLP9RpORFaLL",
        "outputId": "f93c5e51-c732-4f83-f7a8-6d6600e6fc76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\" \":0,\"!\":1,\"#\":2,\"$\":3,\"%\":4,\"&\":5,\"'\":6,\"(\":7,\")\":8,\"*\":9,\"+\":10,\",\":11,\"-\":12,\".\":13,\"\\/\":14,\"0\":15,\"1\":16,\"2\":17,\"3\":18,\"4\":19,\"5\":20,\"6\":21,\"7\":22,\"8\":23,\"9\":24,\":\":25,\";\":26,\"=\":27,\"?\":28,\"@\":29,\"[\":30,\"_\":31,\"a\":32,\"b\":33,\"c\":34,\"d\":35,\"e\":36,\"f\":37,\"g\":38,\"h\":39,\"i\":40,\"j\":41,\"k\":42,\"l\":43,\"m\":44,\"n\":45,\"o\":46,\"p\":47,\"q\":48,\"r\":49,\"s\":50,\"t\":51,\"u\":52,\"v\":53,\"w\":54,\"x\":55,\"y\":56,\"z\":57,\"~\":58}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gafs_dataset_very_small/LICENSE.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iQlJCDtU4d8",
        "outputId": "1cda7388-b448-42e2-a53b-c074010332d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset provided by Natsuki Takayama (Takayama Research and Development Office) is licensed under CC-BY 4.0.\r\n",
            "Author: Copyright 2024 Natsuki Takayama\r\n",
            "Title: GASF very small dataset\r\n",
            "Original licenser: Google LLC\r\n",
            "Modification\r\n",
            "- Extract only 3 parquet file.\r\n",
            "- Packaged into HDF5 format.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "with h5py.File(\"gafs_dataset_very_small/0.hdf5\", \"r\") as fread:\n",
        "    keys = list(fread.keys())\n",
        "    print(keys[:10])\n",
        "    group = fread[keys[0]]\n",
        "    print(group.keys())\n",
        "    feature = group[\"feature\"][:]\n",
        "    token = group[\"token\"][:]\n",
        "    print(feature.shape)\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37SBiPQeVSB7",
        "outputId": "dd7e3025-3d5a-412d-b30c-39cc2c92ef48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1720198121', '1722303176', '1723157122', '1731934631', '1737624109', '1739256200', '1743069372', '1743412187', '1744795751', '1746320345']\n",
            "<KeysViewHDF5 ['feature', 'token']>\n",
            "(2, 271, 543)\n",
            "[14 38 32 45 44 36 40 32 43 43 36 56 14 43 40 45 32 12 34 32 49 50 51 36\n",
            " 45 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/takayama-rado/trado_samples/archive/refs/tags/v0.3.4.zip -O master.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjcUxrq2VmlE",
        "outputId": "cb52e10c-899c-4b9c-f44f-1ed1b336d5db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-14 13:48:54--  https://github.com/takayama-rado/trado_samples/archive/refs/tags/v0.3.4.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/takayama-rado/trado_samples/zip/refs/tags/v0.3.4 [following]\n",
            "--2024-09-14 13:48:54--  https://codeload.github.com/takayama-rado/trado_samples/zip/refs/tags/v0.3.4\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.121.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.121.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [                 <=>]  76.54M  17.4MB/s    in 4.4s    \n",
            "\n",
            "2024-09-14 13:48:59 (17.4 MB/s) - ‘master.zip’ saved [80254068]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o master.zip -d master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUTwKBOMVw-j",
        "outputId": "2856af0c-63d8-4c96-8b1a-f5306e9274f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  master.zip\n",
            "3406d5a0072e08879272e622ff8efdc1c7b78ee8\n",
            "   creating: master/trado_samples-0.3.4/\n",
            "  inflating: master/trado_samples-0.3.4/.gitignore  \n",
            "  inflating: master/trado_samples-0.3.4/LICENSE  \n",
            "  inflating: master/trado_samples-0.3.4/README.md  \n",
            "   creating: master/trado_samples-0.3.4/colab_files/\n",
            " extracting: master/trado_samples-0.3.4/colab_files/.gitkeep  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_affine_np_einsum.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_jax_static.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_mpholistic_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_mpothers_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_jax.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_numpy.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_affine_torch.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_jax.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_numpy_0.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_numpy_1.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_tensorflow.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/exp_track_interp_torch.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gafs_create_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_access_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_conformer_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_create_dataset.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_macaronnet_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_normalize_landmarks.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_1.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_2.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_rnn_islr_model_3.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_select_landmarks.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_simple_attention.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_simple_islr_model.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_drop.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_hflip.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_resize.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_saffine.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_snoise.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_tclip.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_tinterp.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_daug_twarping.ipynb  \n",
            "  inflating: master/trado_samples-0.3.4/colab_files/gislr_transformer_encoder_w_ls.ipynb  \n",
            "   creating: master/trado_samples-0.3.4/src/\n",
            "   creating: master/trado_samples-0.3.4/src/modules_gislr/\n",
            " extracting: master/trado_samples-0.3.4/src/modules_gislr/__init__.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/activation.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/dataset.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/defines.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/draw_functions.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/layers.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/train_functions.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/transforms.py  \n",
            "  inflating: master/trado_samples-0.3.4/src/modules_gislr/utils.py  \n",
            "   creating: master/trado_samples-0.3.4/test_data/\n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static_affine.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_far0_non_static_interp.npy  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_middle0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/finger_near0.mp4  \n",
            "  inflating: master/trado_samples-0.3.4/test_data/hand_only.mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv master/trado_samples-0.3.4/src/modules_gislr ."
      ],
      "metadata": {
        "id": "yXsIhVAWVyej"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf master master.zip gafs_dataset_very_small.zip"
      ],
      "metadata": {
        "id": "ohykNs7zV3TL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koeHiooRV7qr",
        "outputId": "b3c1f279-527e-4baa-ab8f-e3791799d82a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  gafs_dataset_very_small\tgafs_dataset.zip  modules_gislr  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load library"
      ],
      "metadata": {
        "id": "ddZ2NhLDV8yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "# Third party's modules\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import (\n",
        "    DataLoader)\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "# Local modules\n",
        "sys.path.append(\"modules_gislr\")\n",
        "from modules_gislr.dataset import (\n",
        "    HDF5Dataset,\n",
        "    merge)\n",
        "from modules_gislr.defines import (\n",
        "    get_fullbody_landmarks\n",
        ")\n",
        "from modules_gislr.layers import (\n",
        "    RNNEncoder,\n",
        "    apply_norm,\n",
        "    create_norm\n",
        ")\n",
        "from modules_gislr.train_functions import (\n",
        "    LabelSmoothingCrossEntropyLoss\n",
        ")\n",
        "from modules_gislr.transforms import (\n",
        "    PartsBasedNormalization,\n",
        "    ReplaceNan,\n",
        "    SelectLandmarksAndFeature,\n",
        "    ToTensor\n",
        ")\n",
        "from modules_gislr.utils import (\n",
        "    select_reluwise_activation\n",
        ")"
      ],
      "metadata": {
        "id": "8K-wtRChV-n7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implement RNN Encoder-Decoder CSLR model"
      ],
      "metadata": {
        "id": "Klup1x0iWlHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention module"
      ],
      "metadata": {
        "id": "aPHXLRYMXE9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttentionEnergy(nn.Module):\n",
        "    def __init__(self,\n",
        "                 key_dim,\n",
        "                 query_dim,\n",
        "                 att_dim,\n",
        "                 add_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_key = nn.Linear(key_dim, att_dim, bias=add_bias)\n",
        "        self.w_query = nn.Linear(query_dim, att_dim, bias=add_bias)\n",
        "        self.w_out = nn.Linear(att_dim, 1, bias=add_bias)\n",
        "\n",
        "    def forward(self, key, query):\n",
        "        # key: `[N, key_len, key_dim]`\n",
        "        # query: `[N, 1, query_dim]`\n",
        "        key = self.w_key(key)\n",
        "        query = self.w_query(query)\n",
        "        # Adding with broadcasting.\n",
        "        # key: `[N, key_len, key_dim]`\n",
        "        # query: `[N, 1, query_dim]`\n",
        "        # query should be broadcasted to `[N, key_len, query_dim]`\n",
        "        temp = key + query\n",
        "        # `[N, key_len, att_dim] -> [N, key_len, 1] -> [N, 1, key_len]`\n",
        "        energy = self.w_out(torch.tanh(temp))\n",
        "        energy = torch.permute(energy, [0, 2, 1])\n",
        "        return energy"
      ],
      "metadata": {
        "id": "9AClEx01WqmU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 key_dim,\n",
        "                 query_dim,\n",
        "                 att_dim,\n",
        "                 add_bias):\n",
        "        super().__init__()\n",
        "\n",
        "        self.att_energy = BahdanauAttentionEnergy(\n",
        "            key_dim=key_dim,\n",
        "            query_dim=query_dim,\n",
        "            att_dim=att_dim,\n",
        "            add_bias=add_bias)\n",
        "\n",
        "        self.neg_inf = None\n",
        "\n",
        "    def forward(self,\n",
        "                key,\n",
        "                value,\n",
        "                query,\n",
        "                mask=None):\n",
        "        if self.neg_inf is None:\n",
        "            self.neg_inf = float(np.finfo(\n",
        "                torch.tensor(0, dtype=key.dtype).numpy().dtype).min)\n",
        "\n",
        "        batch, klen, kdim = key.shape\n",
        "        _, qlen, qdim = query.shape\n",
        "        energy = self.att_energy(key=key, query=query)\n",
        "        assert energy.shape == (batch, qlen, klen)\n",
        "\n",
        "        # Apply mask.\n",
        "        if mask is not None:\n",
        "            if len(mask.shape) == 2:\n",
        "                # `[N, klen] -> [N, qlen(=1), klen]`\n",
        "                mask = mask.unsqueeze(1)\n",
        "            # Negative infinity should be 0 in softmax.\n",
        "            energy = energy.masked_fill_(mask==0, self.neg_inf)\n",
        "\n",
        "        # Compute attention mask.\n",
        "        attw = torch.softmax(energy, dim=-1)\n",
        "        # attw: `[N, qlen, klen]`\n",
        "        # value: `[N, klen, kdim]`\n",
        "        # bmm: `[N, qlen, klen] x [N, klen, kdim] -> [N, qlen, kdim]`\n",
        "        cvec = torch.bmm(attw, value)\n",
        "        return cvec, attw"
      ],
      "metadata": {
        "id": "VEy2_ljtXNEz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Decoder"
      ],
      "metadata": {
        "id": "IRNyq0OkX8Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauRNNDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 hidden_channels,\n",
        "                 out_channels,\n",
        "                 emb_channels,\n",
        "                 att_dim,\n",
        "                 att_add_bias,\n",
        "                 rnn_type,\n",
        "                 num_layers,\n",
        "                 activation,\n",
        "                 dropout,\n",
        "                 padding_val,\n",
        "                 proj_size=0):\n",
        "        super().__init__()\n",
        "        assert rnn_type in [\"srnn\", \"lstm\", \"gru\"]\n",
        "\n",
        "        self.emb_layer = nn.Embedding(\n",
        "            num_embeddings=out_channels,\n",
        "            embedding_dim=emb_channels,\n",
        "            padding_idx=padding_val)\n",
        "\n",
        "        self.att_layer = SingleHeadAttention(\n",
        "            key_dim=in_channels,\n",
        "            query_dim=hidden_channels,\n",
        "            att_dim=att_dim,\n",
        "            add_bias=att_add_bias)\n",
        "\n",
        "        if rnn_type == \"srnn\":\n",
        "            self.rnn = nn.RNN(input_size=in_channels + emb_channels,\n",
        "                              hidden_size=hidden_channels,\n",
        "                              num_layers=num_layers,\n",
        "                              nonlinearity=activation,\n",
        "                              batch_first=True,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=False)\n",
        "        elif rnn_type == \"lstm\":\n",
        "            self.rnn = nn.LSTM(input_size=in_channels + emb_channels,\n",
        "                               hidden_size=hidden_channels,\n",
        "                               num_layers=num_layers,\n",
        "                               batch_first=True,\n",
        "                               dropout=dropout,\n",
        "                               bidirectional=False,\n",
        "                               proj_size=proj_size)\n",
        "        elif rnn_type == \"gru\":\n",
        "            self.rnn = nn.GRU(input_size=in_channels + emb_channels,\n",
        "                              hidden_size=hidden_channels,\n",
        "                              num_layers=num_layers,\n",
        "                              batch_first=True,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=False)\n",
        "        self.head = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.dec_hstate = None\n",
        "        self.attw = None\n",
        "\n",
        "    def init_dec_hstate(self, enc_hstate, init_as_zero=False):\n",
        "        if init_as_zero:\n",
        "            dec_hstate = torch.zeros_like(enc_hstate)\n",
        "        else:\n",
        "            dec_hstate = enc_hstate\n",
        "        # To avoid error at RNN layer.\n",
        "        self.dec_hstate = dec_hstate.contiguous()\n",
        "\n",
        "    def forward(self,\n",
        "                dec_inputs,\n",
        "                enc_seqs,\n",
        "                enc_mask):\n",
        "        assert self.dec_hstate is not None, f\"dec_hstate has not been initialized.\"\n",
        "        dec_hstate = self.dec_hstate\n",
        "\n",
        "        # Attention layer requires hidden state of 2nd rnn layer.\n",
        "        # as `[N, 1, C]`\n",
        "        query = dec_hstate[-1].unsqueeze(1)\n",
        "        cvec, self.attw = self.att_layer(\n",
        "            key=enc_seqs,\n",
        "            value=enc_seqs,\n",
        "            query=query,\n",
        "            mask=enc_mask)\n",
        "\n",
        "        emb_out = self.emb_layer(dec_inputs)\n",
        "        # `[N, C] -> [N, 1, C]`\n",
        "        emb_out = emb_out.reshape([-1, 1, emb_out.shape[-1]])\n",
        "        feature = torch.cat([cvec, emb_out], dim=-1)\n",
        "        if isinstance(self.rnn, nn.LSTM):\n",
        "            hidden_seqs, (last_hstate, last_cstate) = self.rnn(feature,\n",
        "                                                               dec_hstate)\n",
        "        else:\n",
        "            hidden_seqs, last_hstate = self.rnn(feature,\n",
        "                                                dec_hstate)\n",
        "            last_cstate = None\n",
        "\n",
        "        output_dec = self.head(hidden_seqs)\n",
        "        self.dec_hstate = last_hstate\n",
        "        return output_dec"
      ],
      "metadata": {
        "id": "UZ4Me3ZRX-GL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN CSLR model"
      ],
      "metadata": {
        "id": "IvVTsyTsYPOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCSLR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_in_channels,\n",
        "                 enc_hidden_channels,\n",
        "                 enc_rnn_type,\n",
        "                 enc_num_layers,\n",
        "                 enc_activation,\n",
        "                 enc_bidir,\n",
        "                 enc_dropout,\n",
        "                 enc_apply_mask,\n",
        "                 enc_proj_size,\n",
        "                 dec_in_channels,\n",
        "                 dec_hidden_channels,\n",
        "                 dec_out_channels,\n",
        "                 dec_emb_channels,\n",
        "                 dec_att_dim,\n",
        "                 dec_att_add_bias,\n",
        "                 dec_rnn_type,\n",
        "                 dec_num_layers,\n",
        "                 dec_activation,\n",
        "                 dec_dropout,\n",
        "                 dec_padding_val,\n",
        "                 dec_proj_size):\n",
        "        super().__init__()\n",
        "        self.enc_bidir = enc_bidir\n",
        "\n",
        "        self.linear = nn.Linear(enc_in_channels, enc_hidden_channels)\n",
        "        self.enc_activation = nn.ReLU()\n",
        "\n",
        "        self.encoder = RNNEncoder(\n",
        "            in_channels=enc_hidden_channels,\n",
        "            out_channels=enc_hidden_channels,\n",
        "            rnn_type=enc_rnn_type,\n",
        "            num_layers=enc_num_layers,\n",
        "            activation=enc_activation,\n",
        "            bidir=enc_bidir,\n",
        "            dropout=enc_dropout,\n",
        "            apply_mask=enc_apply_mask,\n",
        "            proj_size=enc_proj_size)\n",
        "\n",
        "        if enc_bidir:\n",
        "            dec_in_channels *= 2\n",
        "            dec_hidden_channels *= 2\n",
        "            dec_att_dim *= 2\n",
        "\n",
        "        self.decoder = BahdanauRNNDecoder(\n",
        "            in_channels=dec_in_channels,\n",
        "            hidden_channels=dec_hidden_channels,\n",
        "            out_channels=dec_out_channels,\n",
        "            emb_channels=dec_emb_channels,\n",
        "            att_dim=dec_att_dim,\n",
        "            att_add_bias=dec_att_add_bias,\n",
        "            rnn_type=dec_rnn_type,\n",
        "            num_layers=dec_num_layers,\n",
        "            activation=dec_activation,\n",
        "            dropout=dec_dropout,\n",
        "            padding_val=dec_padding_val,\n",
        "            proj_size=dec_proj_size)\n",
        "\n",
        "    def _apply_encoder(self, feature, feature_pad_mask=None):\n",
        "        # Feature extraction.\n",
        "        # `[N, C, T, J] -> [N, T, C, J] -> [N, T, C*J] -> [N, T, C']`\n",
        "        N, C, T, J = feature.shape\n",
        "        feature = feature.permute([0, 2, 1, 3])\n",
        "        feature = feature.reshape(N, T, -1)\n",
        "\n",
        "        feature = self.linear(feature)\n",
        "        feature = self.enc_activation(feature)\n",
        "\n",
        "        # Apply encoder.\n",
        "        enc_seqs, enc_hstate = self.encoder(feature, feature_pad_mask)[:2]\n",
        "\n",
        "        # Basically, decoder should not be bidirectional.\n",
        "        # So, we should concatenate backwarded feature.\n",
        "        if self.enc_bidir:\n",
        "            # `[2*layers, N, C] -> [layers, N, 2*C]`\n",
        "            enc_hstate = torch.permute(enc_hstate, [1, 0, 2])\n",
        "            enc_hstate = enc_hstate.reshape([enc_hstate.shape[0],\n",
        "                                             enc_hstate.shape[1] // 2,\n",
        "                                             -1])\n",
        "            enc_hstate = torch.permute(enc_hstate, [1, 0, 2])\n",
        "        return enc_seqs, enc_hstate\n",
        "\n",
        "    def forward(self,\n",
        "                feature, tokens,\n",
        "                feature_pad_mask=None, tokens_pad_mask=None):\n",
        "        \"\"\"Forward computation for train.\n",
        "        \"\"\"\n",
        "        enc_seqs, enc_hstate = self._apply_encoder(feature, feature_pad_mask)\n",
        "\n",
        "        # Apply decoder.\n",
        "        self.decoder.init_dec_hstate(enc_hstate)\n",
        "        preds = None\n",
        "        for t_index in range(0, tokens.shape[-1]):\n",
        "            # Teacher forcing.\n",
        "            dec_inputs = tokens[:, t_index].reshape([-1, 1])\n",
        "            pred = self.decoder(\n",
        "                dec_inputs=dec_inputs,\n",
        "                enc_seqs=enc_seqs,\n",
        "                enc_mask=feature_pad_mask)\n",
        "            if preds is None:\n",
        "                preds = pred\n",
        "            else:\n",
        "                # `[N, T, C]`\n",
        "                preds = torch.cat([preds, pred], dim=1)\n",
        "        return preds\n",
        "\n",
        "    def inference(self,\n",
        "                  feature,\n",
        "                  start_id,\n",
        "                  end_id,\n",
        "                  feature_pad_mask=None,\n",
        "                  max_seqlen=62):\n",
        "        \"\"\"Forward computation for test.\n",
        "        \"\"\"\n",
        "        enc_seqs, enc_hstate = self._apply_encoder(feature, feature_pad_mask)\n",
        "\n",
        "        # Apply decoder.\n",
        "        self.decoder.init_dec_hstate(enc_hstate)\n",
        "        dec_inputs = torch.tensor([start_id]).to(feature.device)\n",
        "        # `[N, T]`\n",
        "        dec_inputs = dec_inputs.reshape([1, 1])\n",
        "        preds = None\n",
        "        pred_ids = [start_id]\n",
        "        for _ in range(max_seqlen):\n",
        "            pred = self.decoder(\n",
        "                dec_inputs=dec_inputs,\n",
        "                enc_seqs=enc_seqs,\n",
        "                enc_mask=feature_pad_mask)\n",
        "            if preds is None:\n",
        "                preds = pred\n",
        "            else:\n",
        "                # `[N, T, C]`\n",
        "                preds = torch.cat([preds, pred], dim=1)\n",
        "\n",
        "            pid = torch.argmax(pred, dim=-1)\n",
        "            dec_inputs = pid\n",
        "\n",
        "            pid = pid.reshape([1]).detach().cpu().numpy()[0]\n",
        "            pred_ids.append(int(pid))\n",
        "            if int(pid) == end_id:\n",
        "                break\n",
        "\n",
        "        # `[N, T]`\n",
        "        pred_ids = np.array([pred_ids])\n",
        "        return pred_ids, preds"
      ],
      "metadata": {
        "id": "QHuAwB3VYRfE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Update to train CSLR model"
      ],
      "metadata": {
        "id": "6vRfzdWzaRFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add transformation to insert keywords"
      ],
      "metadata": {
        "id": "ho8wlf3UcNCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InsertTokensForS2S():\n",
        "    def __init__(self,\n",
        "                 sos_token,\n",
        "                 eos_token,\n",
        "                 error_at_exist=False):\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.error_at_exist = error_at_exist\n",
        "\n",
        "    def check_format(self, tokens):\n",
        "        insert_sos = False\n",
        "        if tokens[0] != self.sos_token:\n",
        "            insert_sos = True\n",
        "        elif self.error_at_exist:\n",
        "            message = f\"The sos_token:{self.sos_token} is exist in {tokens}.\" \\\n",
        "                + \"Please check the format.\"\n",
        "            raise ValueError(message)\n",
        "        insert_eos = False\n",
        "        if tokens[-1] != self.eos_token:\n",
        "            insert_eos = True\n",
        "        elif self.error_at_exist:\n",
        "            message = f\"The eos_token:{self.eos_token} is exist in {tokens}.\" \\\n",
        "                + \"Please check the format.\"\n",
        "            raise ValueError(message)\n",
        "        return insert_sos, insert_eos\n",
        "\n",
        "    def __call__(self, data):\n",
        "\n",
        "        tokens = data[\"token\"]\n",
        "        dtype = tokens.dtype\n",
        "\n",
        "        insert_sos, insert_eos = self.check_format(tokens)\n",
        "        # Insert.\n",
        "        new_tokens = []\n",
        "        if insert_sos:\n",
        "            new_tokens.append(self.sos_token)\n",
        "        new_tokens += tokens.tolist()\n",
        "        if insert_eos:\n",
        "            new_tokens.append(self.eos_token)\n",
        "        new_tokens = np.array(new_tokens, dtype=dtype)\n",
        "        data[\"token\"] = new_tokens\n",
        "        return data"
      ],
      "metadata": {
        "id": "5A9SURRYaW9d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update merge function to create padded token"
      ],
      "metadata": {
        "id": "pOioAkXmgg2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_padded_batch(batch,\n",
        "                       feature_shape,\n",
        "                       token_shape,\n",
        "                       feature_padding_val=0,\n",
        "                       token_padding_val=0):\n",
        "    feature_batch = [sample[\"feature\"] for sample in batch]\n",
        "    token_batch = [sample[\"token\"] for sample in batch]\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge feature.\n",
        "    # ==========================================================\n",
        "    # `[B, C, T, J]`\n",
        "    merged_shape = [len(batch), *feature_shape]\n",
        "    # Use maximum frame length in a batch as padded length.\n",
        "    if merged_shape[2] == -1:\n",
        "        tlen = max([feature.shape[1] for feature in feature_batch])\n",
        "        merged_shape[2] = tlen\n",
        "    merged_feature = merge(feature_batch, merged_shape, padding_val=feature_padding_val)\n",
        "\n",
        "    # ==========================================================\n",
        "    # Merge tocken.\n",
        "    # ==========================================================\n",
        "    # `[B, L]`\n",
        "    merged_shape = [len(batch), *token_shape]\n",
        "    # Use maximum token length in a batch as padded length.\n",
        "    if merged_shape[1] == -1:\n",
        "        tlen = max([token.shape[0] for token in token_batch])\n",
        "        merged_shape[1] = tlen\n",
        "    merged_token = merge(token_batch, merged_shape, padding_val=token_padding_val)\n",
        "\n",
        "    # Generate padding mask.\n",
        "    # Pad: 0, Signal: 1\n",
        "    # The frames which all channels and landmarks are equals to padding value\n",
        "    # should be padded.\n",
        "    feature_pad_mask = merged_feature == feature_padding_val\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=1)\n",
        "    feature_pad_mask = torch.all(feature_pad_mask, dim=-1)\n",
        "    feature_pad_mask = torch.logical_not(feature_pad_mask)\n",
        "    token_pad_mask = torch.logical_not(merged_token == token_padding_val)\n",
        "\n",
        "    retval = {\n",
        "        \"feature\": merged_feature,\n",
        "        \"token\": merged_token,\n",
        "        \"feature_pad_mask\": feature_pad_mask,\n",
        "        \"token_pad_mask\": token_pad_mask}\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Lw2jeDOTgtbc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update train, validation, and test loops"
      ],
      "metadata": {
        "id": "CmD69vQahEMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(model, feature, tokens, feature_pad_mask, tokens_pad_mask):\n",
        "    if isinstance(model, RNNCSLR):\n",
        "        preds = model(feature,\n",
        "                      tokens,\n",
        "                      feature_pad_mask=feature_pad_mask,\n",
        "                      tokens_pad_mask=tokens_pad_mask)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model type:{type(model)}.\")\n",
        "    return preds"
      ],
      "metadata": {
        "id": "r-CoDrCOhKy8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, feature, start_id, end_id, max_seqlen=62):\n",
        "    if isinstance(model, RNNCSLR):\n",
        "        pred_ids, _ = model.inference(feature,\n",
        "                                      start_id,\n",
        "                                      end_id,\n",
        "                                      max_seqlen=max_seqlen)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model type:{type(model)}.\")\n",
        "    return pred_ids"
      ],
      "metadata": {
        "id": "CryTqBaihbgE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_tokens_format(tokens, tokens_pad_mask, start_id, end_id):\n",
        "    # Check token's format.\n",
        "    end_indices0 = np.arange(len(tokens))\n",
        "    end_indices1 = tokens_pad_mask.sum(dim=-1).detach().cpu().numpy() - 1\n",
        "    message = \"The start and/or end ids are not included in tokens. \" \\\n",
        "        f\"Please check data format. start_id:{start_id}, \" \\\n",
        "        f\"end_id:{end_id}, enc_indices:{end_indices1}, tokens:{tokens}\"\n",
        "    ref_tokens = tokens.detach().cpu().numpy()\n",
        "    assert (ref_tokens[:, 0] == start_id).all(), message\n",
        "    assert (ref_tokens[end_indices0, end_indices1] == end_id).all(), message"
      ],
      "metadata": {
        "id": "Rchi8Hx2hZCs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_csir_s2s(dataloader,\n",
        "                        model,\n",
        "                        loss_fn,\n",
        "                        optimizer,\n",
        "                        device,\n",
        "                        start_id,\n",
        "                        end_id,\n",
        "                        return_pred_times=False):\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss = 0\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to training mode.\n",
        "    model.train()\n",
        "    # Main loop.\n",
        "    print(\"Start training.\")\n",
        "    start = time.perf_counter()\n",
        "    for batch_idx, batch_sample in enumerate(dataloader):\n",
        "        feature = batch_sample[\"feature\"]\n",
        "        feature_pad_mask = batch_sample[\"feature_pad_mask\"]\n",
        "        tokens = batch_sample[\"token\"]\n",
        "        tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "        check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "        feature = feature.to(device)\n",
        "        feature_pad_mask = feature_pad_mask.to(device)\n",
        "        tokens = tokens.to(device)\n",
        "        tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "        frames = feature.shape[-2]\n",
        "\n",
        "        # Predict.\n",
        "        pred_start = time.perf_counter()\n",
        "        preds = forward(model, feature, tokens,\n",
        "                        feature_pad_mask, tokens_pad_mask)\n",
        "        pred_end = time.perf_counter()\n",
        "        pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "        # Compute loss.\n",
        "        # Preds do not include <start>, so skip that of tokens.\n",
        "        loss = 0\n",
        "        if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "            for t_index in range(1, tokens.shape[-1]):\n",
        "                pred = preds[:, t_index-1, :]\n",
        "                token = tokens[:, t_index]\n",
        "                loss += loss_fn(pred, token)\n",
        "            loss /= tokens.shape[-1]\n",
        "        # LabelSmoothingCrossEntropyLoss\n",
        "        else:\n",
        "            # `[N, T, C] -> [N, C, T]`\n",
        "            preds = preds.permute([0, 2, 1])\n",
        "            # Remove prediction after the last token.\n",
        "            if preds.shape[-1] == tokens.shape[-1]:\n",
        "                preds = preds[:, :, :-1]\n",
        "            loss = loss_fn(preds, tokens[:, 1:])\n",
        "\n",
        "        # Back propagation.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Print current loss per 100 steps.\n",
        "        if batch_idx % 100 == 0:\n",
        "            loss = loss.item()\n",
        "            steps = batch_idx * len(feature)\n",
        "            print(f\"loss:{loss:>7f} [{steps:>5d}/{size:>5d}]\")\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "    # Average loss.\n",
        "    train_loss /= num_batches\n",
        "    print(\"Training performance: \\n\",\n",
        "          f\"Avg loss:{train_loss:>8f}\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (train_loss, pred_times) if return_pred_times else train_loss\n",
        "    return retval"
      ],
      "metadata": {
        "id": "Fd3z-wjohe1d"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_loop_csir_s2s(dataloader,\n",
        "                      model,\n",
        "                      loss_fn,\n",
        "                      device,\n",
        "                      start_id,\n",
        "                      end_id,\n",
        "                      return_pred_times=False):\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss = 0\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to evaluation mode.\n",
        "    model.eval()\n",
        "    # Main loop.\n",
        "    print(\"Start validation.\")\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_sample in enumerate(dataloader):\n",
        "            feature = batch_sample[\"feature\"]\n",
        "            feature_pad_mask = batch_sample[\"feature_pad_mask\"]\n",
        "            tokens = batch_sample[\"token\"]\n",
        "            tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "            check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "            feature = feature.to(device)\n",
        "            feature_pad_mask = feature_pad_mask.to(device)\n",
        "            tokens = tokens.to(device)\n",
        "            tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "            frames = feature.shape[-2]\n",
        "\n",
        "            # Predict.\n",
        "            pred_start = time.perf_counter()\n",
        "            preds = forward(model, feature, tokens,\n",
        "                            feature_pad_mask, tokens_pad_mask)\n",
        "            pred_end = time.perf_counter()\n",
        "            pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "            # Compute loss.\n",
        "            # Preds do not include <start>, so skip that of tokens.\n",
        "            loss = 0\n",
        "            if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
        "                for t_index in range(1, tokens.shape[-1]):\n",
        "                    pred = preds[:, t_index-1, :]\n",
        "                    token = tokens[:, t_index]\n",
        "                    loss += loss_fn(pred, token)\n",
        "                loss /= tokens.shape[-1]\n",
        "            # LabelSmoothingCrossEntropyLoss\n",
        "            else:\n",
        "                # `[N, T, C] -> [N, C, T]`\n",
        "                preds = preds.permute([0, 2, 1])\n",
        "                # Remove prediction after the last token.\n",
        "                if preds.shape[-1] == tokens.shape[-1]:\n",
        "                    preds = preds[:, :, :-1]\n",
        "                loss = loss_fn(preds, tokens[:, 1:])\n",
        "\n",
        "            val_loss += loss.item()\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average loss.\n",
        "    val_loss /= num_batches\n",
        "    print(\"Validation performance: \\n\",\n",
        "          f\"Avg loss:{val_loss:>8f}\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (val_loss, pred_times) if return_pred_times else val_loss\n",
        "    return retval"
      ],
      "metadata": {
        "id": "ewn5yeIYhj0d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop_csir_s2s(dataloader,\n",
        "                       model,\n",
        "                       device,\n",
        "                       start_id,\n",
        "                       end_id,\n",
        "                       return_pred_times=False,\n",
        "                       max_seqlen=62):\n",
        "    size = len(dataloader.dataset)\n",
        "    total_wer = 0\n",
        "\n",
        "    # Collect prediction time.\n",
        "    pred_times = []\n",
        "\n",
        "    # Switch to evaluation mode.\n",
        "    model.eval()\n",
        "    # Main loop.\n",
        "    print(\"Start test.\")\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_sample in enumerate(dataloader):\n",
        "            feature = batch_sample[\"feature\"]\n",
        "            tokens = batch_sample[\"token\"]\n",
        "            tokens_pad_mask = batch_sample[\"token_pad_mask\"]\n",
        "\n",
        "            check_tokens_format(tokens, tokens_pad_mask, start_id, end_id)\n",
        "\n",
        "            feature = feature.to(device)\n",
        "            tokens = tokens.to(device)\n",
        "            tokens_pad_mask = tokens_pad_mask.to(device)\n",
        "\n",
        "            frames = feature.shape[-2]\n",
        "\n",
        "            # Predict.\n",
        "            pred_start = time.perf_counter()\n",
        "            pred_ids = inference(model, feature, start_id, end_id, max_seqlen=max_seqlen)\n",
        "            pred_end = time.perf_counter()\n",
        "            pred_times.append([frames, pred_end - pred_start])\n",
        "\n",
        "            # Compute WER.\n",
        "            # <sos> and <eos> should be removed because they may boost performance.\n",
        "            # print(tokens)\n",
        "            # print(pred_ids)\n",
        "            tokens = tokens[0, 1:-1]\n",
        "            # pred_ids = pred_ids[0, 1:-1]\n",
        "            pred_ids = [pid for pid in pred_ids[0] if pid not in [start_id, end_id]]\n",
        "            ref_length = len(tokens)\n",
        "            wer = edit_distance(tokens, pred_ids)\n",
        "            wer /= ref_length\n",
        "            total_wer += wer\n",
        "    print(f\"Done. Time:{time.perf_counter()-start}\")\n",
        "\n",
        "    # Average WER.\n",
        "    awer = total_wer / size * 100\n",
        "    print(\"Test performance: \\n\",\n",
        "          f\"Avg WER:{awer:>0.1f}%\\n\")\n",
        "    pred_times = np.array(pred_times)\n",
        "    retval = (awer, pred_times) if return_pred_times else awer\n",
        "    return retval"
      ],
      "metadata": {
        "id": "55N9jLUGhmpd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sanity check"
      ],
      "metadata": {
        "id": "nMXYp4-gZVRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access check.\n",
        "dataset_dir = Path(\"gafs_dataset_very_small\")\n",
        "files = list(dataset_dir.iterdir())\n",
        "dictionary = [fin for fin in files if \".json\" in fin.name][0]\n",
        "hdf5_files = [fin for fin in files if \".hdf5\" in fin.name]\n",
        "\n",
        "print(dictionary)\n",
        "print(hdf5_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc19b-2pZXQE",
        "outputId": "73f62b5c-915f-4935-9755-e84e02c9f479"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gafs_dataset_very_small/character_to_prediction_index.json\n",
            "[PosixPath('gafs_dataset_very_small/125.hdf5'), PosixPath('gafs_dataset_very_small/56.hdf5'), PosixPath('gafs_dataset_very_small/196.hdf5'), PosixPath('gafs_dataset_very_small/157.hdf5'), PosixPath('gafs_dataset_very_small/0.hdf5'), PosixPath('gafs_dataset_very_small/53.hdf5'), PosixPath('gafs_dataset_very_small/145.hdf5'), PosixPath('gafs_dataset_very_small/171.hdf5'), PosixPath('gafs_dataset_very_small/186.hdf5'), PosixPath('gafs_dataset_very_small/20.hdf5'), PosixPath('gafs_dataset_very_small/71.hdf5'), PosixPath('gafs_dataset_very_small/4.hdf5'), PosixPath('gafs_dataset_very_small/109.hdf5'), PosixPath('gafs_dataset_very_small/251.hdf5'), PosixPath('gafs_dataset_very_small/160.hdf5'), PosixPath('gafs_dataset_very_small/227.hdf5'), PosixPath('gafs_dataset_very_small/68.hdf5'), PosixPath('gafs_dataset_very_small/233.hdf5'), PosixPath('gafs_dataset_very_small/154.hdf5'), PosixPath('gafs_dataset_very_small/158.hdf5'), PosixPath('gafs_dataset_very_small/1.hdf5'), PosixPath('gafs_dataset_very_small/115.hdf5'), PosixPath('gafs_dataset_very_small/80.hdf5'), PosixPath('gafs_dataset_very_small/230.hdf5'), PosixPath('gafs_dataset_very_small/161.hdf5'), PosixPath('gafs_dataset_very_small/223.hdf5'), PosixPath('gafs_dataset_very_small/176.hdf5'), PosixPath('gafs_dataset_very_small/203.hdf5'), PosixPath('gafs_dataset_very_small/70.hdf5'), PosixPath('gafs_dataset_very_small/21.hdf5'), PosixPath('gafs_dataset_very_small/217.hdf5'), PosixPath('gafs_dataset_very_small/9.hdf5'), PosixPath('gafs_dataset_very_small/246.hdf5'), PosixPath('gafs_dataset_very_small/168.hdf5'), PosixPath('gafs_dataset_very_small/169.hdf5'), PosixPath('gafs_dataset_very_small/181.hdf5'), PosixPath('gafs_dataset_very_small/202.hdf5'), PosixPath('gafs_dataset_very_small/40.hdf5'), PosixPath('gafs_dataset_very_small/38.hdf5'), PosixPath('gafs_dataset_very_small/143.hdf5'), PosixPath('gafs_dataset_very_small/10.hdf5'), PosixPath('gafs_dataset_very_small/216.hdf5'), PosixPath('gafs_dataset_very_small/43.hdf5'), PosixPath('gafs_dataset_very_small/76.hdf5'), PosixPath('gafs_dataset_very_small/135.hdf5'), PosixPath('gafs_dataset_very_small/13.hdf5'), PosixPath('gafs_dataset_very_small/74.hdf5'), PosixPath('gafs_dataset_very_small/18.hdf5'), PosixPath('gafs_dataset_very_small/147.hdf5'), PosixPath('gafs_dataset_very_small/192.hdf5'), PosixPath('gafs_dataset_very_small/93.hdf5'), PosixPath('gafs_dataset_very_small/254.hdf5'), PosixPath('gafs_dataset_very_small/24.hdf5'), PosixPath('gafs_dataset_very_small/36.hdf5'), PosixPath('gafs_dataset_very_small/105.hdf5'), PosixPath('gafs_dataset_very_small/138.hdf5'), PosixPath('gafs_dataset_very_small/107.hdf5'), PosixPath('gafs_dataset_very_small/239.hdf5'), PosixPath('gafs_dataset_very_small/242.hdf5'), PosixPath('gafs_dataset_very_small/89.hdf5'), PosixPath('gafs_dataset_very_small/231.hdf5'), PosixPath('gafs_dataset_very_small/15.hdf5'), PosixPath('gafs_dataset_very_small/136.hdf5'), PosixPath('gafs_dataset_very_small/59.hdf5'), PosixPath('gafs_dataset_very_small/113.hdf5'), PosixPath('gafs_dataset_very_small/81.hdf5'), PosixPath('gafs_dataset_very_small/241.hdf5'), PosixPath('gafs_dataset_very_small/112.hdf5'), PosixPath('gafs_dataset_very_small/88.hdf5'), PosixPath('gafs_dataset_very_small/95.hdf5'), PosixPath('gafs_dataset_very_small/178.hdf5'), PosixPath('gafs_dataset_very_small/151.hdf5'), PosixPath('gafs_dataset_very_small/128.hdf5'), PosixPath('gafs_dataset_very_small/225.hdf5'), PosixPath('gafs_dataset_very_small/63.hdf5'), PosixPath('gafs_dataset_very_small/72.hdf5'), PosixPath('gafs_dataset_very_small/121.hdf5'), PosixPath('gafs_dataset_very_small/187.hdf5'), PosixPath('gafs_dataset_very_small/122.hdf5'), PosixPath('gafs_dataset_very_small/141.hdf5'), PosixPath('gafs_dataset_very_small/25.hdf5'), PosixPath('gafs_dataset_very_small/2.hdf5'), PosixPath('gafs_dataset_very_small/219.hdf5'), PosixPath('gafs_dataset_very_small/92.hdf5'), PosixPath('gafs_dataset_very_small/188.hdf5'), PosixPath('gafs_dataset_very_small/117.hdf5'), PosixPath('gafs_dataset_very_small/6.hdf5'), PosixPath('gafs_dataset_very_small/73.hdf5'), PosixPath('gafs_dataset_very_small/102.hdf5'), PosixPath('gafs_dataset_very_small/236.hdf5'), PosixPath('gafs_dataset_very_small/27.hdf5'), PosixPath('gafs_dataset_very_small/153.hdf5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dictionary.\n",
        "with open(dictionary, \"r\") as fread:\n",
        "    key2token = json.load(fread)\n",
        "\n",
        "VOCAB = len(key2token)\n",
        "# Add keywords.\n",
        "key2token[\"<sos>\"] = VOCAB\n",
        "key2token[\"<eos>\"] = VOCAB + 1\n",
        "key2token[\"<pad>\"] = VOCAB + 2\n",
        "# Reset.\n",
        "VOCAB = len(key2token)"
      ],
      "metadata": {
        "id": "BPoxLXpDZchs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, use_landmarks = get_fullbody_landmarks()\n",
        "use_features = [\"x\", \"y\"]\n",
        "\n",
        "trans_select_feature = SelectLandmarksAndFeature(landmarks=use_landmarks, features=use_features)\n",
        "trans_repnan = ReplaceNan()\n",
        "trans_norm = PartsBasedNormalization(align_mode=\"framewise\", scale_mode=\"unique\")\n",
        "trans_insert_token = InsertTokensForS2S(sos_token=key2token[\"<sos>\"], eos_token=key2token[\"<eos>\"])\n",
        "\n",
        "pre_transforms = Compose([\n",
        "    trans_select_feature,\n",
        "    trans_repnan,\n",
        "    trans_insert_token,\n",
        "    trans_norm\n",
        "])\n",
        "\n",
        "train_transforms = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "val_transforms = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = Compose([\n",
        "    ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "I6KEUSeqZf2U"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "feature_shape = (len(use_features), -1, len(use_landmarks))\n",
        "token_shape = (-1,)\n",
        "merge_fn = partial(merge_padded_batch,\n",
        "                   feature_shape=feature_shape,\n",
        "                   token_shape=token_shape,\n",
        "                   feature_padding_val=0.0,\n",
        "                   token_padding_val=key2token[\"<pad>\"])\n",
        "\n",
        "dataset = HDF5Dataset(hdf5_files, pre_transforms=pre_transforms, transforms=train_transforms)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=merge_fn)\n",
        "try:\n",
        "    data = next(iter(dataloader))\n",
        "    feature_origin = data[\"feature\"]\n",
        "    tokens_origin = data[\"token\"]\n",
        "\n",
        "    print(feature_origin.shape)\n",
        "    print(tokens_origin)\n",
        "except Exception as inst:\n",
        "    print(inst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rExP4cCiGAm",
        "outputId": "70c06503-0f4b-47a9-cd5e-233adfc17009"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 218, 130])\n",
            "tensor([[59, 14, 32, 53, 40, 43, 43, 46, 52, 51, 49, 36, 40, 55, 14, 60],\n",
            "        [59, 38, 49, 32, 34, 40, 36, 43, 32,  0, 43, 32, 49, 32, 60, 61]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model.\n",
        "# in_channels: J * C (130*2=260)\n",
        "#   J: use_landmarks (130)\n",
        "#   C: use_channels (2)\n",
        "# out_channels: 10\n",
        "in_channels = len(use_landmarks) * len(use_features)\n",
        "out_channels = VOCAB\n",
        "enc_hidden_channels = 64\n",
        "dec_hidden_channels = 64\n",
        "dec_emb_channels = 4\n",
        "dec_att_dim = 64\n",
        "model = RNNCSLR(\n",
        "    enc_in_channels=in_channels,\n",
        "    enc_hidden_channels=enc_hidden_channels,\n",
        "    enc_rnn_type=\"gru\",\n",
        "    enc_num_layers=2,\n",
        "    enc_activation=\"relu\",\n",
        "    enc_bidir=True,\n",
        "    enc_dropout=0.1,\n",
        "    enc_apply_mask=True,\n",
        "    enc_proj_size=0,\n",
        "    dec_in_channels=enc_hidden_channels,\n",
        "    dec_hidden_channels=dec_hidden_channels,\n",
        "    dec_out_channels=out_channels,\n",
        "    dec_emb_channels=dec_emb_channels,\n",
        "    dec_att_dim=dec_att_dim,\n",
        "    dec_att_add_bias=True,\n",
        "    dec_rnn_type=\"gru\",\n",
        "    dec_num_layers=2,\n",
        "    dec_activation=\"relu\",\n",
        "    dec_dropout=0.1,\n",
        "    dec_padding_val=key2token[\"<pad>\"],\n",
        "    dec_proj_size=0)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Sanity check.\n",
        "sample = next(iter(dataloader))\n",
        "logit = model(sample[\"feature\"],\n",
        "              tokens=sample[\"token\"],\n",
        "              feature_pad_mask=sample[\"feature_pad_mask\"])\n",
        "print(logit.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yQKHqQtiUTE",
        "outputId": "2cd63889-ec20-4a2f-d078-a4d16aa0e6f0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNCSLR(\n",
            "  (linear): Linear(in_features=260, out_features=64, bias=True)\n",
            "  (enc_activation): ReLU()\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (decoder): BahdanauRNNDecoder(\n",
            "    (emb_layer): Embedding(62, 4, padding_idx=61)\n",
            "    (att_layer): SingleHeadAttention(\n",
            "      (att_energy): BahdanauAttentionEnergy(\n",
            "        (w_key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_out): Linear(in_features=128, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (rnn): GRU(132, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (head): Linear(in_features=128, out_features=62, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([2, 16, 62])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train and evaluation"
      ],
      "metadata": {
        "id": "r4o9bbOMjG_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Set common parameters"
      ],
      "metadata": {
        "id": "22_3Ne9ojKLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set common parameters.\n",
        "batch_size = 32\n",
        "load_into_ram = True\n",
        "test_pid = 0\n",
        "num_workers = os.cpu_count()\n",
        "print(f\"Using {num_workers} cores for data loading.\")\n",
        "lr = 3e-4\n",
        "label_smoothing = 0.1\n",
        "sos_token = key2token[\"<sos>\"]\n",
        "eos_token = key2token[\"<eos>\"]\n",
        "pad_token = key2token[\"<pad>\"]\n",
        "max_seqlen = 60\n",
        "\n",
        "epochs = 50\n",
        "eval_every_n_epochs = 1\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} for computation.\")\n",
        "\n",
        "train_hdf5files = [fin for fin in hdf5_files if str(test_pid) not in fin.name]\n",
        "val_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "test_hdf5files = [fin for fin in hdf5_files if str(test_pid) in fin.name]\n",
        "\n",
        "_, use_landmarks = get_fullbody_landmarks()\n",
        "use_features = [\"x\", \"y\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3UpkewijNd0",
        "outputId": "6b6879e4-33d8-4890-d281-ac3220c659c2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2 cores for data loading.\n",
            "Using cuda for computation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataloaders.\n",
        "train_dataset = HDF5Dataset(train_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=train_transforms, load_into_ram=load_into_ram)\n",
        "val_dataset = HDF5Dataset(val_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=val_transforms, load_into_ram=load_into_ram)\n",
        "test_dataset = HDF5Dataset(test_hdf5files,\n",
        "    pre_transforms=pre_transforms, transforms=test_transforms, load_into_ram=load_into_ram)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=merge_fn, num_workers=num_workers, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=merge_fn, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "RiTJhuSSjW8k"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Run training process"
      ],
      "metadata": {
        "id": "idjHfhW4jerd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = RNNCSLR(\n",
        "    enc_in_channels=in_channels,\n",
        "    enc_hidden_channels=enc_hidden_channels,\n",
        "    enc_rnn_type=\"gru\",\n",
        "    enc_num_layers=2,\n",
        "    enc_activation=\"relu\",\n",
        "    enc_bidir=True,\n",
        "    enc_dropout=0.1,\n",
        "    enc_apply_mask=True,\n",
        "    enc_proj_size=0,\n",
        "    dec_in_channels=enc_hidden_channels,\n",
        "    dec_hidden_channels=dec_hidden_channels,\n",
        "    dec_out_channels=out_channels,\n",
        "    dec_emb_channels=dec_emb_channels,\n",
        "    dec_att_dim=dec_att_dim,\n",
        "    dec_att_add_bias=True,\n",
        "    dec_rnn_type=\"gru\",\n",
        "    dec_num_layers=2,\n",
        "    dec_activation=\"relu\",\n",
        "    dec_dropout=0.1,\n",
        "    dec_padding_val=pad_token,\n",
        "    dec_proj_size=0)\n",
        "\n",
        "print(model_rnn)\n",
        "\n",
        "loss_fn = LabelSmoothingCrossEntropyLoss(\n",
        "    ignore_indices=pad_token, reduction=\"mean_temporal_prior\",\n",
        "    label_smoothing=label_smoothing)\n",
        "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAXnUtz5jheb",
        "outputId": "c4e08472-3492-4dfb-81cb-bf4ddf0553f6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNCSLR(\n",
            "  (linear): Linear(in_features=260, out_features=64, bias=True)\n",
            "  (enc_activation): ReLU()\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (decoder): BahdanauRNNDecoder(\n",
            "    (emb_layer): Embedding(62, 4, padding_idx=61)\n",
            "    (att_layer): SingleHeadAttention(\n",
            "      (att_energy): BahdanauAttentionEnergy(\n",
            "        (w_key): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_query): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (w_out): Linear(in_features=128, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (rnn): GRU(132, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "    (head): Linear(in_features=128, out_features=62, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, validation, and evaluation.\n",
        "model_rnn.to(device)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_wers = []\n",
        "print(\"Start training.\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "    train_loss, train_times = train_loop_csir_s2s(\n",
        "        train_dataloader, model_rnn, loss_fn, optimizer, device,\n",
        "        sos_token, eos_token,\n",
        "        return_pred_times=True)\n",
        "    val_loss, val_times = val_loop_csir_s2s(\n",
        "        val_dataloader, model_rnn, loss_fn, device,\n",
        "        sos_token, eos_token,\n",
        "        return_pred_times=True)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if (epoch+1) % eval_every_n_epochs == 0:\n",
        "        wer, test_times = test_loop_csir_s2s(\n",
        "            test_dataloader, model_rnn, device,\n",
        "            sos_token, eos_token,\n",
        "            return_pred_times=True,\n",
        "            max_seqlen=max_seqlen)\n",
        "        test_wers.append(wer)\n",
        "train_losses_rnn = np.array(train_losses)\n",
        "val_losses_rnn = np.array(val_losses)\n",
        "test_wers_rnn = np.array(test_wers)\n",
        "\n",
        "val_losses_rnn = np.array(val_losses_rnn)\n",
        "test_wers_rnn = np.array(test_wers_rnn)\n",
        "print(f\"Minimum validation loss:{val_losses_rnn.min()} at {np.argmin(val_losses_rnn)+1} epoch.\")\n",
        "print(f\"Minimum WER:{test_wers_rnn.min()} at {np.argmin(test_wers_rnn)*eval_every_n_epochs+1} epoch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSxf_6kBjzAk",
        "outputId": "e9d2befd-f332-472d-d867-b61c9b044d2d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training.\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1\n",
            "Start training.\n",
            "loss:4.138615 [    0/ 2513]\n",
            "Done. Time:17.80025859699998\n",
            "Training performance: \n",
            " Avg loss:3.748931\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4134294060000343\n",
            "Validation performance: \n",
            " Avg loss:3.600302\n",
            "\n",
            "Start test.\n",
            "Done. Time:14.551496429999986\n",
            "Test performance: \n",
            " Avg WER:90.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2\n",
            "Start training.\n",
            "loss:3.583274 [    0/ 2513]\n",
            "Done. Time:16.144281954999997\n",
            "Training performance: \n",
            " Avg loss:3.585932\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.060495639999999\n",
            "Validation performance: \n",
            " Avg loss:3.585156\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.715763079\n",
            "Test performance: \n",
            " Avg WER:94.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3\n",
            "Start training.\n",
            "loss:3.590196 [    0/ 2513]\n",
            "Done. Time:16.188231419000033\n",
            "Training performance: \n",
            " Avg loss:3.569763\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.390618399999994\n",
            "Validation performance: \n",
            " Avg loss:3.553747\n",
            "\n",
            "Start test.\n",
            "Done. Time:14.521273856999983\n",
            "Test performance: \n",
            " Avg WER:91.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4\n",
            "Start training.\n",
            "loss:3.545389 [    0/ 2513]\n",
            "Done. Time:16.14732353400001\n",
            "Training performance: \n",
            " Avg loss:3.498836\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4006207229999745\n",
            "Validation performance: \n",
            " Avg loss:3.422230\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.359725027000025\n",
            "Test performance: \n",
            " Avg WER:100.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5\n",
            "Start training.\n",
            "loss:3.406738 [    0/ 2513]\n",
            "Done. Time:16.80120446799998\n",
            "Training performance: \n",
            " Avg loss:3.366013\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.4010318859999984\n",
            "Validation performance: \n",
            " Avg loss:3.333904\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.057113886000025\n",
            "Test performance: \n",
            " Avg WER:93.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6\n",
            "Start training.\n",
            "loss:3.186252 [    0/ 2513]\n",
            "Done. Time:16.51833997\n",
            "Training performance: \n",
            " Avg loss:3.290741\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4153542540000217\n",
            "Validation performance: \n",
            " Avg loss:3.283073\n",
            "\n",
            "Start test.\n",
            "Done. Time:17.162653271000067\n",
            "Test performance: \n",
            " Avg WER:91.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7\n",
            "Start training.\n",
            "loss:3.268678 [    0/ 2513]\n",
            "Done. Time:16.20617981999999\n",
            "Training performance: \n",
            " Avg loss:3.234715\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4324082960000624\n",
            "Validation performance: \n",
            " Avg loss:3.226814\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.863725885000008\n",
            "Test performance: \n",
            " Avg WER:91.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8\n",
            "Start training.\n",
            "loss:3.254235 [    0/ 2513]\n",
            "Done. Time:17.23018361900006\n",
            "Training performance: \n",
            " Avg loss:3.194285\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3957912590000205\n",
            "Validation performance: \n",
            " Avg loss:3.189643\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.155109338000102\n",
            "Test performance: \n",
            " Avg WER:89.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9\n",
            "Start training.\n",
            "loss:3.144718 [    0/ 2513]\n",
            "Done. Time:16.731584139000006\n",
            "Training performance: \n",
            " Avg loss:3.159715\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4512033400000064\n",
            "Validation performance: \n",
            " Avg loss:3.154590\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.687678835000042\n",
            "Test performance: \n",
            " Avg WER:88.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10\n",
            "Start training.\n",
            "loss:3.141302 [    0/ 2513]\n",
            "Done. Time:16.258170793999966\n",
            "Training performance: \n",
            " Avg loss:3.124771\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4671557409999423\n",
            "Validation performance: \n",
            " Avg loss:3.129954\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.95258653999997\n",
            "Test performance: \n",
            " Avg WER:87.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11\n",
            "Start training.\n",
            "loss:3.129721 [    0/ 2513]\n",
            "Done. Time:16.355777773\n",
            "Training performance: \n",
            " Avg loss:3.095732\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.395809426000028\n",
            "Validation performance: \n",
            " Avg loss:3.095232\n",
            "\n",
            "Start test.\n",
            "Done. Time:16.439444983000044\n",
            "Test performance: \n",
            " Avg WER:85.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12\n",
            "Start training.\n",
            "loss:3.105307 [    0/ 2513]\n",
            "Done. Time:17.117472243000066\n",
            "Training performance: \n",
            " Avg loss:3.061367\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.515100709999956\n",
            "Validation performance: \n",
            " Avg loss:3.070323\n",
            "\n",
            "Start test.\n",
            "Done. Time:18.951994334000005\n",
            "Test performance: \n",
            " Avg WER:88.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13\n",
            "Start training.\n",
            "loss:3.058137 [    0/ 2513]\n",
            "Done. Time:18.43185225100001\n",
            "Training performance: \n",
            " Avg loss:3.035635\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.42701092599998\n",
            "Validation performance: \n",
            " Avg loss:3.048731\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.3245051560001\n",
            "Test performance: \n",
            " Avg WER:90.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14\n",
            "Start training.\n",
            "loss:3.000008 [    0/ 2513]\n",
            "Done. Time:16.573021625000024\n",
            "Training performance: \n",
            " Avg loss:3.008480\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4594914409999546\n",
            "Validation performance: \n",
            " Avg loss:3.019689\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.139000667999994\n",
            "Test performance: \n",
            " Avg WER:89.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15\n",
            "Start training.\n",
            "loss:3.021697 [    0/ 2513]\n",
            "Done. Time:16.565591184000027\n",
            "Training performance: \n",
            " Avg loss:2.980795\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.0345048110000334\n",
            "Validation performance: \n",
            " Avg loss:2.996636\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.252911704999974\n",
            "Test performance: \n",
            " Avg WER:87.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16\n",
            "Start training.\n",
            "loss:2.990853 [    0/ 2513]\n",
            "Done. Time:16.77709212299999\n",
            "Training performance: \n",
            " Avg loss:2.960987\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3953534049999234\n",
            "Validation performance: \n",
            " Avg loss:2.973466\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.14612467300003\n",
            "Test performance: \n",
            " Avg WER:87.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17\n",
            "Start training.\n",
            "loss:2.862390 [    0/ 2513]\n",
            "Done. Time:17.457348118000027\n",
            "Training performance: \n",
            " Avg loss:2.937732\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.528858575000072\n",
            "Validation performance: \n",
            " Avg loss:2.961670\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.20136645299999\n",
            "Test performance: \n",
            " Avg WER:89.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18\n",
            "Start training.\n",
            "loss:2.928396 [    0/ 2513]\n",
            "Done. Time:16.476797042000044\n",
            "Training performance: \n",
            " Avg loss:2.915656\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.6922109700000192\n",
            "Validation performance: \n",
            " Avg loss:2.942604\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.510484828000017\n",
            "Test performance: \n",
            " Avg WER:89.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19\n",
            "Start training.\n",
            "loss:2.740343 [    0/ 2513]\n",
            "Done. Time:16.243431948999955\n",
            "Training performance: \n",
            " Avg loss:2.896271\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3859753529999352\n",
            "Validation performance: \n",
            " Avg loss:2.928188\n",
            "\n",
            "Start test.\n",
            "Done. Time:26.29528223500006\n",
            "Test performance: \n",
            " Avg WER:98.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20\n",
            "Start training.\n",
            "loss:2.881488 [    0/ 2513]\n",
            "Done. Time:16.192587013999855\n",
            "Training performance: \n",
            " Avg loss:2.875379\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3992968279999332\n",
            "Validation performance: \n",
            " Avg loss:2.906744\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.247876130999884\n",
            "Test performance: \n",
            " Avg WER:92.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21\n",
            "Start training.\n",
            "loss:2.907153 [    0/ 2513]\n",
            "Done. Time:16.692926370000123\n",
            "Training performance: \n",
            " Avg loss:2.856238\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.839521053999988\n",
            "Validation performance: \n",
            " Avg loss:2.903430\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.129585070000076\n",
            "Test performance: \n",
            " Avg WER:93.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22\n",
            "Start training.\n",
            "loss:2.815756 [    0/ 2513]\n",
            "Done. Time:17.118292196000084\n",
            "Training performance: \n",
            " Avg loss:2.842481\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.8123973029998979\n",
            "Validation performance: \n",
            " Avg loss:2.885060\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.80319572999997\n",
            "Test performance: \n",
            " Avg WER:97.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23\n",
            "Start training.\n",
            "loss:2.829437 [    0/ 2513]\n",
            "Done. Time:17.11088309899992\n",
            "Training performance: \n",
            " Avg loss:2.826732\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1366664639999726\n",
            "Validation performance: \n",
            " Avg loss:2.869212\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.082414062999987\n",
            "Test performance: \n",
            " Avg WER:91.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24\n",
            "Start training.\n",
            "loss:2.854586 [    0/ 2513]\n",
            "Done. Time:16.03560913199999\n",
            "Training performance: \n",
            " Avg loss:2.815391\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.384652517999939\n",
            "Validation performance: \n",
            " Avg loss:2.867071\n",
            "\n",
            "Start test.\n",
            "Done. Time:23.77620941400005\n",
            "Test performance: \n",
            " Avg WER:103.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25\n",
            "Start training.\n",
            "loss:2.919020 [    0/ 2513]\n",
            "Done. Time:16.321479683999996\n",
            "Training performance: \n",
            " Avg loss:2.799048\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.389958686\n",
            "Validation performance: \n",
            " Avg loss:2.848570\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.081456677999995\n",
            "Test performance: \n",
            " Avg WER:92.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 26\n",
            "Start training.\n",
            "loss:2.808258 [    0/ 2513]\n",
            "Done. Time:16.415791866000063\n",
            "Training performance: \n",
            " Avg loss:2.789371\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3995395239999198\n",
            "Validation performance: \n",
            " Avg loss:2.838539\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.39257685000007\n",
            "Test performance: \n",
            " Avg WER:93.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 27\n",
            "Start training.\n",
            "loss:2.674319 [    0/ 2513]\n",
            "Done. Time:18.76869171300018\n",
            "Training performance: \n",
            " Avg loss:2.776926\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.6095193060000383\n",
            "Validation performance: \n",
            " Avg loss:2.828690\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.87555350799994\n",
            "Test performance: \n",
            " Avg WER:91.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 28\n",
            "Start training.\n",
            "loss:2.699914 [    0/ 2513]\n",
            "Done. Time:16.52748433900001\n",
            "Training performance: \n",
            " Avg loss:2.768672\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4134292029998505\n",
            "Validation performance: \n",
            " Avg loss:2.833921\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.950807593999798\n",
            "Test performance: \n",
            " Avg WER:93.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 29\n",
            "Start training.\n",
            "loss:2.911854 [    0/ 2513]\n",
            "Done. Time:16.446289466000053\n",
            "Training performance: \n",
            " Avg loss:2.757781\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3914239480000106\n",
            "Validation performance: \n",
            " Avg loss:2.812578\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.497054833999982\n",
            "Test performance: \n",
            " Avg WER:93.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 30\n",
            "Start training.\n",
            "loss:2.647235 [    0/ 2513]\n",
            "Done. Time:16.236682525000106\n",
            "Training performance: \n",
            " Avg loss:2.744472\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4798469390000264\n",
            "Validation performance: \n",
            " Avg loss:2.804337\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.314877025999976\n",
            "Test performance: \n",
            " Avg WER:96.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 31\n",
            "Start training.\n",
            "loss:2.836749 [    0/ 2513]\n",
            "Done. Time:17.383475894999947\n",
            "Training performance: \n",
            " Avg loss:2.737663\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.272533224999961\n",
            "Validation performance: \n",
            " Avg loss:2.810000\n",
            "\n",
            "Start test.\n",
            "Done. Time:24.491216154000085\n",
            "Test performance: \n",
            " Avg WER:103.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 32\n",
            "Start training.\n",
            "loss:2.611518 [    0/ 2513]\n",
            "Done. Time:17.138053160000027\n",
            "Training performance: \n",
            " Avg loss:2.728513\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1832086290000916\n",
            "Validation performance: \n",
            " Avg loss:2.794850\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.762590297000088\n",
            "Test performance: \n",
            " Avg WER:89.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 33\n",
            "Start training.\n",
            "loss:2.688011 [    0/ 2513]\n",
            "Done. Time:16.154142860000093\n",
            "Training performance: \n",
            " Avg loss:2.720967\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4325018080000973\n",
            "Validation performance: \n",
            " Avg loss:2.785171\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.19280276099994\n",
            "Test performance: \n",
            " Avg WER:90.4%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 34\n",
            "Start training.\n",
            "loss:2.701349 [    0/ 2513]\n",
            "Done. Time:16.31331895800008\n",
            "Training performance: \n",
            " Avg loss:2.711602\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4203460549999818\n",
            "Validation performance: \n",
            " Avg loss:2.779178\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.05828362400007\n",
            "Test performance: \n",
            " Avg WER:95.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 35\n",
            "Start training.\n",
            "loss:2.634381 [    0/ 2513]\n",
            "Done. Time:16.59712628899979\n",
            "Training performance: \n",
            " Avg loss:2.702974\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3880636579999646\n",
            "Validation performance: \n",
            " Avg loss:2.776353\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.075026077999837\n",
            "Test performance: \n",
            " Avg WER:92.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 36\n",
            "Start training.\n",
            "loss:2.689470 [    0/ 2513]\n",
            "Done. Time:16.540279455000018\n",
            "Training performance: \n",
            " Avg loss:2.695207\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.1799455269999726\n",
            "Validation performance: \n",
            " Avg loss:2.765713\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.887600606000206\n",
            "Test performance: \n",
            " Avg WER:91.3%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 37\n",
            "Start training.\n",
            "loss:2.622910 [    0/ 2513]\n",
            "Done. Time:16.201956565000046\n",
            "Training performance: \n",
            " Avg loss:2.684396\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.482880072000171\n",
            "Validation performance: \n",
            " Avg loss:2.762070\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.963614880000023\n",
            "Test performance: \n",
            " Avg WER:93.1%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 38\n",
            "Start training.\n",
            "loss:2.682283 [    0/ 2513]\n",
            "Done. Time:16.176687892000018\n",
            "Training performance: \n",
            " Avg loss:2.681198\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3997225600001002\n",
            "Validation performance: \n",
            " Avg loss:2.754264\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.330076353000095\n",
            "Test performance: \n",
            " Avg WER:89.2%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 39\n",
            "Start training.\n",
            "loss:2.658091 [    0/ 2513]\n",
            "Done. Time:16.84877560699988\n",
            "Training performance: \n",
            " Avg loss:2.669485\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.3797130919999745\n",
            "Validation performance: \n",
            " Avg loss:2.748266\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.998307762999957\n",
            "Test performance: \n",
            " Avg WER:90.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 40\n",
            "Start training.\n",
            "loss:2.599532 [    0/ 2513]\n",
            "Done. Time:16.291301719999865\n",
            "Training performance: \n",
            " Avg loss:2.661643\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.8437722510000185\n",
            "Validation performance: \n",
            " Avg loss:2.744877\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.86571514100001\n",
            "Test performance: \n",
            " Avg WER:93.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 41\n",
            "Start training.\n",
            "loss:2.512650 [    0/ 2513]\n",
            "Done. Time:16.552361395999924\n",
            "Training performance: \n",
            " Avg loss:2.654504\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.5030244590000166\n",
            "Validation performance: \n",
            " Avg loss:2.744635\n",
            "\n",
            "Start test.\n",
            "Done. Time:22.690012389000003\n",
            "Test performance: \n",
            " Avg WER:88.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 42\n",
            "Start training.\n",
            "loss:2.656835 [    0/ 2513]\n",
            "Done. Time:16.243245308999803\n",
            "Training performance: \n",
            " Avg loss:2.646444\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4197388920001686\n",
            "Validation performance: \n",
            " Avg loss:2.742101\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.41273238000008\n",
            "Test performance: \n",
            " Avg WER:91.5%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 43\n",
            "Start training.\n",
            "loss:2.613726 [    0/ 2513]\n",
            "Done. Time:17.429118559000017\n",
            "Training performance: \n",
            " Avg loss:2.638696\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.3661641160001636\n",
            "Validation performance: \n",
            " Avg loss:2.734274\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.254978176999884\n",
            "Test performance: \n",
            " Avg WER:88.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 44\n",
            "Start training.\n",
            "loss:2.708719 [    0/ 2513]\n",
            "Done. Time:16.72627242800013\n",
            "Training performance: \n",
            " Avg loss:2.630627\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4220050889998674\n",
            "Validation performance: \n",
            " Avg loss:2.730884\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.551622999000074\n",
            "Test performance: \n",
            " Avg WER:87.9%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 45\n",
            "Start training.\n",
            "loss:2.565041 [    0/ 2513]\n",
            "Done. Time:16.20540187100005\n",
            "Training performance: \n",
            " Avg loss:2.622532\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4822169930000655\n",
            "Validation performance: \n",
            " Avg loss:2.724219\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.87157391100004\n",
            "Test performance: \n",
            " Avg WER:88.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 46\n",
            "Start training.\n",
            "loss:2.543045 [    0/ 2513]\n",
            "Done. Time:18.11347491300012\n",
            "Training performance: \n",
            " Avg loss:2.614104\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.441078787999686\n",
            "Validation performance: \n",
            " Avg loss:2.727591\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.608871872000236\n",
            "Test performance: \n",
            " Avg WER:88.8%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 47\n",
            "Start training.\n",
            "loss:2.554721 [    0/ 2513]\n",
            "Done. Time:16.493633473000045\n",
            "Training performance: \n",
            " Avg loss:2.606052\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.490568397000061\n",
            "Validation performance: \n",
            " Avg loss:2.715195\n",
            "\n",
            "Start test.\n",
            "Done. Time:20.98693728299986\n",
            "Test performance: \n",
            " Avg WER:88.6%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 48\n",
            "Start training.\n",
            "loss:2.668036 [    0/ 2513]\n",
            "Done. Time:16.650834198999746\n",
            "Training performance: \n",
            " Avg loss:2.598466\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4167336959999375\n",
            "Validation performance: \n",
            " Avg loss:2.720110\n",
            "\n",
            "Start test.\n",
            "Done. Time:19.639225603999876\n",
            "Test performance: \n",
            " Avg WER:87.0%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 49\n",
            "Start training.\n",
            "loss:2.520159 [    0/ 2513]\n",
            "Done. Time:16.734904945999915\n",
            "Training performance: \n",
            " Avg loss:2.594040\n",
            "\n",
            "Start validation.\n",
            "Done. Time:2.172629888999836\n",
            "Validation performance: \n",
            " Avg loss:2.710680\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.45148971100025\n",
            "Test performance: \n",
            " Avg WER:92.7%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 50\n",
            "Start training.\n",
            "loss:2.465452 [    0/ 2513]\n",
            "Done. Time:16.295255010000346\n",
            "Training performance: \n",
            " Avg loss:2.586409\n",
            "\n",
            "Start validation.\n",
            "Done. Time:1.4608242510003038\n",
            "Validation performance: \n",
            " Avg loss:2.708993\n",
            "\n",
            "Start test.\n",
            "Done. Time:21.428014906000044\n",
            "Test performance: \n",
            " Avg WER:89.3%\n",
            "\n",
            "Minimum validation loss:2.70899298787117 at 50 epoch.\n",
            "Minimum WER:85.15103855954102 at 11 epoch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot result"
      ],
      "metadata": {
        "id": "bI_5zh65kfss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.grid(axis=\"y\", linestyle=\"dotted\", color=\"k\")\n",
        "\n",
        "xs = np.arange(1, len(val_losses_rnn)+1)\n",
        "plt.plot(xs, val_losses_rnn, label=\"RNN\", marker=\".\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.ylim([0.0, 5.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "kQ1seXMDkiHj",
        "outputId": "aa69e98c-d4b4-4001-b81a-0e96ccc023f2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid(axis=\"y\", linestyle=\"dotted\", color=\"k\")\n",
        "\n",
        "xs = np.arange(1, len(test_wers_rnn)+1)\n",
        "plt.plot(xs, test_wers_rnn, label=\"RNN\", marker=\".\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"WER\")\n",
        "plt.ylim([0.0, 100.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "4Uq2CQVDk9ol",
        "outputId": "f82d8045-3424-4f39-e965-0e3cacd37b69"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}